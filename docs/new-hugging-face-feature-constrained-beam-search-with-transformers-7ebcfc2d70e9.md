# çº¦æŸæ³¢æŸæœç´¢ğŸ¤—å˜å½¢é‡‘åˆš(ç”µå½±å)

> åŸæ–‡ï¼š<https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9>

## ä¸€ä¸ªæ–°çš„æ‹¥æŠ±è„¸ç‰¹æ€§å…è®¸ä½ å®šåˆ¶å’Œå¼•å¯¼ä½ çš„è¯­è¨€æ¨¡å‹è¾“å‡º(æ¯”å¦‚åœ¨è¾“å‡ºä¸­å¼ºåˆ¶ä¸€ä¸ªç‰¹å®šçš„åºåˆ—)ã€‚

![](img/45d5aa1915e919027904ce69e936c6d6.png)

ç…§ç‰‡ç”± Priscilla Du Preez åœ¨ [Unsplash](https://unsplash.com/s/photos/hugging?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹æ‘„|ä¸€ä¸ªæ–°çš„â€œæ‹¥æŠ±â€é¢éƒ¨ç‰¹å¾ï¼

[æŠ±è„¸å˜å½¢é‡‘åˆš](https://github.com/huggingface/transformers)æœ‰äº†æ–°åŠŸèƒ½ï¼å®ƒè¢«ç§°ä¸º ***çº¦æŸå…‰æŸæœç´¢*** ï¼Œå®ƒå…è®¸æˆ‘ä»¬å¼•å¯¼æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿™åœ¨ä»¥å‰æ˜¯å®Œå…¨ç‹¬ç«‹äºæ¨¡å‹çš„ã€‚

# ä»‹ç»

æœ‰æ—¶æˆ‘ä»¬ç¡®åˆ‡åœ°çŸ¥é“åœ¨æ–‡æœ¬ç”Ÿæˆè¾“å‡ºä¸­æˆ‘ä»¬æƒ³è¦ä»€ä¹ˆã€‚

ä¾‹å¦‚ï¼Œåœ¨ç¥ç»æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½é€šè¿‡å­—å…¸æŸ¥æ‰¾çŸ¥é“å“ªäº›å•è¯å¿…é¡»åŒ…å«åœ¨æœ€ç»ˆç¿»è¯‘ä¸­ã€‚æœ‰æ—¶ï¼Œç”±äºç‰¹å®šçš„ä¸Šä¸‹æ–‡ï¼Œå¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´å‡ ä¹åŒæ ·å¯èƒ½çš„ç”Ÿæˆè¾“å‡ºå¯¹äºæœ€ç»ˆç”¨æˆ·æ¥è¯´å¯èƒ½å¹¶ä¸åŒæ ·ç†æƒ³ã€‚

è¿™ä¸¤ç§æƒ…å†µéƒ½å¯ä»¥é€šè¿‡å…è®¸ç”¨æˆ·å‘Šè¯‰æ¨¡å‹å“ªäº›å•è¯å¿…é¡»åŒ…å«åœ¨æœ€ç»ˆè¾“å‡ºä¸­æ¥è§£å†³ã€‚æ–°çš„çº¦æŸæ³¢æŸæœç´¢ç‰¹æ€§å’Œæ–°çš„`model.generate()`å‡½æ•°çš„`force_words_ids`å‚æ•°å…è®¸æˆ‘ä»¬è¿™æ ·åšï¼

# ç¤ºä¾‹ 1:å¼ºåˆ¶å•è¯

å‡è®¾æˆ‘ä»¬è¯•å›¾å°†`"How old are you?"`ç¿»è¯‘æˆå¾·è¯­ã€‚

`"Wie alt bist du?"`æ˜¯ä½ åœ¨éæ­£å¼åœºåˆè¯´çš„è¯ï¼Œ`"Wie alt sind Sie?"`æ˜¯ä½ åœ¨æ­£å¼åœºåˆè¯´çš„è¯ã€‚

æ ¹æ®ä¸Šä¸‹æ–‡çš„ä¸åŒï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä¸€ç§å½¢å¼æ¯”å¦ä¸€ç§å½¢å¼æ›´æ­£å¼ï¼Œä½†æ˜¯æˆ‘ä»¬å¦‚ä½•å‘Šè¯‰æ¨¡å‹å‘¢ï¼Ÿ

# ä¼ ç»Ÿæ³¢æŸæœç´¢

ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨*ä¼ ç»Ÿå…‰æŸæœç´¢è®¾ç½®ä¸­è¿›è¡Œæ–‡æœ¬ç¿»è¯‘ã€‚*

è®©æˆ‘ä»¬é¦–å…ˆå®‰è£…å˜å½¢é‡‘åˆšåº“:

```
!pip install -q git+https://github.com/huggingface/transformers.git
```

ä¸‹é¢æ˜¯æ•™ç§‘ä¹¦ huggingface ä»£ç ï¼Œç”¨äºä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¥å®Œæˆåƒ NMT è¿™æ ·çš„ä»»åŠ¡ï¼Œå®ƒæ˜¯é€šè¿‡ä¼ ç»Ÿçš„æ³¢æŸæœç´¢å®ç°çš„:

```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLMtokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")encoder_input_str = "translate English to German: How old are you?"input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_idsoutputs = model.generate(
    input_ids,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)print("Output:\n" + 100 * '-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))Output:
--------------------------------------------------------------------
Wie alt bist du?
```

# ä½¿ç”¨çº¦æŸæ³¢æŸæœç´¢

ä½†æ˜¯å¦‚æœæˆ‘ä»¬çŸ¥é“æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªæ­£å¼çš„è¾“å‡ºï¼Œè€Œä¸æ˜¯éæ­£å¼çš„è¾“å‡ºå‘¢ï¼Ÿå¦‚æœæˆ‘ä»¬ä»å…ˆå‰çš„çŸ¥è¯†ä¸­çŸ¥é“è¿™ä¸€ä»£äººå¿…é¡»åŒ…æ‹¬ä»€ä¹ˆï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥*å°†å®ƒ*æ³¨å…¥åˆ°è¿™ä¸€ä»£äººä¸­ï¼Œä¼šæ€ä¹ˆæ ·ï¼Ÿ

ä¸‹é¢æ˜¯ä½¿ç”¨`force_words_ids`å…³é”®å­—å‚æ•°åˆ°`model.generate()`çš„å¯èƒ½ç»“æœ:

```
tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")encoder_input_str = "translate English to German: How old are you?"force_words = ["Sie"]input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids
force_words_ids = tokenizer(force_words, add_special_tokens=False).input_idsoutputs = model.generate(
    input_ids,
    force_words_ids=force_words_ids,
    num_beams=5,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)print("Output:\n" + 100 * '-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))Output:
--------------------------------------------------------------------
Wie alt sind Sie?
```

å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å…³äºæˆ‘ä»¬æœŸæœ›è¾“å‡ºçš„å…ˆéªŒçŸ¥è¯†æ¥å¼•å¯¼è¿™ä¸€ä»£äººã€‚ä»¥å‰ï¼Œæˆ‘ä»¬å¿…é¡»ç”Ÿæˆä¸€å †å¯èƒ½çš„è¾“å‡ºï¼Œç„¶åè¿‡æ»¤å‡ºç¬¦åˆæˆ‘ä»¬è¦æ±‚çš„è¾“å‡ºã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨ç”Ÿæˆé˜¶æ®µåšåˆ°è¿™ä¸€ç‚¹ã€‚

# ç¤ºä¾‹ 2:æå–çº¦æŸ

æˆ‘ä»¬ä¸Šé¢æåˆ°äº†ä¸€ä¸ªç”¨ä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬æƒ³è¦åœ¨æœ€ç»ˆè¾“å‡ºä¸­åŒ…å«å“ªäº›å•è¯ã€‚è¿™æ–¹é¢çš„ä¸€ä¸ªä¾‹å­å¯èƒ½æ˜¯åœ¨ç¥ç»æœºå™¨ç¿»è¯‘æœŸé—´ä½¿ç”¨å­—å…¸æŸ¥æ‰¾ã€‚

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä¸çŸ¥é“ä½¿ç”¨å“ªç§*å•è¯å½¢å¼*ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›åƒ`["raining", "rained", "rains", ...]`è¿™æ ·çš„è¾“å‡ºåŒæ ·å¯èƒ½ï¼Œé‚£è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿä»æ›´ä¸€èˆ¬çš„æ„ä¹‰ä¸Šæ¥è¯´ï¼Œæ€»ä¼šæœ‰è¿™æ ·çš„æƒ…å†µï¼Œæˆ‘ä»¬ä¸å¸Œæœ›*ä¸€å­—ä¸å·®åœ°*ä¸€ä¸ªå­—æ¯ä¸€ä¸ªå­—æ¯åœ°ç²¾ç¡®è¡¨è¾¾ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘å…¶ä»–ç›¸å…³çš„å¯èƒ½æ€§ã€‚

å…è®¸è¿™ç§è¡Œä¸ºçš„çº¦æŸæ˜¯*æå–çº¦æŸ*ï¼Œå®ƒå…è®¸ç”¨æˆ·è¾“å…¥ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼Œå…¶ç›®çš„æ˜¯æŒ‡å¯¼ç”Ÿæˆï¼Œä½¿å¾—æœ€ç»ˆè¾“å‡ºå¿…é¡»åªåŒ…å«*å•è¯åˆ—è¡¨ä¸­çš„è‡³å°‘ä¸€ä¸ª*ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªæ··åˆä½¿ç”¨ä¸Šè¿°ä¸¤ç§çº¦æŸçš„ç¤ºä¾‹:

```
from transformers import GPT2LMHeadModel, GPT2Tokenizermodel = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")force_word = "scared"
force_flexible = ["scream", "screams", "screaming", "screamed"]force_words_ids = [
    tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,
    tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,
]starting_text = ["The soldiers", "The child"]input_ids = tokenizer(starting_text, return_tensors="pt").input_idsoutputs = model.generate(
    input_ids,
    force_words_ids=force_words_ids,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
)print("Output:\n" + 100 * '-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print(tokenizer.decode(outputs[1], skip_special_tokens=True))Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.Output:
--------------------------------------------------------------------
The soldiers, who were all scared and screaming at each other as they tried to get out of theThe child was taken to a local hospital where she screamed and scared for her life, police said.
```

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œç¬¬ä¸€ä¸ªè¾“å‡ºä½¿ç”¨äº†`"screaming"`ï¼Œç¬¬äºŒä¸ªè¾“å‡ºä½¿ç”¨äº†`"screamed"`ï¼Œä¸¤ä¸ªè¾“å‡ºéƒ½ä¸€å­—ä¸å·®åœ°ä½¿ç”¨äº†`"scared"`ã€‚ä»`["screaming", "screamed", ...]`ä¸­é€‰æ‹©çš„åˆ—è¡¨ä¸ä¸€å®šæ˜¯å•è¯å½¢å¼ï¼›è¿™å¯ä»¥æ»¡è¶³æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå•è¯åˆ—è¡¨çš„ä»»ä½•ç”¨ä¾‹ã€‚

# ä¸ºä»€ä¹ˆå¾ˆéš¾

ä½†æ˜¯ï¼Œè¿™å…¶å®æ˜¯ä¸€ä¸ªéå¸¸ä¸å°çš„é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºä»»åŠ¡è¦æ±‚æˆ‘ä»¬åœ¨æœ€ç»ˆè¾“å‡ºä¸­çš„æŸä¸ªåœ°æ–¹å¼ºåˆ¶ç”ŸæˆæŸäº›å­åºåˆ—*ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŸä¸ªç‚¹*ã€‚**

*é—®é¢˜æ˜¯æ³¢æŸæœç´¢é€ä»¤ç‰Œç”Ÿæˆåºåˆ—*ã€‚è¿™å°±äº§ç”Ÿäº†ä»¥ä¸‹é—®é¢˜:**

**![](img/7db64a76bf0978dfc65707f82684a1d9.png)**

**ä½œè€…å›¾ç‰‡**

**å¦‚æœæ‚¨æœ‰ä¸åŒéœ€æ±‚çš„å¤šä¸ªçº¦æŸï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå¦‚æœä½ æƒ³å¼ºåˆ¶çŸ­è¯­`P1` *å’Œ*ä¹Ÿæ˜¯çŸ­è¯­`P2`å‘¢ï¼Ÿå¦‚æœæ‚¨å¸Œæœ›æ¨¡å‹åœ¨è¿™ä¸¤ä¸ªçŸ­è¯­ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿå¦‚æœæˆ‘ä»¬æƒ³å¼ºåˆ¶ä½¿ç”¨çŸ­è¯­`P1`ï¼Œå¹¶ä¸”åªå¼ºåˆ¶ä½¿ç”¨çŸ­è¯­åˆ—è¡¨`[P21, P22, P23]`ä¸­çš„ä¸€ä¸ªçŸ­è¯­ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ**

**ä½†æ˜¯æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œä¸Šé¢çš„ä»£ç ç¤ºä¾‹æ¼”ç¤ºäº†ä¸Šè¿°æ‰€æœ‰æƒ…å†µçš„å¯èƒ½æ€§ï¼Œä¸ç®¡å®ƒä»¬çœ‹èµ·æ¥æœ‰å¤šä¹ˆåšä½œã€‚æ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†è§£é‡Šè¿™ä¸€åˆ‡æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚**

# **ä¼ ç»Ÿæ³¢æŸæœç´¢**

**ä»¥ä¸‹æ˜¯ä¼ ç»Ÿæ³¢æŸæœç´¢çš„ä¸€ä¸ªä¾‹å­ï¼Œæ‘˜è‡ªä¹‹å‰çš„[åšæ–‡](https://huggingface.co/blog/how-to-generate):**

**![](img/d65b60fd97a744d1f0324afa08a134a4.png)**

**å›¾ç‰‡æ¥è‡ªä¼ ç»Ÿæ³¢æŸæœç´¢ä¸Šçš„[æ‹¥æŠ±è„¸è´´å­](https://huggingface.co/blog/how-to-generate)**

**ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œæ³¢æŸæœç´¢é€šè¿‡ä¿æŒä¸€ä¸ªæ›´é•¿çš„å‡è®¾åˆ—è¡¨æ¥å·¥ä½œã€‚åœ¨ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆçš„æ¯ä¸ªå¯èƒ½æ­¥éª¤ä¸­æ˜¾ç¤ºäº†ä¸‰ä¸ªä¸‹ä¸€ä¸ªå¯èƒ½çš„ä»¤ç‰Œã€‚**

**åœ¨`num_beams=3`çš„æƒ…å†µä¸‹ï¼Œè¿™é‡Œæœ‰å¦ä¸€ç§æ–¹å¼æ¥çœ‹ä¸Šé¢ä¾‹å­çš„æ³¢æŸæœç´¢çš„ç¬¬ä¸€æ­¥:**

**![](img/7ee570c38a0fc72781c747e821b4f0a4.png)**

**ä½œè€…å›¾ç‰‡**

**æ³¢æŸæœç´¢å°†å…è®¸*è¿›ä¸€æ­¥è€ƒè™‘`"The nice"`å’Œ`"The car"`ä¸­çš„*ï¼Œè€Œä¸æ˜¯åƒè´ªå©ªæœç´¢é‚£æ ·åªé€‰æ‹©`"The dog"`ã€‚**

**åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘æˆ‘ä»¬åœ¨ä¸Šä¸€æ­¥ä¸­åˆ›å»ºçš„ä¸‰ä¸ªåˆ†æ”¯çš„ä¸‹ä¸€ä¸ªå¯èƒ½çš„ä»¤ç‰Œã€‚**

**![](img/ec25e7cb37a727c7e7e9c57d0aac8a31.png)**

**ä½œè€…å›¾ç‰‡**

**å°½ç®¡æˆ‘ä»¬æœ€ç»ˆ*è®¤ä¸º*æ¯”`num_beams`è¾“å‡ºå¤šå¾—å¤šï¼Œä½†åœ¨è¿™ä¸€æ­¥ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†å®ƒä»¬å‡å°‘åˆ°`num_beams`ã€‚æˆ‘ä»¬ä¸èƒ½ä¸€ç›´åˆ†æ”¯ä¸‹å»ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»è·Ÿè¸ªçš„`beams`çš„æ•°é‡å°†ä¼šå˜å¾—éš¾ä»¥æ§åˆ¶çš„é•¿(10 æ­¥ä¹‹åçš„ 10 ä¸ªæ³¢æŸå°†ä¼šæ˜¯ 100 äº¿ä¸ªæ³¢æŸï¼).**

**å¯¹äºç”Ÿæˆçš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ»¡è¶³ç»“æŸæ ‡å‡†ï¼Œä¾‹å¦‚ç”Ÿæˆ`<eos>`ä»¤ç‰Œæˆ–åˆ°è¾¾`max_length`ã€‚åˆ†æ”¯ã€æ’åºã€å‡å°‘å’Œé‡å¤ã€‚**

# **çº¦æŸæ³¢æŸæœç´¢**

**çº¦æŸæ³¢æŸæœç´¢è¯•å›¾é€šè¿‡*åœ¨ç”Ÿæˆçš„æ¯ä¸€æ­¥æ³¨å…¥*æœŸæœ›çš„ä»¤ç‰Œæ¥æ»¡è¶³çº¦æŸã€‚**

**å‡è®¾æˆ‘ä»¬è¯•å›¾åœ¨ç”Ÿæˆè¾“å‡ºä¸­å¼ºåˆ¶ä½¿ç”¨çŸ­è¯­`"is fast"`ã€‚**

**åœ¨ä¼ ç»Ÿçš„å°„æŸæœç´¢è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªåˆ†æ”¯æ‰¾åˆ°é¡¶éƒ¨`k`æœ€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå¹¶é™„åŠ å®ƒä»¬ä»¥ä¾›è€ƒè™‘ã€‚åœ¨å—çº¦æŸçš„è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬åšäº†åŒæ ·çš„äº‹æƒ…ï¼Œä½†æ˜¯ä¹Ÿæ·»åŠ äº†æ ‡è®°ï¼Œè¿™äº›æ ‡è®°å°†å¸¦æˆ‘ä»¬*æ›´æ¥è¿‘æ»¡è¶³æˆ‘ä»¬çš„çº¦æŸ*ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ¼”ç¤º:**

**![](img/82ef7f1cdd717bb33dee602a1090d85b.png)**

**ä½œè€…å›¾ç‰‡**

**é™¤äº†å¸¸è§çš„é«˜æ¦‚ç‡ next ä»¤ç‰Œ(å¦‚`"dog"`å’Œ`"nice"`)ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼ºåˆ¶ä½¿ç”¨ä»¤ç‰Œ`"is"`ï¼Œä»¥ä¾¿æ›´æ¥è¿‘æ»¡è¶³çº¦æŸ`"is fast"`ã€‚**

**å¯¹äºä¸‹ä¸€æ­¥ï¼Œä¸‹é¢çš„åˆ†æ”¯å€™é€‰é¡¹ä¸ä¼ ç»Ÿçš„æ³¢æŸæœç´¢åŸºæœ¬ç›¸åŒã€‚ä½†æ˜¯å’Œä¸Šé¢çš„ä¾‹å­ä¸€æ ·ï¼Œå—çº¦æŸçš„æ³¢æŸæœç´¢é€šè¿‡åœ¨æ¯ä¸ªæ–°çš„åˆ†æ”¯æ–½åŠ çº¦æŸæ¥å¢åŠ ç°æœ‰çš„å€™é€‰:**

**![](img/ff9447033486f2fbf869331e7e40150e.png)**

**ä½œè€…å›¾ç‰‡**

# **é“¶è¡Œ**

**åœ¨æˆ‘ä»¬è°ˆè®ºä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘åœ¨ä¸Šé¢çš„æ­¥éª¤ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°çš„ä¸è‰¯è¡Œä¸ºã€‚**

**å¤©çœŸåœ°åœ¨è¾“å‡ºä¸­å¼ºåˆ¶è¾“å…¥æƒ³è¦çš„çŸ­è¯­`"is fast"`çš„é—®é¢˜æ˜¯ï¼Œå¤§å¤šæ•°æ—¶å€™ï¼Œä½ ä¼šå¾—åˆ°åƒä¸Šé¢çš„`"The is fast"`è¿™æ ·æ— æ„ä¹‰çš„è¾“å‡ºã€‚è¿™å®é™…ä¸Šä½¿å¾—è¿™ä¸ªé—®é¢˜å¾ˆéš¾è§£å†³ã€‚åœ¨`huggingface/transformers`æå‡ºçš„[åŸå§‹ç‰¹æ€§è¯·æ±‚é—®é¢˜](https://github.com/huggingface/transformers/issues/14081#issuecomment-1004479944)ä¸­å¯ä»¥æ‰¾åˆ°å…³äºè§£å†³è¿™ä¸ªé—®é¢˜çš„å¤æ‚æ€§çš„æ›´æ·±å…¥çš„è®¨è®ºã€‚**

**é“¶è¡Œé€šè¿‡åœ¨æ»¡è¶³çº¦æŸå’Œåˆ›é€ åˆç†äº§å‡ºä¹‹é—´å»ºç«‹å¹³è¡¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚**

**ç¬¬ n ç»„ *n* æŒ‡çš„æ˜¯åœ¨æ»¡è¶³çº¦æŸæ–¹é¢å–å¾—äº† n *n æ­¥è¿›å±•çš„*æ¢åˆ—è¡¨ã€‚åœ¨å°†æ‰€æœ‰å¯èƒ½çš„æ³¢æŸåˆ†ç±»åˆ°å®ƒä»¬å„è‡ªçš„ç»„ä¸­ä¹‹åï¼Œæˆ‘ä»¬è¿›è¡Œå¾ªç¯é€‰æ‹©ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»åº“ 2 ä¸­é€‰æ‹©æœ€å¯èƒ½çš„è¾“å‡ºï¼Œç„¶åä»åº“ 1 ä¸­é€‰æ‹©æœ€å¯èƒ½çš„è¾“å‡ºï¼Œä»åº“ 0 ä¸­é€‰æ‹©ä¸€ä¸ªè¾“å‡ºï¼Œä»åº“ 2 ä¸­é€‰æ‹©ç¬¬äºŒä¸ªæœ€å¯èƒ½çš„è¾“å‡ºï¼Œä»åº“ 1 ä¸­é€‰æ‹©ç¬¬äºŒä¸ªæœ€å¯èƒ½çš„è¾“å‡ºï¼Œä¾æ­¤ç±»æ¨ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯`num_beams=3`ï¼Œæˆ‘ä»¬åªéœ€é‡å¤ä¸Šè¿°è¿‡ç¨‹ä¸‰æ¬¡ï¼Œå°±å¯ä»¥å¾—åˆ°`["The is fast", "The dog is", "The dog and"]`ã€‚****

*è¿™æ ·ï¼Œå³ä½¿æˆ‘ä»¬*å¼ºè¿«*æ¨¡å‹è€ƒè™‘æˆ‘ä»¬å·²ç»æ‰‹åŠ¨é™„åŠ äº†æ‰€éœ€æ ‡è®°çš„åˆ†æ”¯ï¼Œæˆ‘ä»¬ä»ç„¶è·Ÿè¸ªå…¶ä»–å¯èƒ½æ›´æœ‰æ„ä¹‰çš„é«˜æ¦‚ç‡åºåˆ—ã€‚å°½ç®¡`"The is fast"`å®Œå…¨æ»¡è¶³äº†æˆ‘ä»¬çš„çº¦æŸï¼Œä½†å®ƒå¹¶ä¸æ˜¯ä¸€ä¸ªéå¸¸æ˜æ™ºçš„çŸ­è¯­ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æœªæ¥çš„æ­¥éª¤ä¸­æœ‰`"The dog is"`å’Œ`"The dog and"`å¯ä»¥ä½¿ç”¨ï¼Œè¿™æœ‰æœ›åœ¨ä»¥åäº§ç”Ÿæ›´åˆç†çš„è¾“å‡ºã€‚*

*ä¸Šè¿°ç¤ºä¾‹çš„ç¬¬ä¸‰æ­¥æ¼”ç¤ºäº†è¿™ç§è¡Œä¸º:*

*![](img/f5c0a8a152564390691a33c5db7e76ea.png)*

*ä½œè€…å›¾ç‰‡*

*æ³¨æ„`"The is fast"`ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ ä»»ä½•çº¦æŸæ ‡è®°ï¼Œå› ä¸ºå®ƒå·²ç»å®Œæˆäº†(å³ï¼Œå·²ç»åŒ…å«äº†çŸ­è¯­`"is fast"`)ã€‚æ­¤å¤–ï¼Œè¯·æ³¨æ„åƒ`"The dog is slow"`æˆ–`"The dog is mad"`è¿™æ ·çš„å…‰æŸå®é™…ä¸Šæ˜¯åœ¨å­˜å‚¨ä½“ 0 ä¸­ï¼Œå› ä¸ºï¼Œå°½ç®¡å®ƒåŒ…æ‹¬ä»¤ç‰Œ`"is"`ï¼Œå®ƒå¿…é¡»ä»å¤´å¼€å§‹é‡æ–°ç”Ÿæˆ`"is fast"`ã€‚é€šè¿‡åœ¨`"is"`åæ·»åŠ ç±»ä¼¼äº`"slow"`çš„ä¸œè¥¿ï¼Œå®ƒæœ‰æ•ˆåœ°*é‡ç½®äº†å®ƒçš„è¿›ç¨‹*ã€‚*

*æœ€åï¼Œè¯·æ³¨æ„æˆ‘ä»¬æ˜¯å¦‚ä½•å¾—åˆ°åŒ…å«çº¦æŸçŸ­è¯­çš„åˆç†è¾“å‡ºçš„:`"The dog is fast"`ï¼*

*æˆ‘ä»¬æœ€åˆå¾ˆæ‹…å¿ƒï¼Œå› ä¸ºç›²ç›®åœ°æ·»åŠ æƒ³è¦çš„æ ‡è®°ä¼šå¯¼è‡´åƒ`"The is fast"`è¿™æ ·æ— æ„ä¹‰çš„çŸ­è¯­ã€‚ç„¶è€Œï¼Œä½¿ç”¨æ¥è‡ªé“¶è¡Œçš„å¾ªç¯é€‰æ‹©ï¼Œæˆ‘ä»¬æœ€ç»ˆéšå«åœ°æ”¾å¼ƒäº†æ— æ„ä¹‰çš„è¾“å‡ºï¼Œè€Œé€‰æ‹©äº†æ›´æœ‰æ„ä¹‰çš„è¾“å‡ºã€‚*

# *å…³äº`Constraint`ç±»å’Œè‡ªå®šä¹‰çº¦æŸçš„æ›´å¤šä¿¡æ¯*

*è§£é‡Šçš„ä¸»è¦å†…å®¹å¯ä»¥æ€»ç»“å¦‚ä¸‹ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬éƒ½ä¸æ–­åœ°çº ç¼ æ¨¡å‹æ¥è€ƒè™‘æ»¡è¶³æˆ‘ä»¬çš„çº¦æŸçš„è®°å·ï¼ŒåŒæ—¶è·Ÿè¸ªä¸æ»¡è¶³æˆ‘ä»¬çš„çº¦æŸçš„æ³¢æŸï¼Œç›´åˆ°æˆ‘ä»¬ä»¥åˆç†çš„é«˜æ¦‚ç‡ç»“æŸåŒ…å«æˆ‘ä»¬æƒ³è¦çš„çŸ­è¯­çš„åºåˆ—ã€‚*

*å› æ­¤ï¼Œè®¾è®¡è¿™ä¸ªå®ç°çš„ä¸€ä¸ªåŸåˆ™æ–¹æ³•æ˜¯å°†æ¯ä¸ªçº¦æŸè¡¨ç¤ºä¸ºä¸€ä¸ª`Constraint`å¯¹è±¡ï¼Œå…¶ç›®çš„æ˜¯è·Ÿè¸ªå…¶è¿›åº¦ï¼Œå¹¶å‘Šè¯‰æ³¢æŸæœç´¢æ¥ä¸‹æ¥è¦ç”Ÿæˆå“ªäº›ä»¤ç‰Œã€‚å°½ç®¡æˆ‘ä»¬å·²ç»ä¸º`model.generate()`æä¾›äº†å…³é”®å­—å‚æ•°`force_words_ids`ï¼Œä½†ä»¥ä¸‹æ˜¯åç«¯å®é™…å‘ç”Ÿçš„æƒ…å†µ:*

```
*from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstrainttokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")encoder_input_str = "translate English to German: How old are you?"constraints = [
    PhrasalConstraint(
        tokenizer("Sie", add_special_tokens=False).input_ids
    )
]input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids outputs = model.generate(
    input_ids,
    constraints=constraints,
    num_beams=10,
    num_return_sequences=1,
    no_repeat_ngram_size=1,
    remove_invalid_values=True,
) print("Output:\n" + 100 * '-')
print(tokenizer.decode(outputs[0], skip_special_tokens=True))Output:
--------------------------------------------------------------------
Wie alt sind Sie?*
```

*æ‚¨å¯ä»¥è‡ªå·±å®šä¹‰ä¸€ä¸ªï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°`constraints`å…³é”®å­—å‚æ•°ä¸­ï¼Œä»¥è®¾è®¡æ‚¨çš„ç‹¬ç‰¹çº¦æŸã€‚ä½ åªéœ€è¦åˆ›å»ºä¸€ä¸ª`Constraint`æŠ½è±¡æ¥å£ç±»çš„å­ç±»ï¼Œå¹¶éµå¾ªå®ƒçš„è¦æ±‚ã€‚ä½ å¯ä»¥åœ¨`Constraint`çš„å®šä¹‰ä¸­æ‰¾åˆ°æ›´å¤šä¿¡æ¯ï¼Œæ‰¾åˆ°[è¿™é‡Œ](https://github.com/huggingface/transformers/blob/master/src/transformers/generation_beam_constraints.py)ã€‚*

*ä¸€äº›ç‹¬ç‰¹çš„æƒ³æ³•(å°šæœªå®æ–½ï¼›ä¹Ÿè®¸ä½ å¯ä»¥è¯•ä¸€è¯•ï¼)åŒ…æ‹¬ç±»ä¼¼äº`OrderedConstraints`ã€`TemplateConstraints`çš„çº¦æŸï¼Œè¿™äº›çº¦æŸå¯ä»¥è¿›ä¸€æ­¥æ·»åŠ ã€‚ç›®å‰ï¼Œç”Ÿæˆæ˜¯é€šè¿‡åœ¨è¾“å‡ºä¸­çš„ä»»ä½•åœ°æ–¹åŒ…å«åºåˆ—æ¥å®Œæˆçš„ã€‚ä¾‹å¦‚ï¼Œåœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œä¸€ä¸ªåºåˆ—åŒ…å«æƒŠå“- >å°–å«ï¼Œå¦ä¸€ä¸ªåŒ…å«å°–å«- >æƒŠå“ã€‚`OrderedConstraints`å…è®¸ç”¨æˆ·æŒ‡å®šæ»¡è¶³è¿™äº›çº¦æŸçš„é¡ºåºã€‚*

*`TemplateConstraints`å¯ä»¥è€ƒè™‘æ›´é€‚åˆçš„åŠŸèƒ½ä½¿ç”¨ï¼Œç›®æ ‡å¯ä»¥æ˜¯:*

```
*starting_text = "The woman"
template = ["the", "", "School of", "", "in"]possible_outputs == [
   "The woman attended the Ross School of Business in Michigan.",
   "The woman was the administrator for the Harvard school of Business in MA."
]*
```

*æˆ–è€…:*

```
*starting_text = "The woman"
template = ["the", "", "", "University", "", "in"]possible_outputs == [
   "The woman attended the Carnegie Mellon University in Pittsburgh.",
]
impossible_outputs == [
  "The woman attended the Harvard University in MA."
]*
```

*æˆ–è€…ï¼Œå¦‚æœç”¨æˆ·ä¸å…³å¿ƒä¸¤ä¸ªå•è¯ä¹‹é—´çš„æ ‡è®°æ•°é‡ï¼Œé‚£ä¹ˆå¯ä»¥åªä½¿ç”¨`OrderedConstraint`ã€‚*

# *ç»“è®º*

*ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æ›´è¯¦ç»†çš„å®˜æ–¹åšå®¢æ–‡ç« [ã€‚](https://huggingface.co/blog/constrained-beam-search)*

*çº¦æŸæ³¢æŸæœç´¢ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§çµæ´»çš„æ–¹æ³•ï¼Œå°†å¤–éƒ¨çŸ¥è¯†å’Œéœ€æ±‚æ³¨å…¥åˆ°æ–‡æœ¬ç”Ÿæˆä¸­ã€‚ä»¥å‰ï¼Œæ²¡æœ‰ç®€å•çš„æ–¹æ³•æ¥å°†æ¨¡å‹è¯†åˆ«ä¸º 1ã€‚åŒ…æ‹¬åºåˆ—åˆ—è¡¨ï¼Œå…¶ä¸­ 2ã€‚å…¶ä¸­ä¸€äº›æ˜¯å¯é€‰çš„ï¼Œä¸€äº›ä¸æ˜¯ï¼Œä¾‹å¦‚ 3ã€‚å®ƒä»¬åœ¨åºåˆ—ä¸­çš„æŸå¤„*ç”Ÿæˆ*åœ¨å„è‡ªåˆç†çš„ä½ç½®ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ··åˆ`Constraint`å¯¹è±¡çš„ä¸åŒå­ç±»æ¥å®Œå…¨æ§åˆ¶æˆ‘ä»¬è¿™ä¸€ä»£ï¼*

*è¿™ä¸€æ–°åŠŸèƒ½ä¸»è¦åŸºäºä»¥ä¸‹è®ºæ–‡:*

*   *[å¸¦çº¦æŸæ³¢æŸæœç´¢çš„å¼•å¯¼å¼å¼€æ”¾è¯æ±‡å›¾åƒå­—å¹•](https://arxiv.org/abs/1612.00576)*
*   *[ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘çš„å…·æœ‰åŠ¨æ€æ³¢æŸåˆ†é…çš„å¿«é€Ÿè¯æ±‡çº¦æŸè§£ç ](https://arxiv.org/abs/1804.06609)*
*   *[æ”¹è¿›ç¿»è¯‘å’Œå•è¯­é‡å†™çš„è¯æ±‡çº¦æŸè§£ç ](https://aclanthology.org/N19-1090/)*
*   *[å› æœå¼•å¯¼ç”Ÿæˆ](https://arxiv.org/pdf/2107.09846.pdf)*

*å’Œä¸Šé¢çš„é‚£äº›ä¸€æ ·ï¼Œè®¸å¤šæ–°çš„ç ”ç©¶è®ºæ–‡æ­£åœ¨æ¢ç´¢ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†(ä¾‹å¦‚ï¼ŒKGsï¼ŒKBs)æ¥æŒ‡å¯¼å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¾“å‡ºçš„æ–¹æ³•ã€‚å¸Œæœ›è¿™ç§çº¦æŸæ³¢æŸæœç´¢ç‰¹æ€§æˆä¸ºå®ç°è¿™ä¸€ç›®çš„çš„å¦ä¸€ç§æœ‰æ•ˆæ–¹å¼ã€‚*
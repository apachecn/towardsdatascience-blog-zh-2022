# ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£

> åŸæ–‡ï¼š<https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8>

## å¯¹ç¨³å®šæ‰©æ•£ä¸–ç•Œçš„å…¨é¢ä»‹ç»ä½¿ç”¨[æ‹¥æŠ±è„¸](https://huggingface.co/)â€”â€”[æ‰©æ•£å™¨åº“](https://github.com/huggingface/diffusers)ä½¿ç”¨æ–‡æœ¬æç¤ºåˆ›å»ºäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒ

# 1.ä»‹ç»

ä½ å¯èƒ½å·²ç»çœ‹åˆ°äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒæœ‰æ‰€ä¸Šå‡ï¼Œè¿™æ˜¯å› ä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…´èµ·ã€‚ç¨³å®šæ‰©æ•£ç®€å•åœ°è¯´æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨ç»™å®šæ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ç”Ÿæˆå›¾åƒã€‚

![](img/bf1adc4be4aaff3fdb63da734f64c54b.png)

å›¾ 1:ç¨³å®šæ‰©æ•£æ¦‚è¿°

ä»ä¸Šé¢çš„å›¾åƒä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’ä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œæ¯”å¦‚â€œä¸€åªæˆ´ç€å¸½å­çš„ç‹—â€ï¼Œä¸€ä¸ªç¨³å®šçš„æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆä¸€ä¸ªä»£è¡¨æ–‡æœ¬çš„å›¾åƒã€‚ç›¸å½“æƒŠäººï¼

# 2.ä½¿ç”¨ğŸ¤—æ‰©æ•£å™¨åº“

ä¸ä»»ä½• python åº“ä¸€æ ·ï¼Œåœ¨è¿è¡Œå®ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦éµå¾ªç‰¹å®šçš„å®‰è£…æ­¥éª¤ï¼Œä¸‹é¢æ˜¯è¿™äº›æ­¥éª¤çš„æ¦‚è¦ã€‚

1.  **æ¥å—è®¸å¯â€”** åœ¨ä½¿ç”¨æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦å‰å¾€[æ­¤å¤„](https://huggingface.co/CompVis/stable-diffusion-v1-4)ä½¿ç”¨æ‚¨çš„æ‹¥æŠ±è„¸å¸æˆ·ç™»å½•ï¼Œç„¶åæ¥å—æ¨¡å‹è®¸å¯ï¼Œä¸‹è½½å¹¶ä½¿ç”¨ç ç ã€‚
2.  **ä»¤ç‰Œç”Ÿæˆâ€”** å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡ä½¿ç”¨æ‹¥æŠ±äººè„¸åº“ï¼Œè¿™å¬èµ·æ¥å¯èƒ½æœ‰ç‚¹å¥‡æ€ªã€‚æ‚¨éœ€è¦è½¬åˆ°è¿™é‡Œçš„[å¹¶ç”Ÿæˆä¸€ä¸ªä»¤ç‰Œ(æœ€å¥½æœ‰å†™æƒé™)æ¥ä¸‹è½½æ¨¡å‹ã€‚](https://huggingface.co/settings/tokens)

![](img/7c05eb786bb748c3979b5e44f705eeb2.png)

å›¾ 2:è®¿é—®ä»¤ç‰Œé¡µé¢

3.**å®‰è£… hugging face hub åº“å¹¶ç™»å½•â€”** ç”Ÿæˆä»¤ç‰Œåï¼Œå¤åˆ¶å®ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„ä»£ç ä¸‹è½½ hugging face hub åº“ã€‚

***æ³¨æ„â€”*** *ä¸ºäº†ç”¨ä»£ç æ­£ç¡®åœ°æ¸²æŸ“è¿™äº›å†…å®¹ï¼Œæˆ‘æ¨èä½ åœ¨è¿™é‡Œé˜…è¯»*<https://aayushmnit.com/posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html>**ã€‚**

```
*!pip install huggingface-hub==0.10.1*
```

*ç„¶åä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼Œä¸€æ—¦è¿è¡Œå®ƒï¼Œå°±ä¼šå‡ºç°ä¸€ä¸ªå°éƒ¨ä»¶ï¼Œç²˜è´´æ‚¨æ–°ç”Ÿæˆçš„ä»¤ç‰Œï¼Œç„¶åå•å‡»ç™»å½•ã€‚*

```
*from huggingface_hub import notebook_login
notebook_login()*
```

*4.I **å®‰è£…æ‰©æ•£å™¨å’Œå˜å‹å™¨åº“â€”** ä¸€æ—¦è¯¥è¿‡ç¨‹å®Œæˆï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç å®‰è£…ä¾èµ–é¡¹ã€‚è¿™å°†ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„[æ‰©æ•£å™¨](https://github.com/huggingface/diffusers)å’Œ[å˜å½¢é‡‘åˆš](https://github.com/huggingface/transformers)åº“ã€‚*

```
*!pip install -qq -U diffusers transformers*
```

*å°±è¿™æ ·ï¼Œç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½ä½¿ç”¨æ‰©æ•£å™¨åº“äº†ã€‚*

# *3.è¿è¡Œç¨³å®šçš„æ‰©æ•£â€”â€”é«˜å±‚ç®¡é“*

*ç¬¬ä¸€æ­¥æ˜¯ä»æ‰©æ•£å™¨åº“ä¸­å¯¼å…¥`StableDiffusionPipeline`ã€‚*

```
*from diffusers import StableDiffusionPipeline*
```

*ä¸‹ä¸€æ­¥æ˜¯åˆå§‹åŒ–ç®¡é“ä»¥ç”Ÿæˆå›¾åƒã€‚ç¬¬ä¸€æ¬¡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ—¶ï¼Œå®ƒä¼šå°†æ¨¡å‹ä» hugging face æ¨¡å‹ä¸­å¿ƒä¸‹è½½åˆ°æ‚¨çš„æœ¬åœ°æœºå™¨ä¸Šã€‚æ‚¨å°†éœ€è¦ä¸€å° GPU æœºå™¨æ¥è¿è¡Œè¿™æ®µä»£ç ã€‚*

```
*pipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda')*
```

*ç°åœ¨è®©æˆ‘ä»¬ä¼ é€’ä¸€ä¸ªæ–‡æœ¬æç¤ºå¹¶ç”Ÿæˆä¸€ä¸ªå›¾åƒã€‚*

```
*# Initialize a prompt
prompt = "a dog wearing hat"
# Pass the prompt in the pipeline
pipe(prompt).images[0]*
```

*![](img/54f915c62e02227ce6fa3edec5540c72.png)*

*å›¾ 3:ç”±æ‰©æ•£ç®¡é“äº§ç”Ÿçš„å›¾åƒçš„ä¾‹å­ã€‚*

# *4.äº†è§£ç¨³å®šæ‰©æ•£çš„æ ¸å¿ƒè¦ç´ *

*å¦‚ä¸Šæ‰€ç¤ºçš„æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚ç¨³å®šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸º**æ½œåœ¨æ‰©æ•£**æ¨¡å‹ã€‚ä»–ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­é¦–æ¬¡æå‡ºäº†[ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ](https://arxiv.org/abs/2112.10752)ã€‚åŸå§‹æ‰©æ•£æ¨¡å‹å¾€å¾€ä¼šæ¶ˆè€—æ›´å¤šçš„å†…å­˜ï¼Œå› æ­¤åˆ›å»ºäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨ç§°ä¸º`Latent`ç©ºé—´çš„ä½ç»´ç©ºé—´ä¸­è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ã€‚åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæ‰©æ•£æ¨¡å‹æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶è¢«é€æ­¥è®­ç»ƒåˆ°`denoise`éšæœºé«˜æ–¯å™ªå£°ï¼Œä»¥å¾—åˆ°ç»“æœï¼Œå³`image`ã€‚åœ¨`latent diffusion`ä¸­ï¼Œæ¨¡å‹è¢«è®­ç»ƒåœ¨ä¸€ä¸ªè¾ƒä½çš„ç»´åº¦ä¸ŠåšåŒæ ·çš„è¿‡ç¨‹ã€‚*

*æ½œåœ¨æ‰©æ•£æœ‰ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†*

1.  *ä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ª[å‰ªè¾‘æ–‡æœ¬ç¼–ç å™¨](https://openai.com/blog/clip/)*
2.  *è‡ªåŠ¨ç¼–ç å™¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå˜å‹è‡ªåŠ¨ç¼–ç å™¨ä¹Ÿç§°ä¸º VAE*
3.  *ä¸€ä¸ª [U å½¢ç½‘](https://arxiv.org/abs/1505.04597)*

*è®©æˆ‘ä»¬æ·±å…¥è¿™äº›ç»„ä»¶ï¼Œäº†è§£å®ƒä»¬åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç”¨é€”ã€‚æˆ‘å°†å°è¯•é€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªé˜¶æ®µæ¥è§£é‡Šè¿™äº›ç»„æˆéƒ¨åˆ†*

1.  ****åŸºç¡€çŸ¥è¯†:ä»€ä¹ˆè¿›å…¥ç»„ä»¶ï¼Œä»€ä¹ˆä»ç»„ä»¶ä¸­å‡ºæ¥***â€”â€”è¿™æ˜¯ç†è§£â€œæ•´ä¸ªæ¸¸æˆâ€çš„[è‡ªä¸Šè€Œä¸‹å­¦ä¹ æ–¹æ³•](https://www.fast.ai/posts/2016-10-08-teaching-philosophy.html)çš„ä¸€ä¸ªé‡è¦ä¸”å…³é”®çš„éƒ¨åˆ†*
2.  ****æ›´æ·±å±‚æ¬¡çš„è§£é‡Šè¿ç”¨ğŸ¤—ä»£ç ã€‚***â€”â€”è¿™ä¸€éƒ¨åˆ†å°†æä¾›å¯¹æ¨¡å‹ä½¿ç”¨ä»£ç äº§ç”Ÿä»€ä¹ˆçš„æ›´å¤šç†è§£*
3.  ****å®ƒä»¬åœ¨ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆ*** â€”è¿™å°†è®©ä½ å¯¹è¿™ç§æˆåˆ†åœ¨ç¨³å®šæ‰©æ•£è¿‡ç¨‹ä¸­çš„ä½œç”¨æœ‰ä¸€ä¸ªç›´è§‚çš„è®¤è¯†ã€‚è¿™å°†æœ‰åŠ©äºä½ å¯¹æ‰©æ•£è¿‡ç¨‹çš„ç›´è§‰*

# *5.å‰ªè¾‘æ–‡æœ¬ç¼–ç å™¨*

## *5.1 åŸºç¡€çŸ¥è¯†â€”ä»€ä¹ˆè¿›å‡ºç»„ä»¶ï¼Ÿ*

*CLIP(å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ)æ–‡æœ¬ç¼–ç å™¨å°†æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ½œåœ¨ç©ºé—´æ¥è¿‘çš„æ–‡æœ¬åµŒå…¥ï¼Œå°±åƒé€šè¿‡ CLIP æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œç¼–ç ä¸€æ ·ã€‚*

*![](img/401593f8496db0d0acc857c31e5834c0.png)*

*å›¾ 4:å‰ªè¾‘æ–‡æœ¬ç¼–ç å™¨*

## *2.2 ä½¿ç”¨æ›´æ·±å…¥çš„è§£é‡ŠğŸ¤—å¯†ç *

*ä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹éƒ½ä¸ç†è§£æ–‡æœ¬æ•°æ®ã€‚å¯¹äºä»»ä½•ç†è§£æ–‡æœ¬æ•°æ®çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éƒ½éœ€è¦å°†è¿™ä¸ªæ–‡æœ¬è½¬æ¢æˆä¿å­˜æ–‡æœ¬å«ä¹‰çš„æ•°å­—ï¼Œç§°ä¸º`embeddings`ã€‚å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—çš„è¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ã€‚ ***è®°å·åŒ–å™¨***â€”â€”å°†æ¯ä¸ªå•è¯åˆ†è§£æˆå­å•è¯ï¼Œç„¶åä½¿ç”¨æŸ¥æ‰¾è¡¨å°†å®ƒä»¬è½¬æ¢æˆæ•°å­—
2ã€‚ ***Token_To_Embedding ç¼–ç å™¨***â€”â€”å°†é‚£äº›æ•°å­—å­è¯è½¬æ¢æˆåŒ…å«è¯¥æ–‡æœ¬è¡¨ç¤ºçš„è¡¨ç¤º*

*æˆ‘ä»¬é€šè¿‡ä»£ç æ¥çœ‹ä¸€ä¸‹ã€‚æˆ‘ä»¬å°†ä»å¯¼å…¥ç›¸å…³çš„å·¥ä»¶å¼€å§‹ã€‚*

****æ³¨â€”*** *è¦ç”¨ä»£ç æ­£ç¡®åœ°æ¸²æŸ“è¿™äº›å†…å®¹ï¼Œæˆ‘æ¨èä½ åœ¨è¿™é‡Œé˜…è¯»*<https://aayushmnit.com/posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html>**ã€‚***

```
**import torch, logging
## disable warnings
logging.disable(logging.WARNING)  
## Import the CLIP artifacts 
from transformers import CLIPTextModel, CLIPTokenizer
## Initiating tokenizer and encoder.
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")**
```

**è®©æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªæç¤ºç¬¦å¹¶å¯¹å…¶è¿›è¡Œæ ‡è®°ã€‚**

```
**prompt = ["a dog wearing hat"]
tok =tokenizer(prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt") 
print(tok.input_ids.shape)
tok**
```

**![](img/9145b1c26602192c5ccf417a1f5370e5.png)**

**A `tokenizer`ä»¥å­—å…¸çš„å½¢å¼è¿”å›ä¸¤ä¸ªå¯¹è±¡-
1ã€‚`***input_ids***` -ä¸€ä¸ªå¤§å°ä¸º 1x77 çš„å¼ é‡ä½œä¸ºä¸€ä¸ªæç¤ºè¢«ä¼ é€’å¹¶å¡«å……åˆ° 77 çš„æœ€å¤§é•¿åº¦ã€‚`*49406*`æ˜¯å¼€å§‹æ ‡è®°ï¼Œ`*320*`æ˜¯ç»™äºˆå•è¯â€œaâ€çš„æ ‡è®°ï¼Œ`*1929*`æ˜¯ç»™äºˆå•è¯â€œdogâ€çš„æ ‡è®°ï¼Œ`*3309*`æ˜¯ç»™äºˆå•è¯â€œwearâ€çš„æ ‡è®°ï¼Œ`*3801*`æ˜¯ç»™äºˆå•è¯â€œhatâ€çš„æ ‡è®°ï¼Œ`*49407*`æ˜¯æ–‡æœ¬ç»“æŸæ ‡è®°ï¼Œé‡å¤ç›´åˆ°å¡«å……é•¿åº¦ä¸º 77ã€‚
2ã€‚`***attention_mask***` - `1`è¡¨ç¤ºåµŒå…¥å€¼ï¼Œ`0`è¡¨ç¤ºå¡«å……ã€‚**

```
**for token in list(tok.input_ids[0,:7]): 
    print(f"{token}:{tokenizer.convert_ids_to_tokens(int(token))}")**
```

**![](img/d832a1e05d4a4f46dff46305ee527382.png)**

**æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹`Token_To_Embedding Encoder`ï¼Œå®ƒæ¥å—ç”±è®°å·èµ‹äºˆå™¨ç”Ÿæˆçš„`input_ids`,å¹¶å°†å®ƒä»¬è½¬æ¢æˆåµŒå…¥-**

```
**emb = text_encoder(tok.input_ids.to("cuda"))[0].half()
print(f"Shape of embedding : {emb.shape}")
emb**
```

**![](img/3eac8bddd01cdaf58bc489b95e19b9fa.png)**

**æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œæ¯ä¸ªå¤§å°ä¸º 1x77 çš„æ ‡è®°åŒ–è¾“å…¥ç°åœ¨å·²ç»è¢«è½¬æ¢ä¸º 1x77x768 å½¢çŠ¶åµŒå…¥ã€‚æ‰€ä»¥ï¼Œæ¯ä¸ªå•è¯éƒ½åœ¨ä¸€ä¸ª 768 ç»´çš„ç©ºé—´ä¸­è¢«è¡¨ç°å‡ºæ¥ã€‚**

# **5.3 ä»–ä»¬åœ¨ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆ**

**ç¨³å®šæ‰©æ•£ä»…ä½¿ç”¨å‰ªè¾‘è®­ç»ƒçš„ç¼–ç å™¨æ¥å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥ã€‚è¿™æˆä¸º U-net çš„è¾“å…¥ä¹‹ä¸€ã€‚åœ¨é«˜å±‚æ¬¡ä¸Šï¼ŒCLIP ä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨æ¥åˆ›å»ºåœ¨æ½œåœ¨ç©ºé—´ä¸­ç›¸ä¼¼çš„åµŒå…¥ã€‚è¿™ç§ç›¸ä¼¼æ€§è¢«æ›´ç²¾ç¡®åœ°å®šä¹‰ä¸º[å¯¹æ¯”ç›®æ ‡](https://arxiv.org/abs/1807.03748)ã€‚å…³äº CLIP å¦‚ä½•è®­ç»ƒçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒè¿™ä¸ª[å¼€æ”¾ AI åšå®¢](https://openai.com/blog/clip/)ã€‚**

**![](img/e64fa9a7e5b65ba9413c9220057eaa11.png)**

**å›¾ 5: CLIP é¢„å…ˆè®­ç»ƒäº†ä¸€ä¸ªå›¾åƒç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥é¢„æµ‹åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­å“ªäº›å›¾åƒä¸å“ªäº›æ–‡æœ¬é…å¯¹ã€‚ä¿¡ç”¨â€” [OpenAI](https://openai.com/blog/clip/)**

# **6.VAE â€”å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨**

## **6.1 åŸºç¡€çŸ¥è¯†â€”ä»€ä¹ˆè¿›å‡ºç»„ä»¶ï¼Ÿ**

**ä¸€ä¸ªè‡ªåŠ¨ç¼–ç å™¨åŒ…å«ä¸¤éƒ¨åˆ†-
1ã€‚`Encoder`å°†å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä½ç»´æ½œåœ¨è¡¨ç¤º
2ã€‚`Decoder`è·å–æ½œåƒå¹¶å°†å…¶è½¬æ¢å›å›¾åƒ**

**![](img/291731162277ed2bacca600ba0c21169.png)**

**å›¾ 6:ä¸€ä¸ªå˜åŒ–çš„è‡ªåŠ¨ç¼–ç å™¨ã€‚åŸé¸Ÿ [pic åŠŸåŠ³](https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg)ã€‚**

**æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œç¼–ç å™¨å°±åƒä¸€ä¸ªå‹ç¼©å™¨ï¼Œå°†å›¾åƒå‹ç¼©åˆ°æ›´ä½çš„ç»´åº¦ï¼Œè§£ç å™¨ä»å‹ç¼©ç‰ˆæœ¬ä¸­é‡æ–°åˆ›å»ºåŸå§‹å›¾åƒã€‚**

> ****æ³¨æ„:**ç¼–è§£ç å‹ç¼©-è§£å‹ç¼©ä¸æ˜¯æ— æŸçš„ã€‚**

## **6.2 æ›´æ·±å…¥çš„è§£é‡Šä½¿ç”¨ğŸ¤—å¯†ç **

**è®©æˆ‘ä»¬é€šè¿‡ä»£ç æ¥çœ‹çœ‹ VAEã€‚æˆ‘ä»¬å°†ä»å¯¼å…¥æ‰€éœ€çš„åº“å’Œä¸€äº›è¾…åŠ©å‡½æ•°å¼€å§‹ã€‚**

```
**## To import an image from a URL 
from fastdownload import FastDownload  
## Imaging  library 
from PIL import Image 
from torchvision import transforms as tfms  
## Basic libraries 
import numpy as np 
import matplotlib.pyplot as plt 
%matplotlib inline  
## Loading a VAE model 
from diffusers import AutoencoderKL 
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")
def load_image(p):
   '''     
   Function to load images from a defined path     
   '''    
    return Image.open(p).convert('RGB').resize((512,512))
def pil_to_latents(image):
    '''     
    Function to convert image to latents     
    '''     
    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0   
    init_image = init_image.to(device="cuda", dtype=torch.float16)
    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215     
    return init_latent_dist  
def latents_to_pil(latents):     
    '''     
    Function to convert latents to images     
    '''     
    latents = (1 / 0.18215) * latents     
    with torch.no_grad():         
        image = vae.decode(latents).sample     

    image = (image / 2 + 0.5).clamp(0, 1)     
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()      
    images = (image * 255).round().astype("uint8")     
    pil_images = [Image.fromarray(image) for image in images]        
    return pil_images**
```

**æˆ‘ä»¬ä»ç½‘ä¸Šä¸‹è½½ä¸€å¼ å›¾ç‰‡å§ã€‚**

```
**p = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')
img = load_image(p)
print(f"Dimension of this image: {np.array(img).shape}")
img**
```

**![](img/d6cd14200b5f32360f829cc6da49f7e3.png)**

**å›¾ 7:åŸé¸Ÿ [pic ä¿¡ç”¨](https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg)ã€‚**

**ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ VAE ç¼–ç å™¨æ¥å‹ç¼©è¿™ä¸ªå›¾åƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`pil_to_latents`è¾…åŠ©å‡½æ•°ã€‚**

```
**latent_img = pil_to_latents(img)
print(f"Dimension of this latent representation: {latent_img.shape}")**
```

**![](img/367c3f231281dff5df1ce2b5617ed374.png)**

**æˆ‘ä»¬å¯ä»¥çœ‹åˆ° VAE æ˜¯å¦‚ä½•å°†ä¸€ä¸ª 3 x 512 x 512 çš„å›¾åƒå‹ç¼©æˆ 4 x 64 x 64 çš„å›¾åƒçš„ã€‚è¿™æ˜¯ 48 å€çš„å‹ç¼©æ¯”ï¼è®©æˆ‘ä»¬æƒ³è±¡è¿™å››ä¸ªæ½œåœ¨è¡¨å¾çš„æ¸ é“ã€‚**

```
**fig, axs = plt.subplots(1, 4, figsize=(16, 4))
for c in range(4):
    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')**
```

**![](img/946541b1bc03d099b25fd83c9af7b74a.png)**

**å›¾ 8:æ¥è‡ª VAE ç¼–ç å™¨çš„æ½œåœ¨è¡¨ç¤ºçš„å¯è§†åŒ–ã€‚**

**è¿™ç§æ½œåœ¨çš„è¡¨ç¤ºåœ¨ç†è®ºä¸Šåº”è¯¥æ•æ‰åˆ°å¾ˆå¤šå…³äºåŸå§‹å›¾åƒçš„ä¿¡æ¯ã€‚è®©æˆ‘ä»¬å¯¹è¿™ä¸ªè¡¨ç¤ºä½¿ç”¨è§£ç å™¨ï¼Œçœ‹çœ‹æˆ‘ä»¬å¾—åˆ°ä»€ä¹ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`latents_to_pil`åŠ©æ‰‹å‡½æ•°ã€‚**

```
**decoded_img = latents_to_pil(latent_img)
decoded_img[0]**
```

**![](img/c80ebe54bdd01ca807f4ba6ff5fa1aae.png)**

**å›¾ 9:æ¥è‡ª VAE è§£ç å™¨çš„è§£ç æ½œåœ¨è¡¨ç¤ºçš„å¯è§†åŒ–ã€‚**

**ä»ä¸Šå›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒVAE è§£ç å™¨èƒ½å¤Ÿä» 48x å‹ç¼©çš„æ½œåœ¨å›¾åƒä¸­æ¢å¤åŸå§‹å›¾åƒã€‚ä»¤äººå°è±¡æ·±åˆ»ï¼**

> ****æ³¨æ„:**å¦‚æœä½ ä»”ç»†çœ‹è§£ç å›¾åƒï¼Œå®ƒä¸åŸå§‹å›¾åƒä¸ä¸€æ ·ï¼Œæ³¨æ„çœ¼ç›å‘¨å›´çš„å·®å¼‚ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ VAE ç¼–ç å™¨/è§£ç å™¨ä¸æ˜¯æ— æŸå‹ç¼©ã€‚**

## **6.3 ä»–ä»¬åœ¨ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„è§’è‰²æ˜¯ä»€ä¹ˆ**

**ç¨³å®šçš„æ‰©æ•£å¯ä»¥åœ¨æ²¡æœ‰ VAE åˆ†é‡çš„æƒ…å†µä¸‹å®Œæˆï¼Œä½†æ˜¯æˆ‘ä»¬ä½¿ç”¨ VAE çš„åŸå› æ˜¯ä¸ºäº†å‡å°‘ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æ—¶é—´ã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨ç”± VAE ç¼–ç å™¨äº§ç”Ÿçš„è¿™ä¸ª*æ½œåœ¨ç©ºé—´*ä¸­æ‰§è¡Œæ‰©æ•£ï¼Œå¹¶ä¸”ä¸€æ—¦æˆ‘ä»¬æœ‰äº†ç”±æ‰©æ•£è¿‡ç¨‹äº§ç”Ÿçš„æˆ‘ä»¬æœŸæœ›çš„æ½œåœ¨è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ VAE è§£ç å™¨å°†å®ƒä»¬è½¬æ¢å›é«˜åˆ†è¾¨ç‡å›¾åƒã€‚ä¸ºäº†æ›´ç›´è§‚åœ°ç†è§£å˜ä½“è‡ªåŠ¨ç¼–ç å™¨ä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•è¢«è®­ç»ƒçš„ï¼Œè¯·é˜…è¯» Irhum Shafkat çš„åšå®¢ã€‚**

# **7.u ç½‘æ¨¡å‹**

## **7.1 åŸºç¡€çŸ¥è¯†â€”ä»€ä¹ˆè¿›å‡ºç»„ä»¶ï¼Ÿ**

**U-Net æ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥-
1ã€‚`Noisy latent`æˆ–`Noise` -å™ªå£°æ½œä¼æ˜¯ç”± VAE ç¼–ç å™¨(åœ¨æä¾›åˆå§‹å›¾åƒçš„æƒ…å†µä¸‹)äº§ç”Ÿçš„å…·æœ‰é™„åŠ å™ªå£°çš„æ½œä¼ï¼Œæˆ–è€…åœ¨æˆ‘ä»¬æƒ³è¦ä»…åŸºäºæ–‡æœ¬æè¿°åˆ›å»ºéšæœºæ–°å›¾åƒçš„æƒ…å†µä¸‹ï¼Œå®ƒå¯ä»¥æ¥å—çº¯å™ªå£°è¾“å…¥
2ã€‚`Text embeddings` -åŸºäºå‰ªè¾‘çš„åµŒå…¥ç”±è¾“å…¥çš„æ–‡æœ¬æç¤ºç”Ÿæˆ**

**U-Net æ¨¡å‹çš„è¾“å‡ºæ˜¯è¾“å…¥å™ªå£°æ½œåŠ¿åŒ…å«çš„é¢„æµ‹å™ªå£°æ®‹å·®ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒé¢„æµ‹ä»å™ªå£°æ½œä¼æ—¶é—´ä¸­å‡å»çš„å™ªå£°ï¼Œä»¥è¿”å›åŸå§‹çš„å»å™ªå£°æ½œä¼æ—¶é—´ã€‚**

**![](img/55f7d11c10056f784a8a2a172dd213e5.png)**

**å›¾ 10:ä¸€ä¸ª U ç½‘è¡¨ç¤ºã€‚**

## **7.2 æ›´æ·±å…¥çš„è§£é‡Šä½¿ç”¨ğŸ¤—å¯†ç **

**è®©æˆ‘ä»¬é€šè¿‡ä»£ç å¼€å§‹çœ‹ U-Netã€‚æˆ‘ä»¬å°†ä»å¯¼å…¥æ‰€éœ€çš„åº“å’Œå¯åŠ¨æˆ‘ä»¬çš„ U-Net æ¨¡å‹å¼€å§‹ã€‚**

```
**from diffusers import UNet2DConditionModel, LMSDiscreteScheduler
## Initializing a scheduler
scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)
## Setting number of sampling steps
scheduler.set_timesteps(51)
## Initializing the U-Net model
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")**
```

**æ‚¨å¯èƒ½å·²ç»ä»ä¸Šé¢çš„ä»£ç ä¸­æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬ä¸ä»…å¯¼å…¥äº†`unet`ï¼Œè¿˜å¯¼å…¥äº†`scheduler`ã€‚`schedular`çš„ç›®çš„æ˜¯ç¡®å®šåœ¨æ‰©æ•£è¿‡ç¨‹çš„ç»™å®šæ­¥éª¤ä¸­æœ‰å¤šå°‘å™ªå£°æ·»åŠ åˆ°æ½œåœ¨å™ªå£°ä¸­ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ schedular å‡½æ•°**

**![](img/852e20bb88c7491d3a15e4d41b57eada.png)**

**å›¾ 11:é‡‡æ ·è®¡åˆ’å¯è§†åŒ–ã€‚**

**æ‰©æ•£è¿‡ç¨‹éµå¾ªè¿™ä¸ªé‡‡æ ·æ—¶é—´è¡¨ï¼Œæˆ‘ä»¬ä»é«˜å™ªå£°å¼€å§‹ï¼Œå¹¶é€æ¸å¯¹å›¾åƒå»å™ªã€‚è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹-**

```
**noise = torch.randn_like(latent_img) # Random noise
fig, axs = plt.subplots(2, 3, figsize=(16, 12))
for c, sampling_step in enumerate(range(0,51,10)):
    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))
    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])
    axs[c//3][c%3].set_title(f"Step - {sampling_step}")**
```

**![](img/11b44c411c1f90ff486c80bfcfdd7736.png)**

**å›¾ 12:é€šè¿‡æ­¥éª¤çš„å™ªå£°è¿›å±•ã€‚**

**è®©æˆ‘ä»¬çœ‹çœ‹ U-Net æ˜¯å¦‚ä½•å»é™¤å›¾åƒä¸­çš„å™ªå£°çš„ã€‚è®©æˆ‘ä»¬ä»ç»™å›¾åƒæ·»åŠ ä¸€äº›å™ªå£°å¼€å§‹ã€‚**

```
**encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]])) latents_to_pil(encoded_and_noised)[0]**
```

**![](img/f7c4518900d640351a388fec8c634879.png)**

**å›¾ 13:é¦ˆå…¥ U-Net çš„å™ªå£°è¾“å…¥ã€‚**

**è®©æˆ‘ä»¬æµè§ˆä¸€ä¸‹ U-Netï¼Œè¯•ç€å»å™ªè¿™ä¸ªå›¾åƒã€‚**

```
**## Unconditional textual prompt
prompt = [""]
## Using clip model to get embeddings
text_input = tokenizer(prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")
with torch.no_grad(): 
    text_embeddings = text_encoder(
        text_input.input_ids.to("cuda")
    )[0]

## Using U-Net to predict noise    
latent_model_input = torch.cat([encoded_and_noised.to("cuda").float()]).half()
with torch.no_grad():
    noise_pred = unet(
        latent_model_input,40,encoder_hidden_states=text_embeddings
    )["sample"]
## Visualize after subtracting noise 
latents_to_pil(encoded_and_noised- noise_pred)[0]**
```

**![](img/059bca6fe75c5b5dd9a27eb8b3ed11b2.png)**

**å›¾ 14:æ¥è‡ª U-Net çš„å»å™ªå£°è¾“å‡º**

**æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼ŒU-Net çš„è¾“å‡ºæ¯”é€šè¿‡çš„åŸå§‹å™ªå£°è¾“å…¥æ›´æ¸…æ™°ã€‚**

## **7.3 ä»–ä»¬åœ¨ç¨³å®šæ‰©æ•£ç®¡é“ä¸­çš„è§’è‰²æ˜¯ä»€ä¹ˆ**

**æ½œåœ¨æ‰©æ•£ä½¿ç”¨ U-Net é€šè¿‡å‡ ä¸ªæ­¥éª¤é€æ¸å‡å»æ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°ï¼Œä»¥è¾¾åˆ°æ‰€éœ€çš„è¾“å‡ºã€‚æ¯èµ°ä¸€æ­¥ï¼Œæ·»åŠ åˆ°å»¶è¿Ÿä¸­çš„å™ªå£°é‡å°±ä¼šå‡å°‘ï¼Œç›´åˆ°æˆ‘ä»¬å¾—åˆ°æœ€ç»ˆçš„å»å™ªè¾“å‡ºã€‚u-ç½‘æœ€åˆæ˜¯ç”±[æœ¬æ–‡](https://arxiv.org/abs/1505.04597)ä»‹ç»çš„ï¼Œç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚U-Net å…·æœ‰ç”± ResNet å—ç»„æˆçš„ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ç¨³å®šæ‰©æ•£ U-Net è¿˜å…·æœ‰äº¤å‰æ³¨æ„å±‚ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ ¹æ®æ‰€æä¾›çš„æ–‡æœ¬æè¿°æ¥è°ƒèŠ‚è¾“å‡ºã€‚äº¤å‰æ³¨æ„å±‚é€šå¸¸åœ¨ ResNet å—ä¹‹é—´è¢«æ·»åŠ åˆ° U-Net çš„ç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºè¿™ä¸ª U-Net æ¶æ„[çš„ä¿¡æ¯ã€‚](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq)**

# **8.æŠŠæ‰€æœ‰ä¸œè¥¿æ”¾åœ¨ä¸€èµ·ï¼Œç†è§£æ‰©æ•£è¿‡ç¨‹**

**å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•å®‰è£…ğŸ¤—æ‰©æ•£å™¨åº“å¼€å§‹ç”Ÿæˆæ‚¨è‡ªå·±çš„äººå·¥æ™ºèƒ½å›¾åƒå’Œç¨³å®šæ‰©æ•£ç®¡é“çš„å…³é”®ç»„ä»¶ï¼Œå³å‰ªè¾‘æ–‡æœ¬ç¼–ç å™¨ï¼ŒVAE å’Œ U-Netã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†å°è¯•æŠŠè¿™äº›å…³é”®éƒ¨åˆ†æ”¾åœ¨ä¸€èµ·ï¼Œå¹¶åšä¸€ä¸ªäº§ç”Ÿå›¾åƒçš„æ‰©æ•£è¿‡ç¨‹çš„æ¼”ç»ƒã€‚**

## **8.1 æ¦‚è¿°-æ‰©æ•£è¿‡ç¨‹**

**ç¨³å®šæ‰©æ•£æ¨¡å‹é‡‡ç”¨æ–‡æœ¬è¾“å…¥å’Œç§å­ã€‚æ–‡æœ¬è¾“å…¥ç„¶åé€šè¿‡å‰ªè¾‘æ¨¡å‹ä»¥ç”Ÿæˆå¤§å°ä¸º 77Ã—768 çš„æ–‡æœ¬åµŒå…¥ï¼Œå¹¶ä¸”ç§å­ç”¨äºç”Ÿæˆå¤§å°ä¸º 4Ã—64Ã—64 çš„é«˜æ–¯å™ªå£°ï¼Œè¯¥é«˜æ–¯å™ªå£°æˆä¸ºç¬¬ä¸€æ½œåƒè¡¨ç¤ºã€‚**

> **æ³¨æ„â€”æ‚¨ä¼šæ³¨æ„åˆ°åœ¨å›¾åƒä¸­æåˆ°äº†ä¸€ä¸ªé¢å¤–çš„ç»´åº¦(1x ),å¦‚ç”¨äºæ–‡æœ¬åµŒå…¥çš„ 1x77x768ï¼Œè¿™æ˜¯å› ä¸ºå®ƒè¡¨ç¤ºæ‰¹é‡å¤§å°ä¸º 1ã€‚**

**![](img/362376c3b3cc955e6090b8888258a9d6.png)**

**å›¾ 15:æ‰©æ•£è¿‡ç¨‹ã€‚**

**æ¥ä¸‹æ¥ï¼ŒU-Net è¿­ä»£åœ°å»é™¤éšæœºæ½œåƒè¡¨ç¤ºçš„å™ªå£°ï¼ŒåŒæ—¶ä»¥æ–‡æœ¬åµŒå…¥ä¸ºæ¡ä»¶ã€‚U-Net çš„è¾“å‡ºæ˜¯é¢„æµ‹çš„å™ªå£°æ®‹å·®ï¼Œè¯¥å™ªå£°æ®‹å·®ç„¶åè¢«ç”¨äºé€šè¿‡è°ƒåº¦å™¨ç®—æ³•æ¥è®¡ç®—æ¡ä»¶å»¶è¿Ÿã€‚è¿™ä¸ªå»å™ªå’Œæ–‡æœ¬è°ƒèŠ‚çš„è¿‡ç¨‹é‡å¤ N æ¬¡(æˆ‘ä»¬å°†ä½¿ç”¨ 50 æ¬¡)ä»¥æ£€ç´¢æ›´å¥½çš„æ½œåƒè¡¨ç¤ºã€‚ä¸€æ—¦è¯¥è¿‡ç¨‹å®Œæˆï¼Œæ½œåƒè¡¨ç¤º(4x64x64)ç”± VAE è§£ç å™¨è§£ç ï¼Œä»¥æ£€ç´¢æœ€ç»ˆçš„è¾“å‡ºå›¾åƒ(3x512x512)ã€‚**

> **æ³¨æ„â€”â€”è¿™ç§è¿­ä»£å»å™ªæ˜¯è·å¾—è‰¯å¥½è¾“å‡ºå›¾åƒçš„é‡è¦æ­¥éª¤ã€‚å…¸å‹çš„æ­¥é•¿èŒƒå›´æ˜¯ 30â€“80ã€‚ç„¶è€Œï¼Œæœ‰[æœ€è¿‘çš„è®ºæ–‡](https://arxiv.org/abs/2202.00512)å£°ç§°é€šè¿‡ä½¿ç”¨è’¸é¦æŠ€æœ¯å°†å…¶å‡å°‘åˆ° 4-5 æ­¥ã€‚**

## **8.2 é€šè¿‡ä»£ç ç†è§£æ‰©æ•£è¿‡ç¨‹**

**è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€éœ€çš„åº“å’ŒåŠ©æ‰‹å‡½æ•°å¼€å§‹ã€‚ä¸Šé¢å·²ç»è§£é‡Šäº†æ‰€æœ‰è¿™äº›ã€‚**

*****æ³¨æ„â€”*** *ä¸ºäº†ç”¨ä»£ç æ­£ç¡®åœ°æ¸²æŸ“è¿™äº›å†…å®¹ï¼Œæˆ‘æ¨èä½ åœ¨è¿™é‡Œé˜…è¯»* [*ã€‚*](https://aayushmnit.com/posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html)**

```
**import torch, logging
## disable warnings
logging.disable(logging.WARNING)  
## Imaging  library
from PIL import Image
from torchvision import transforms as tfms
## Basic libraries
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
%matplotlib inline
from IPython.display import display
import shutil
import os
## For video display
from IPython.display import HTML
from base64 import b64encode

## Import the CLIP artifacts 
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler
## Initiating tokenizer and encoder.
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")
## Initiating the VAE
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")
## Initializing a scheduler and Setting number of sampling steps
scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)
scheduler.set_timesteps(50)
## Initializing the U-Net model
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")
## Helper functions
def load_image(p):
    '''
    Function to load images from a defined path
    '''
    return Image.open(p).convert('RGB').resize((512,512))
def pil_to_latents(image):
    '''
    Function to convert image to latents
    '''
    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0
    init_image = init_image.to(device="cuda", dtype=torch.float16) 
    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215
    return init_latent_dist
def latents_to_pil(latents):
    '''
    Function to convert latents to images
    '''
    latents = (1 / 0.18215) * latents
    with torch.no_grad():
        image = vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
    images = (image * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]
    return pil_images
def text_enc(prompts, maxlen=None):
    '''
    A function to take a texual promt and convert it into embeddings
    '''
    if maxlen is None: maxlen = tokenizer.model_max_length
    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") 
    return text_encoder(inp.input_ids.to("cuda"))[0].half()**
```

**ä¸‹é¢çš„ä»£ç æ˜¯`[StableDiffusionPipeline.from_pretrained](https://github.com/huggingface/diffusers/blob/269109dbfbbdbe2800535239b881e96e1828a0ef/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)`å‡½æ•°ä¸­çš„ç²¾ç®€ç‰ˆæœ¬ï¼Œæ˜¾ç¤ºäº†æ‰©æ•£è¿‡ç¨‹çš„é‡è¦éƒ¨åˆ†ã€‚**

```
**def prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):
    """
    Diffusion process to convert prompt to image
    """

    # Defining batch size
    bs = len(prompts) 

    # Converting textual prompts to embedding
    text = text_enc(prompts) 

    # Adding an unconditional prompt , helps in the generation process
    uncond =  text_enc([""] * bs, text.shape[1])
    emb = torch.cat([uncond, text])

    # Setting the seed
    if seed: torch.manual_seed(seed)

    # Initiating random noise
    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))

    # Setting number of steps in scheduler
    scheduler.set_timesteps(steps)

    # Adding noise to the latents 
    latents = latents.to("cuda").half() * scheduler.init_noise_sigma

    # Iterating through defined steps
    for i,ts in enumerate(tqdm(scheduler.timesteps)):
        # We need to scale the i/p latents to match the variance
        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)

        # Predicting noise residual using U-Net
        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)

        # Performing Guidance
        pred = u + g*(t-u)

        # Conditioning  the latents
        latents = scheduler.step(pred, ts, latents).prev_sample

        # Saving intermediate images
        if save_int: 
            if not os.path.exists(f'./steps'):
                os.mkdir(f'./steps')
            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')

    # Returning the latent representation to output an image of 3x512x512
    return latents_to_pil(latents)**
```

**è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå‡½æ•°æ˜¯å¦å¦‚é¢„æœŸçš„é‚£æ ·å·¥ä½œã€‚**

```
**images = prompt_2_img(["A dog wearing a hat", "a photograph of an astronaut riding a horse"], save_int=False)
for img in images:display(img)**
```

**![](img/e6a8a3f547d68fb7e7f05cf6325c728f.png)****![](img/6484f7b8384838451dd60696dd9507cb.png)**

**çœ‹èµ·æ¥å®ƒæ­£åœ¨å·¥ä½œï¼å› æ­¤ï¼Œè®©æˆ‘ä»¬æ›´æ·±å…¥åœ°äº†è§£è¯¥å‡½æ•°çš„è¶…å‚æ•°ã€‚
1ã€‚`prompt` -è¿™æ˜¯æˆ‘ä»¬ç”¨æ¥ç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æç¤ºã€‚ç±»ä¼¼äºæˆ‘ä»¬åœ¨ç¬¬ 1 éƒ¨åˆ†
2 ä¸­çœ‹åˆ°çš„`pipe(prompt)`å‡½æ•°ã€‚`g`æˆ–`guidance scale`â€”â€”è¿™æ˜¯ä¸€ä¸ªå†³å®šå›¾åƒåº”è¯¥å¤šæ¥è¿‘æ–‡æœ¬æç¤ºçš„å€¼ã€‚è¿™ä¸ä¸€ç§åä¸º[åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼](https://benanne.github.io/2022/05/26/guidance.html)çš„æŠ€æœ¯æœ‰å…³ï¼Œè¯¥æŠ€æœ¯æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æŒ‡å¯¼æ¯”ä¾‹å€¼è¶Šé«˜ï¼Œè¶Šæ¥è¿‘æ–‡æœ¬æç¤º
3ã€‚`seed` -è®¾ç½®ç”Ÿæˆåˆå§‹é«˜æ–¯å™ªå£°æ½œä¼æ—¶é—´çš„ç§å­
4ã€‚`steps` -ç”Ÿæˆæœ€ç»ˆå»¶è¿Ÿæ‰€é‡‡å–çš„å»å™ªæ­¥éª¤æ•°ã€‚
5ã€‚`dim` -å›¾åƒçš„å°ºå¯¸ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç›®å‰æ­£åœ¨ç”Ÿæˆæ­£æ–¹å½¢å›¾åƒï¼Œå› æ­¤åªéœ€è¦ä¸€ä¸ªå€¼
6ã€‚`save_int` -è¿™æ˜¯å¯é€‰çš„ï¼Œä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä¿å­˜ä¸­é—´æ½œåƒï¼Œæœ‰åŠ©äºå¯è§†åŒ–ã€‚**

**è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹ä»å™ªå£°åˆ°æœ€ç»ˆå›¾åƒçš„ç”Ÿæˆè¿‡ç¨‹ã€‚**

**![](img/dbbea614dfa77bacb682ed9137fe4d4a.png)**

**å›¾ 16:å»å™ªæ­¥éª¤å¯è§†åŒ–ã€‚**

# **9.ç»“è®º**

**æˆ‘å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»å®ƒï¼Œå¹¶éšæ—¶ä½¿ç”¨æˆ‘çš„ä»£ç ï¼Œå¹¶å°è¯•ç”Ÿæˆæ‚¨çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå¦‚æœå¯¹ä»£ç æˆ–åšå®¢å¸–å­æœ‰ä»»ä½•åé¦ˆï¼Œè¯·éšæ—¶è”ç³» LinkedIn æˆ–ç»™æˆ‘å‘ç”µå­é‚®ä»¶ï¼Œåœ°å€æ˜¯ aayushmnit@gmail.comã€‚**

# **10.å‚è€ƒ**

*   **[Fast.ai è¯¾ç¨‹â€”â€”ã€Šä»æ·±åº¦å­¦ä¹ åŸºç¡€åˆ°ç¨³å®šæ‰©æ•£ã€‹å‰ä¸¤èŠ‚](https://www.fast.ai/posts/part2-2022-preview.html)**
*   **[ğŸ§¨æ‰©æ•£å™¨çš„ç¨³å®šæ‰©æ•£](https://huggingface.co/blog/stable_diffusion)**
*   **[ç¨³å®šæ‰©æ•£ä¸–ç•Œå…¥é—¨](https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/)**
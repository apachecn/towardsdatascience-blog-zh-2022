# ä»ç†ŠçŒ«åˆ° PySpark çš„å¹³ç¨³è¿‡æ¸¡

> åŸæ–‡ï¼š<https://towardsdatascience.com/equivalents-between-pandas-and-pyspark-c8b5ba57dc1d>

## ç†ŠçŒ«å¤§æˆ˜ PySpark 101

ä½œè€…:[é˜¿ç›å°”Â·å“ˆæ–¯å°¼](https://medium.com/u/d38873cbc5aa?source=post_page-----40d1ab7243c2--------------------------------) & [è¿ªäºšÂ·èµ«ç±³æ‹‰](https://medium.com/u/7f47bdb8b8c0?source=post_page-----40d1ab7243c2--------------------------------)

![](img/8a29eda56a3a3ff4103de32036e263b2.png)

[æ°ç‘ç±³Â·æ‰˜é©¬æ–¯](https://unsplash.com/@jeremythomasphoto?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/s/photos/change?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹ç…§

ç†ŠçŒ«æ˜¯æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶çš„é¦–é€‰å›¾ä¹¦é¦†ã€‚å¯¹äºæ¯ä¸ªå¸Œæœ›æ“ä½œæ•°æ®å’Œæ‰§è¡Œä¸€äº›æ•°æ®åˆ†æçš„äººæ¥è¯´ï¼Œè¿™æ˜¯å¿…ä¸å¯å°‘çš„ã€‚

ç„¶è€Œï¼Œå°½ç®¡å®ƒçš„å®ç”¨æ€§å’Œå¹¿æ³›çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¾ˆå¿«å°±å¼€å§‹çœ‹åˆ°å®ƒåœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶çš„å±€é™æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿‡æ¸¡åˆ° PySpark å˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæä¾›äº†åœ¨å¤šå°æœºå™¨ä¸Šè¿è¡Œæ“ä½œçš„å¯èƒ½æ€§ï¼Œä¸åƒ Pandasã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æä¾› PySpark ä¸­ pandas æ–¹æ³•çš„ç­‰æ•ˆæ–¹æ³•ï¼Œä»¥åŠç°æˆçš„ä»£ç ç‰‡æ®µï¼Œä»¥ä¾¿äº PySpark æ–°æ‰‹å®Œæˆä»»åŠ¡ğŸ˜‰

> PySpark æä¾›äº†åœ¨å¤šå°æœºå™¨ä¸Šè¿è¡Œæ“ä½œçš„å¯èƒ½æ€§ï¼Œä¸åƒ Pandas

```
**Table Of Contents**
Â· [DataFrame creation](#8396)
Â· [Specifying columns types](#dfc7)
Â· [Reading and writing files](#7382)
Â· [Filtering](#710b)
    âˆ˜ [Specific columns](#50d3)
    âˆ˜ [Specific lines](#f04d)
    âˆ˜ [Using a condition](#e50f)
Â· [Add a column](#901e)
Â· [Concatenate dataframes](#6990)
    âˆ˜ [Two Dataframes](#0b25)
    âˆ˜ [Multiple Dataframes](#da7a)
Â· [Computing specified statistics](#a849)
Â· [Aggregations](#8b6c)
Â· [Apply a transformation over a column](#27da)
```

# å…¥é—¨æŒ‡å—

åœ¨æ·±å…¥ç ”ç©¶å¯¹ç­‰ç‰©ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸ºåé¢åšå‡†å¤‡ã€‚ä¸è¨€è€Œå–»ï¼Œç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥æ‰€éœ€çš„åº“:

```
import pandas as pd
import pyspark.sql.functions as F
```

PySpark åŠŸèƒ½çš„å…¥å£ç‚¹æ˜¯ SparkSession ç±»ã€‚é€šè¿‡ SparkSession å®ä¾‹ï¼Œæ‚¨å¯ä»¥åˆ›å»ºæ•°æ®å¸§ã€åº”ç”¨å„ç§è½¬æ¢ã€è¯»å†™æ–‡ä»¶ç­‰ã€‚è¦å®šä¹‰ SparkSessionï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å†…å®¹:

```
from pyspark.sql import SparkSession
spark = SparkSession\
.builder\
.appName('SparkByExamples.com')\
.getOrCreate()
```

ç°åœ¨ä¸€åˆ‡éƒ½å‡†å¤‡å¥½äº†ï¼Œè®©æˆ‘ä»¬ç›´æ¥è¿›å…¥ç†ŠçŒ«å¤§æˆ˜ PySpark çš„éƒ¨åˆ†å§ï¼

# æ•°æ®å¸§åˆ›å»º

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®æ ·æœ¬:

```
columns = ["employee","department","state","salary","age"]
data = [("Alain","Sales","Paris",60000,34),
        ("Ahmed","Sales","Lyon",80000,45),
        ("Ines","Sales","Nice",55000,30),
        ("Fatima","Finance","Paris",90000,28),
        ("Marie","Finance","Nantes",100000,40)]
```

è¦åˆ›å»ºä¸€åª**ç†ŠçŒ«** `DataFrame`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„:

```
df = pd.DataFrame(data=data, columns=columns)

# Show a few lines
df.head(2)
```

**PySpark**

```
df = spark.createDataFrame(data).toDF(*columns)

# Show a few lines
df.limit(2).show()
```

# æŒ‡å®šåˆ—ç±»å‹

**ç†ŠçŒ«**

```
types_dict = {
    "employee": pd.Series([r[0] for r in data], dtype='str'),
    "department": pd.Series([r[1] for r in data], dtype='str'),
    "state": pd.Series([r[2] for r in data], dtype='str'),
    "salary": pd.Series([r[3] for r in data], dtype='int'),
    "age": pd.Series([r[4] for r in data], dtype='int')
}

df = pd.DataFrame(types_dict)
```

æ‚¨å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹ä»£ç è¡Œæ¥æ£€æŸ¥æ‚¨çš„ç±»å‹:

```
df.dtypes
```

**PySpark**

```
from pyspark.sql.types import StructType,StructField, StringType, IntegerType

schema = StructType([ \
    StructField("employee",StringType(),True), \
    StructField("department",StringType(),True), \
    StructField("state",StringType(),True), \
    StructField("salary", IntegerType(), True), \
    StructField("age", IntegerType(), True) \
  ])

df = spark.createDataFrame(data=data,schema=schema)
```

æ‚¨å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ£€æŸ¥æ•°æ®å¸§çš„æ¨¡å¼:

```
df.dtypes
# OR
df.printSchema()
```

# è¯»å–å’Œå†™å…¥æ–‡ä»¶

ç†ŠçŒ«å’Œ PySpark çš„é˜…è¯»å’Œå†™ä½œæ˜¯å¦‚æ­¤çš„ç›¸ä¼¼ã€‚è¯­æ³•å¦‚ä¸‹:**ç†ŠçŒ«**

```
df = pd.read_csv(path, sep=';', header=True)
df.to_csv(path, ';', index=False)
```

**PySpark**

```
df = spark.read.csv(path, sep=';')
df.coalesce(n).write.mode('overwrite').csv(path, sep=';')
```

**æ³¨ 1ğŸ’¡:**æ‚¨å¯ä»¥æŒ‡å®šè¦å¯¹å…¶è¿›è¡Œåˆ†åŒºçš„åˆ—:

```
df.partitionBy("department","state").write.mode('overwrite').csv(path, sep=';')
```

**æ³¨ 2ğŸ’¡:**æ‚¨å¯ä»¥é€šè¿‡åœ¨ä¸Šé¢çš„æ‰€æœ‰ä»£ç è¡Œä¸­æ›´æ”¹ CSV by parquet æ¥è¯»å†™ä¸åŒçš„æ ¼å¼ï¼Œæ¯”å¦‚ parquet æ ¼å¼

# è¿‡æ»¤

## ç‰¹å®šåˆ—

é€‰æ‹©ç†ŠçŒ«ä¸­çš„æŸäº›åˆ—æ˜¯è¿™æ ·å®Œæˆçš„:**ç†ŠçŒ«**

```
columns_subset = ['employee', 'salary']

df[columns_subset].head()

df.loc[:, columns_subset].head()
```

è€Œåœ¨ PySpark ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹åˆ—åˆ—è¡¨ä½¿ç”¨ select æ–¹æ³•: **PySpark**

```
columns_subset = ['employee', 'salary']

df.select(columns_subset).show(5)
```

## ç‰¹å®šçº¿è·¯

è¦é€‰æ‹©ä¸€ç³»åˆ—çº¿æ¡ï¼Œæ‚¨å¯ä»¥åœ¨ Pandas ä¸­ä½¿ç”¨`iloc`æ–¹æ³•:

**ç†ŠçŒ«**

```
# Take a sample ( first 2 lines )

df.iloc[:2].head()
```

åœ¨ Spark ä¸­ï¼Œä¸å¯èƒ½è·å¾—ä»»ä½•èŒƒå›´çš„è¡Œå·ã€‚ç„¶è€Œï¼Œå¯ä»¥åƒè¿™æ ·é€‰æ‹©å‰ n è¡Œ:

**PySpark**

```
df.take(2).head()
# Or
df.limit(2).head()
```

**æ³¨**ğŸ’¡:è¯·è®°ä½ sparkï¼Œæ•°æ®å¯èƒ½åˆ†å¸ƒåœ¨ä¸åŒçš„è®¡ç®—èŠ‚ç‚¹ä¸Šï¼Œå¹¶ä¸”â€œç¬¬ä¸€â€è¡Œå¯èƒ½ä¼šå› è¿è¡Œè€Œå¼‚ï¼Œå› ä¸ºæ²¡æœ‰åº•å±‚é¡ºåº

## ä½¿ç”¨æ¡ä»¶

å¯ä»¥æ ¹æ®ç‰¹å®šæ¡ä»¶è¿‡æ»¤æ•°æ®ã€‚Pandas ä¸­çš„è¯­æ³•å¦‚ä¸‹:

ç†ŠçŒ«

```
# First method
flt = (df['salary'] >= 90_000) & (df['state'] == 'Paris')
filtered_df = df[flt]

# Second Method: Using query which is generally faster
filtered_df = df.query('(salary >= 90_000) and (state == "Paris")')
# Or
target_state = "Paris"
filtered_df = df.query('(salary >= 90_000) and (state == @target_state)')
```

åœ¨ Spark ä¸­ï¼Œé€šè¿‡ä½¿ç”¨`filter`æ–¹æ³•æˆ–æ‰§è¡Œ SQL æŸ¥è¯¢å¯ä»¥å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚è¯­æ³•å¦‚ä¸‹:

**PySpark**

```
# First method
filtered_df = df.filter((F.col('salary') >= 90_000) & (F.col('state') == 'Paris'))

# Second Method:
df.createOrReplaceTempView("people")

filtered_df = spark.sql("""
SELECT * FROM people
WHERE (salary >= 90000) and (state == "Paris")
""") 

# OR
filtered_df = df.filter(F.expr('(salary >= 90000) and (state == "Paris")'))
```

# æ·»åŠ åˆ—

åœ¨ Pandas ä¸­ï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥æ·»åŠ åˆ—:

**ç†ŠçŒ«**

```
seniority = [3, 5, 2, 4, 10]
# Method 1
df['seniority'] = seniority

# Method 2
df.insert(2, "seniority", seniority, True)
```

åœ¨ PySpark ä¸­æœ‰ä¸€ä¸ªå«åš`withColumn`çš„ç‰¹æ®Šæ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥æ·»åŠ ä¸€ä¸ªåˆ—:

PySpark

```
from itertools import chainseniority= {
    'Alain': 3,
    'Ahmed': 5,
    'Ines': 2,
    'Fatima': 4,
    'Marie': 10,
}mapping = create_map([lit(x) for x in chain(*seniority.items())])df.withColumn('seniority', mapping.getItem(F.col("employee")))
```

# è¿æ¥æ•°æ®å¸§

## ä¸¤ä¸ªæ•°æ®å¸§

**ç†ŠçŒ«**

```
df_to_add = pd.DataFrame(data=[("Robert","Advertisement","Paris",55000,27)], columns=columns)
df = pd.concat([df, df_to_add], ignore_index = True)
```

**PySpark**

```
df_to_add = spark.createDataFrame([("Robert","Advertisement","Paris",55000,27)]).toDF(*columns)
df = df.union(df_to_add)
```

## å¤šä¸ªæ•°æ®å¸§

ç†ŠçŒ«

```
dfs = [df, df1, df2,...,dfn]
df = pd.concat(dfs, ignore_index = True)
```

PySpark çš„æ–¹æ³•`unionAll`åªè¿æ¥äº†ä¸¤ä¸ªæ•°æ®å¸§ã€‚è§£å†³è¿™ä¸€é™åˆ¶çš„æ–¹æ³•æ˜¯æ ¹æ®éœ€è¦å¤šæ¬¡è¿­ä»£è¿æ¥ã€‚ä¸ºäº†è·å¾—æ›´ç®€æ´ä¼˜é›…çš„è¯­æ³•ï¼Œæˆ‘ä»¬å°†é¿å…å¾ªç¯ï¼Œå¹¶ä½¿ç”¨ reduce æ–¹æ³•æ¥åº”ç”¨`unionAll`:

**PySpark**

```
from functools import reduce
from pyspark.sql import DataFrame

def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)

dfs = [df, df1, df2,...,dfn]
df = unionAll(*dfs)
```

## è®¡ç®—æŒ‡å®šçš„ç»Ÿè®¡æ•°æ®

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä¸€äº›ç»Ÿè®¡ KPI æ¥æ‰§è¡Œä¸€äº›æ•°æ®åˆ†æã€‚Pandas å’Œ PySpark éƒ½æä¾›äº†éå¸¸å®¹æ˜“åœ°è·å¾—æ•°æ®å¸§ä¸­æ¯ä¸€åˆ—çš„ä»¥ä¸‹ä¿¡æ¯çš„å¯èƒ½æ€§:

*   åˆ—å…ƒç´ çš„è®¡æ•°
*   åˆ—å…ƒç´ çš„å¹³å‡å€¼
*   æ€§ä¼ æ’­ç–¾ç—…
*   æœ€å°å€¼
*   ä¸‰ä¸ªç™¾åˆ†ç‚¹:25%ã€50%å’Œ 75%
*   æœ€å¤§å€¼

æ‚¨å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹è¡Œæ¥è®¡ç®—è¿™äº›å€¼:

**ç†ŠçŒ«**å’Œ **PySpark**

```
df.summary()
#OR
df.describe() # the method describe doesn't return the percentiles
```

## èšé›†

ä¸ºäº†æ‰§è¡Œä¸€äº›èšåˆï¼Œè¯­æ³•å‡ ä¹æ˜¯ Pandas å’Œ PySpark: **Pandas**

```
df.groupby('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'})
```

**PySpark**

```
df.groupBy('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'})
```

ç„¶è€Œï¼Œç†ŠçŒ«å’Œ PySpark çš„ç»“æœéœ€è¦ä¸€äº›è°ƒæ•´æ‰èƒ½ç›¸ä¼¼ã€‚1.åœ¨ pandas ä¸­ï¼Œåˆ†ç»„ä¾æ®çš„åˆ—æˆä¸ºç´¢å¼•:

![](img/4496e70482da115672dbd07100dbd5c1.png)

è¦å°†å®ƒä½œä¸ºä¸€ä¸ªåˆ—å–å›ï¼Œæˆ‘ä»¬éœ€è¦åº”ç”¨ `reset_index`æ–¹æ³•:**ç†ŠçŒ«**

```
df.groupby('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'}).reset_index()
```

![](img/a7acd662984232b165f8d81c9523348a.png)

1.  åœ¨ **PySpark** ä¸­ï¼Œåˆ—ååœ¨ç»“æœæ•°æ®å¸§ä¸­è¢«ä¿®æ”¹ï¼Œæåˆ°äº†æ‰§è¡Œçš„èšåˆ:

![](img/f0779685fba253ad6d7d9200800f3cdb.png)

å¦‚æœæ‚¨å¸Œæœ›é¿å…è¿™ç§æƒ…å†µï¼Œæ‚¨éœ€è¦åƒè¿™æ ·ä½¿ç”¨åˆ«åæ–¹æ³•:

```
df.groupBy('department').agg(F.count('employee').alias('employee'), F.max('salary').alias('salary'), F.mean('age').alias('age'))
```

![](img/a04cc8d7a8ef8fe2ef5afb30ccb7b1a3.png)

## å¯¹åˆ—åº”ç”¨å˜æ¢

è¦å¯¹åˆ—åº”ç”¨æŸç§è½¬æ¢ï¼ŒPySpark ä¸­ä¸å†æä¾› apply æ–¹æ³•ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåä¸º`udf`(æˆ–è€…ç”¨æˆ·å®šä¹‰çš„å‡½æ•°)çš„æ–¹æ³•æ¥å°è£… python å‡½æ•°ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœå·¥èµ„ä½äº 60000 è‹±é•‘ï¼Œæˆ‘ä»¬éœ€è¦å¢åŠ  15%çš„å·¥èµ„ï¼Œå¦‚æœè¶…è¿‡ 60000 è‹±é•‘ï¼Œæˆ‘ä»¬éœ€è¦å¢åŠ  5%çš„å·¥èµ„ã€‚

pandas ä¸­çš„è¯­æ³•å¦‚ä¸‹:

```
df['new_salary'] = df['salary'].apply(lambda x: x*1.15 if x<= 60000 else x*1.05)
```

PySpark ä¸­çš„å¯¹ç­‰ç”¨æ³•å¦‚ä¸‹:

```
from pyspark.sql.types import FloatType

df.withColumn('new_salary', F.udf(lambda x: x*1.15 if x<= 60000 else x*1.05, FloatType())('salary'))
```

âš ï¸æ³¨æ„åˆ°`udf`æ–¹æ³•éœ€è¦æ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯ FloatType)

## æœ€åçš„æƒ³æ³•

æ€»ä¹‹ï¼Œå¾ˆæ˜æ˜¾ï¼ŒPandas å’Œ PySpark çš„è¯­æ³•æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚è¿™å°†æå¤§åœ°ä¿ƒè¿›ä»ä¸€ä¸ªåˆ°å¦ä¸€ä¸ªçš„è¿‡æ¸¡ã€‚

ä½¿ç”¨ PySpark åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ä¼šç»™ä½ å¸¦æ¥å¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒå…è®¸å¹¶è¡Œè®¡ç®—ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æ­£åœ¨å¤„ç†çš„æ•°æ®é›†å¾ˆå°ï¼Œé‚£ä¹ˆæ¢å¤åˆ°å”¯ä¸€çš„ç†ŠçŒ«ä¼šå¾ˆå¿«å˜å¾—æ›´æœ‰æ•ˆã€‚

å› ä¸ºè¿™ç¯‡æ–‡ç« æ˜¯å…³äºä» pandas åˆ° PySpark çš„å¹³ç¨³è¿‡æ¸¡ï¼Œæ‰€ä»¥æœ‰å¿…è¦æåˆ°ä¸€ä¸ª Pandas çš„ç­‰ä»· APIï¼Œå«åš[è€ƒæ‹‰](https://koalas.readthedocs.io/en/latest/)ï¼Œå®ƒå·¥ä½œåœ¨ Apache Spark ä¸Šï¼Œå› æ­¤å¡«è¡¥äº†ä¸¤è€…ä¹‹é—´çš„ç©ºç™½ã€‚

è°¢è°¢ä½ åšæŒåˆ°ç°åœ¨ã€‚æ³¨æ„å®‰å…¨ï¼Œä¸‹ä¸€ä¸ªæ•…äº‹å†è§ğŸ˜Šï¼

å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£å…³äº scikit-learn çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ç« :

</4-scikit-learn-tools-every-data-scientist-should-use-4ee942958d9e>  </5-hyperparameter-optimization-methods-you-should-use-521e47d7feb0> 
# åœ¨ K-means èšç±»ä¸­ä¸è¦å†ç”¨è‚˜æ³•äº†ï¼Œå–è€Œä»£ä¹‹ï¼Œç”¨è¿™ä¸ªï¼

> åŸæ–‡ï¼š<https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d>

## äº†è§£å¦‚ä½•åœ¨ K-å‡å€¼èšç±»ä¸­æŸ¥æ‰¾èšç±»æ•°

![](img/e63f16578e099928702a3ae2905cb800.png)

å›¾ç‰‡æ¥æº:Unsplash

**K-means èšç±»**æ˜¯æ•°æ®ç§‘å­¦é¢†åŸŸä½¿ç”¨æœ€å¤šçš„èšç±»ç®—æ³•ä¹‹ä¸€ã€‚ä¸ºäº†æˆåŠŸå®ç° K-means ç®—æ³•ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šä½¿ç”¨ K-means åˆ›å»ºçš„èšç±»çš„æ•°é‡ã€‚

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨â€”â€”å¯¹äºä½ çš„ K-means èšç±»ç®—æ³•æ¥è¯´ï¼Œæ‰¾åˆ°èšç±»æ•°(æˆ– K)çš„æœ€**å®ç”¨çš„æ–¹æ³•æ˜¯ä»€ä¹ˆã€‚**

**è€Œè‚˜æ³•ä¸æ˜¯ç­”æ¡ˆï¼**

ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬åšå®¢ä¸­æ¶‰åŠçš„ä¸»é¢˜:

1.  *ä»€ä¹ˆæ˜¯ K å‡å€¼èšç±»ï¼Ÿ*
2.  *ä»€ä¹ˆæ˜¯è‚˜æ³•åŠå…¶å¼Šç«¯ï¼Ÿ*
3.  *å¦‚ä½•æ±‚ K-means ä¸­â€˜Kâ€™çš„å€¼ï¼Ÿ*
4.  *Python å®ç°*
5.  *ç»“è®º*

æˆ‘ä»¬å¼€å§‹å§ï¼

## ä»€ä¹ˆæ˜¯ K-means èšç±»ï¼Ÿ

**K-means** èšç±»æ˜¯ä¸€ç§**åŸºäºè·ç¦»çš„æ— ç›‘ç£èšç±»ç®—æ³•**ï¼Œå…¶ä¸­å½¼æ­¤æ¥è¿‘çš„æ•°æ®ç‚¹è¢«åˆ†ç»„åˆ°ç»™å®šæ•°é‡çš„èšç±»/ç»„ä¸­ã€‚

ä»¥ä¸‹æ˜¯ K-means ç®—æ³•éµå¾ªçš„æ­¥éª¤:

1.  **åˆå§‹åŒ–**â€˜Kâ€™ï¼Œå³**è¦åˆ›å»ºçš„é›†ç¾¤æ•°é‡**ã€‚
2.  éšæœº**åˆ†é… K ä¸ªè´¨å¿ƒ**ç‚¹ã€‚
3.  **å°†**æ¯ä¸ª**æ•°æ®ç‚¹åˆ†é…åˆ°å…¶æœ€è¿‘çš„è´¨å¿ƒ**ä»¥åˆ›å»º K ä¸ªèšç±»ã€‚
4.  **ä½¿ç”¨æ–°åˆ›å»ºçš„èšç±»é‡æ–°è®¡ç®—è´¨å¿ƒ**ã€‚
5.  **é‡å¤æ­¥éª¤ 3 å’Œ 4** ï¼Œç›´åˆ°è´¨å¿ƒå›ºå®šã€‚

## ä»€ä¹ˆæ˜¯è‚˜æ³•åŠå…¶å¼Šç«¯ï¼Ÿ

**è‚˜æ–¹æ³•æ˜¯åœ¨ K å‡å€¼èšç±»ä¸­å¯»æ‰¾æœ€ä½³â€œKâ€**çš„å›¾å½¢è¡¨ç¤ºã€‚å…¶å·¥ä½œåŸç†æ˜¯å¯»æ‰¾ WCSS(ç±»å†…å¹³æ–¹å’Œ)ï¼Œå³ç±»ä¸­å„ç‚¹ä¸ç±»è´¨å¿ƒä¹‹é—´çš„å¹³æ–¹è·ç¦»ä¹‹å’Œã€‚

è‚˜å½¢å›¾æ˜¾ç¤ºäº†å¯¹åº”äºä¸åŒ K å€¼(åœ¨ x è½´ä¸Š)çš„ WCSS å€¼(åœ¨ y è½´ä¸Š)ã€‚å½“æˆ‘ä»¬åœ¨å›¾ä¸­çœ‹åˆ°ä¸€ä¸ª**å¼¯å¤´å½¢çŠ¶æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹©åˆ›å»ºå¼¯å¤´çš„ K å€¼**ã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªç‚¹å«åš**è‚˜ç‚¹**ã€‚è¶…è¿‡æ‹ç‚¹åï¼Œå¢åŠ â€œKâ€å€¼ä¸ä¼šå¯¼è‡´ WCSS æ˜¾è‘—é™ä½ã€‚

**è‚˜éƒ¨æ›²çº¿é¢„è®¡æ˜¯è¿™æ ·çš„ğŸ˜Š**

![](img/484e154bf8c49a0d8c1792e1971d0d7d.png)

é¢„æœŸè‚˜æ›²çº¿(å›¾ç‰‡ç”±ä½œè€…æä¾›)

**ä»€ä¹ˆæ ·å­ï¼ğŸ˜’**

![](img/c10b25d8152914f38c6f1d933c5d3eaf.png)

å®é™…è‚˜éƒ¨æ›²çº¿(å›¾ç‰‡ç”±ä½œè€…æä¾›)

å› æ­¤ï¼Œåœ¨å¤§å¤šæ•°çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸­ï¼Œä½¿ç”¨è‚˜æ–¹æ³•è¯†åˆ«æ­£ç¡®çš„â€œKâ€å¹¶ä¸ååˆ†æ¸…æ¥šã€‚

## é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•åœ¨ K-means ä¸­æ‰¾åˆ°â€œKâ€å‘¢ï¼Ÿ

åœ¨è‚˜æ³•ä¸æ˜¾ç¤ºè‚˜ç‚¹çš„æƒ…å†µä¸‹ï¼Œä¾§å½±è¯„åˆ†æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„æ±‚ K æ•°çš„æ–¹æ³•ã€‚

**å‰ªå½±å¾—åˆ†çš„å€¼èŒƒå›´ä»-1 åˆ° 1** ã€‚ä»¥ä¸‹æ˜¯å‰ªå½±é…ä¹è§£è¯»ã€‚

*   **1:** ç‚¹è¢«**å®Œç¾åœ°åˆ†é…åœ¨ä¸€ä¸ªç°‡**ä¸­ï¼Œç°‡å¾ˆå®¹æ˜“åŒºåˆ†ã€‚
*   **0:** ç°‡æ˜¯**é‡å **ã€‚
*   **-1:** ç‚¹è¢«**é”™è¯¯åˆ†é…åˆ°**ç°‡ä¸­ã€‚

![](img/27d2c732332165c0f3b366337dbbb264.png)

ä¸¤ä¸ªèšç±»çš„è½®å»“åˆ†æ•°

**å‰ªå½±å¾—åˆ†= (b-a)/max(aï¼Œb)**

å…¶ä¸­ï¼Œa=å¹³å‡ç±»å†…è·ç¦»ï¼Œå³ç±»å†…æ¯ä¸ªç‚¹**ä¹‹é—´çš„å¹³å‡**è·ç¦»**ã€‚**

b=å¹³å‡ç°‡é—´è·ç¦»ï¼Œå³æ‰€æœ‰ç°‡ä¹‹é—´çš„å¹³å‡**è·ç¦»** **ã€‚**

## Python å®ç°

è®©æˆ‘ä»¬ä½¿ç”¨è™¹è†œæ•°æ®é›†æ¥æ¯”è¾ƒ**è‚˜éƒ¨æ›²çº¿**å’Œ****è½®å»“å¾—åˆ†**ã€‚**

**ä½¿ç”¨ä»¥ä¸‹ä»£ç å¯ä»¥åˆ›å»º**å¼¯å¤´æ›²çº¿**:**

```
#install yellowbrick to vizualize the Elbow curve
!pip install yellowbrick  

from sklearn import datasets
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer

# Load the IRIS dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Instantiate the clustering model and visualizer
km = KMeans(random_state=42)
visualizer = KElbowVisualizer(km, k=(2,10))

visualizer.fit(X)        # Fit the data to the visualizer
visualizer.show()        # Finalize and render the figure
```

**![](img/4b81be79ceac953dd96cef7d270b0681.png)**

**å¼¯å¤´å›¾åœ¨ K=4 å¤„æ‰¾åˆ°å¼¯å¤´ç‚¹**

**ä¸Šå›¾åœ¨ K=4 å¤„é€‰æ‹©äº†ä¸€ä¸ªå¼¯å¤´ç‚¹ï¼Œä½†æ˜¯ K=3 çœ‹èµ·æ¥ä¹Ÿæ˜¯ä¸€ä¸ªåˆç†çš„å¼¯å¤´ç‚¹ã€‚æ‰€ä»¥ï¼Œ**ä¸æ¸…æ¥šè‚˜ç‚¹**åº”è¯¥æ˜¯ä»€ä¹ˆã€‚è®©æˆ‘ä»¬ä½¿ç”¨è½®å»“å›¾éªŒè¯ **K çš„å€¼(ä½¿ç”¨ä»¥ä¸‹ä»£ç )ã€‚****

```
from sklearn import datasets
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer

# Load the IRIS dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

fig, ax = plt.subplots(3, 2, figsize=(15,8))
for i in [2, 3, 4, 5]:
    '''
    Create KMeans instances for different number of clusters
    '''
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(X) 
```

**![](img/a5e27190984e21dcbb0f089cd8b4c21f.png)**

**K = 2 åˆ° 5 çš„è½®å»“å›¾(å›¾ç‰‡ç”±ä½œè€…æä¾›)**

**å¯¹äº K = 2ï¼Œè½®å»“åˆ†æ•°æ˜¯æœ€å¤§çš„(0.68)ï¼Œä½†æ˜¯è¿™ä¸è¶³ä»¥é€‰æ‹©æœ€ä½³çš„ Kã€‚**

**åº”æ£€æŸ¥ä»¥ä¸‹**æ¡ä»¶****ä»¥ä½¿ç”¨å‰ªå½±å›¾é€‰æ‹©æ­£ç¡®çš„â€˜Kâ€™**:**

1.  **å¯¹äºç‰¹å®šçš„ Kï¼Œ**æ‰€æœ‰çš„èšç±»éƒ½åº”è¯¥å…·æœ‰å¤§äºæ•°æ®é›†**çš„å¹³å‡åˆ†æ•°çš„è½®å»“åˆ†æ•°(ç”±çº¢è‰²è™šçº¿è¡¨ç¤º)ã€‚x è½´ä»£è¡¨è½®å»“åˆ†æ•°ã€‚K = 4 å’Œ 5 çš„èšç±»è¢«æ¶ˆé™¤ï¼Œå› ä¸ºå®ƒä»¬ä¸ç¬¦åˆè¿™ä¸ªæ¡ä»¶ã€‚**
2.  ****æ˜Ÿå›¢çš„å¤§å°ä¸åº”è¯¥æœ‰å¤§çš„æ³¢åŠ¨**ã€‚èšç±»çš„å®½åº¦ä»£è¡¨æ•°æ®ç‚¹çš„æ•°é‡ã€‚å¯¹äº K = 2ï¼Œè“è‰²ç°‡çš„å®½åº¦å‡ ä¹æ˜¯ç»¿è‰²ç°‡çš„ä¸¤å€ã€‚å¯¹äº K = 3ï¼Œè¿™ä¸ªè“è‰²ç°‡è¢«åˆ†è§£æˆ 2 ä¸ªå­ç°‡ï¼Œä»è€Œå½¢æˆå¤§å°ä¸€è‡´çš„ç°‡ã€‚**

**å› æ­¤ï¼Œ**è½®å»“å›¾æ–¹æ³•ç»™å‡ºäº† K = 3 ä½œä¸ºæœ€ä½³å€¼**ã€‚**

**å¯¹äº Iris æ•°æ®é›†ä¸Šçš„æœ€ç»ˆèšç±»ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹© K = 3ã€‚**

```
import plotly.graph_objects as go  #for 3D plot

## K-means using k = 3
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

## 3D plot 
Scene = dict(xaxis = dict(title  = 'sepal_length -->'),yaxis = dict(title  = 'sepal_width--->'),zaxis = dict(title  = 'petal_length-->'))

labels = kmeans.labels_
trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))
layout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.show()
```

**![](img/363be00d60dc1b163e340c782abf6b11.png)**

**èšç±»çš„ä¸‰ç»´å›¾(å›¾ç‰‡ç”±ä½œè€…æä¾›)**

**æˆ‘è¿˜**é€šè¿‡ç´¢å¼•/æ£€æŸ¥è¾“å…¥ç‰¹å¾**åœ¨é›†ç¾¤å†…çš„åˆ†å¸ƒæ¥éªŒè¯è¾“å‡ºé›†ç¾¤**ã€‚****

**[](https://pub.towardsai.net/expectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0)  

## ç»“è®º

è‚˜æ›²çº¿å’Œå‰ªå½±å›¾éƒ½æ˜¯ä¸º K-å‡å€¼èšç±»å¯»æ‰¾æœ€ä½³ K çš„éå¸¸æœ‰ç”¨çš„æŠ€æœ¯ã€‚åœ¨ç°å®ä¸–ç•Œçš„æ•°æ®é›†ä¸­ï¼Œä½ ä¼šå‘ç°å¾ˆå¤šè‚˜æ›²çº¿ä¸è¶³ä»¥æ‰¾åˆ°æ­£ç¡®çš„â€œKâ€çš„æƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨è½®å»“å›¾æ¥è®¡ç®—æ•°æ®é›†çš„æœ€ä½³èšç±»æ•°ã€‚

æˆ‘å»ºè®®æ‚¨ç»“åˆä½¿ç”¨è¿™ä¸¤ç§æŠ€æœ¯æ¥è®¡ç®— K å‡å€¼èšç±»çš„æœ€ä½³ K å€¼ã€‚** 

# **è°¢è°¢å¤§å®¶ï¼**

**ä½ å¯ä»¥åœ¨æ”¶ä»¶ç®±é‡Œçœ‹åˆ°æˆ‘æ‰€æœ‰çš„å¸–å­ã€‚ [***åšåˆ°è¿™é‡Œ*** *ï¼*](https://anmol3015.medium.com/subscribe) *å¦‚æœä½ å–œæ¬¢ä½“éªŒåª’ä»‹çš„è‡ªå·±ï¼Œå¯ä»¥è€ƒè™‘é€šè¿‡* [***æŠ¥åä¼šå‘˜æ¥æ”¯æŒæˆ‘å’Œå…¶ä»–æˆåƒä¸Šä¸‡çš„ä½œå®¶ã€‚å®ƒæ¯ä¸ªæœˆåªéœ€è¦ 5 ç¾å…ƒï¼Œå®ƒæå¤§åœ°æ”¯æŒäº†æˆ‘ä»¬ï¼Œä½œå®¶ï¼Œä½ å¯ä»¥åœ¨åª’ä½“ä¸Šçœ‹åˆ°æ‰€æœ‰ç²¾å½©çš„æ•…äº‹ã€‚***](https://anmol3015.medium.com/membership)**

## ****ä½ å¯èƒ½å–œæ¬¢çš„æ•…äº‹ï¼****

**[](https://medium.com/codex/feature-engineering-techniques-every-data-scientist-should-know-e40dc656c71f)  [](https://medium.com/codex/24-powerful-must-know-pandas-functions-for-every-data-analysis-a1a9990d47c8)  [](https://medium.com/codex/know-everything-about-bias-and-variance-7c7b9f9ee0ed) **
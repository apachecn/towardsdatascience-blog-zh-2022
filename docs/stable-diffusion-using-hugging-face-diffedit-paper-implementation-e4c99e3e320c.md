# ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£â€” DiffEdit çº¸å¼ å®ç°

> åŸæ–‡ï¼š<https://towardsdatascience.com/stable-diffusion-using-hugging-face-diffedit-paper-implementation-e4c99e3e320c>

DIFFEDIT çš„ä¸€ç§å®ç°:åŸºäºæ‰©æ•£çš„è¯­ä¹‰å›¾åƒç¼–è¾‘ï¼Œå…·æœ‰æ©æ¨¡å¼•å¯¼ğŸ¤—[æŠ±ç´§è„¸æ‰©æ•£å™¨åº“](https://github.com/huggingface/diffusers)ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å®ç° Meta AI å’Œç´¢é‚¦å¤§å­¦çš„ç ”ç©¶äººå‘˜æœ€è¿‘å‘è¡¨çš„ä¸€ç¯‡åä¸º`**DIFFEDIT**`çš„è®ºæ–‡ã€‚è¿™ç¯‡åšå®¢å¯¹äºé‚£äº›ç†Ÿæ‚‰ç¨³å®šæ‰©æ•£è¿‡ç¨‹æˆ–è€…æ­£åœ¨é˜…è¯»æˆ‘å†™çš„å…³äºç¨³å®šæ‰©æ•£çš„å¦å¤–ä¸¤ç¯‡åšå®¢çš„äººæ¥è¯´æ›´æœ‰æ„ä¹‰ã€‚**ç¬¬ä¸€éƒ¨åˆ†** - [ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£|ä½œè€… Aayush agr awal | 2022 å¹´ 11 æœˆ|èµ°å‘æ•°æ®ç§‘å­¦](/stable-diffusion-using-hugging-face-501d8dbdd8)
2 .**ç¬¬ 2 éƒ¨åˆ†** - [ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£-ç¨³å®šæ‰©æ•£çš„å˜åŒ–|ä½œè€… Aayush agr awal | 2022 å¹´ 11 æœˆ|èµ°å‘æ•°æ®ç§‘å­¦](/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265)

æœ€åˆï¼Œè¿™æ˜¯æˆ‘æƒ³å†™çš„åšæ–‡ï¼Œä½†æ˜¯æ„è¯†åˆ°æ²¡æœ‰ä¸€ä¸ªåœ°æ–¹å¯ä»¥ç†è§£ä»£ç çš„ç¨³å®šæ‰©æ•£ã€‚è¿™å°±æ˜¯æˆ‘æœ€ç»ˆåˆ›å»ºå…¶ä»–åšå®¢ä½œä¸ºå‚è€ƒæˆ–é¢„è¯»ææ–™æ¥ç†è§£æœ¬æ–‡çš„åŸå› ã€‚

# ä»€ä¹ˆæ˜¯ DiffEditï¼Ÿ

ç®€å•åœ°è¯´ï¼Œä½ å¯ä»¥æŠŠ`DiffEdit`æ–¹æ³•çœ‹ä½œæ˜¯`Image to Image`ç®¡é“çš„ä¸€ä¸ªæ›´å—æ§åˆ¶çš„ç‰ˆæœ¬ã€‚`DiffEdit`æ¥å—ä¸‰ä¸ªè¾“å…¥-
1ã€‚ä¸€ä¸ªè¾“å…¥å›¾åƒ
2ã€‚`Caption` -æè¿°è¾“å…¥å›¾åƒ
3ã€‚`Target Query` -æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„æ–°å›¾åƒ

å¹¶ä¸”åŸºäºæŸ¥è¯¢æ–‡æœ¬äº§ç”ŸåŸå§‹å›¾åƒçš„ä¿®æ”¹ç‰ˆæœ¬ã€‚å¦‚æœæ‚¨æƒ³å¯¹å®é™…å›¾åƒç¨ä½œè°ƒæ•´è€Œä¸å®Œå…¨ä¿®æ”¹å®ƒï¼Œè¿™ä¸ªè¿‡ç¨‹ç‰¹åˆ«å¥½ã€‚

![](img/1bbdd6877209ee6fc64b4cb726e47506.png)

å›¾ 1:å·®å¼‚ç¼–è¾‘æ¦‚è¿°ã€‚

ä»ä¸Šé¢çš„å›¾ç‰‡ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåªæœ‰å›¾ç‰‡ä¸­çš„æ°´æœéƒ¨åˆ†è¢«æ›¿æ¢æˆäº†æ¢¨ã€‚ç›¸å½“æƒŠäººçš„ç»“æœï¼

ä½œè€…è§£é‡Šè¯´ï¼Œä»–ä»¬å®ç°è¿™ä¸€ç›®æ ‡çš„æ–¹å¼æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªé®ç½©ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ç¡®å®šå›¾åƒçš„å“ªä¸€éƒ¨åˆ†åº”è¯¥è¢«ç¼–è¾‘ï¼Œç„¶ååªå¯¹è¢«é®ç½©çš„éƒ¨åˆ†æ‰§è¡ŒåŸºäºæ–‡æœ¬çš„æ‰©æ•£è°ƒèŠ‚ã€‚

![](img/a3d03abacabe2e13067247629120e915.png)

å›¾ 2:æ¥è‡ªè®ºæ–‡ [DiffEdit](https://arxiv.org/pdf/2210.11427.pdf) ã€‚ä¸€ç§é€šè¿‡æä¾›æ ‡é¢˜æ–‡æœ¬å’Œæ–°æ–‡æœ¬æ¥æ”¹å˜è¾“å…¥å›¾åƒçš„æ–¹æ³•ã€‚

ä»ä¸Šé¢å–è‡ªè®ºæ–‡çš„å›¾åƒä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä½œè€…ä»è¾“å…¥å›¾åƒä¸­åˆ›å»ºäº†ä¸€ä¸ªé®ç½©ï¼Œå®ƒå¯ä»¥å‡†ç¡®åœ°ç¡®å®šå›¾åƒä¸­å­˜åœ¨æ°´æœçš„éƒ¨åˆ†ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªé®ç½©(ä»¥æ©™è‰²æ˜¾ç¤º),ç„¶åæ‰§è¡Œé®ç½©æ‰©æ•£ä»¥ç”¨æ¢¨æ›¿æ¢æ°´æœã€‚è¿›ä¸€æ­¥é˜…è¯»ï¼Œä½œè€…æä¾›äº†æ•´ä¸ª`DiffEdit`è¿‡ç¨‹çš„ä¸€ä¸ªå¾ˆå¥½çš„å¯è§†åŒ–è¡¨ç¤ºã€‚

![](img/8438306b3f4eba121c3ae5fced0370f0.png)

å›¾ DiffEdit çš„ä¸‰ä¸ªæ­¥éª¤ã€‚[è®ºæ–‡](https://arxiv.org/pdf/2210.11427.pdf)

å½“æˆ‘é˜…è¯»è¿™ç¯‡è®ºæ–‡æ—¶ï¼Œä¼¼ä¹ç”Ÿæˆæ©è”½æ˜¯æœ€é‡è¦çš„æ­¥éª¤ï¼Œå‰©ä¸‹çš„åªæ˜¯ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹çš„æ–‡æœ¬æ¡ä»¶ã€‚ä½¿ç”¨è’™ç‰ˆå¯¹å›¾åƒè¿›è¡Œè°ƒèŠ‚çš„æƒ³æ³•ä¸[æ‹¥æŠ±é¢éƒ¨ç”»ä¸­ç”»ç®¡é“](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py)ä¸­å®ç°çš„æƒ³æ³•ç±»ä¼¼ã€‚æ­£å¦‚ä½œè€…ä»¬æ‰€å»ºè®®çš„ï¼Œâ€œè¿™ä¸ª`DiffEdit`è¿‡ç¨‹åˆ†ä¸‰æ­¥â€”â€”
**ç¬¬ä¸€æ­¥:**ç»™è¾“å…¥å›¾åƒåŠ å™ªå£°ï¼Œå»å™ª:ä¸€æ¬¡æ¡ä»¶åŒ–åœ¨æŸ¥è¯¢æ–‡æœ¬ä¸Šï¼Œä¸€æ¬¡æ¡ä»¶åŒ–åœ¨å‚è€ƒæ–‡æœ¬ä¸Š(æˆ–è€…æ— æ¡ä»¶)ã€‚æˆ‘ä»¬åŸºäºå»å™ªç»“æœçš„å·®å¼‚æ¥å¯¼å‡ºæ©æ¨¡ã€‚

åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹åœ¨å®é™…çš„ä»£ç ä¸­å®ç°è¿™äº›æƒ³æ³•ã€‚

è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€éœ€çš„åº“å’ŒåŠ©æ‰‹å‡½æ•°å¼€å§‹ã€‚æ‰€æœ‰è¿™äº›éƒ½å·²ç»åœ¨ç¨³å®šæ‰©æ•£ç³»åˆ—çš„å‰[ç¬¬ 1 éƒ¨åˆ†](/stable-diffusion-using-hugging-face-501d8dbdd8)å’Œ[ç¬¬ 2 éƒ¨åˆ†](/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265)ä¸­ä½¿ç”¨å’Œè§£é‡Šè¿‡ã€‚

```
import torch, logging

## disable warnings
logging.disable(logging.WARNING)  

## Imaging  library
from PIL import Image
from torchvision import transforms as tfms

## Basic libraries
from fastdownload import FastDownload
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
%matplotlib inline
from IPython.display import display
import shutil
import os

## For video display
from IPython.display import HTML
from base64 import b64encode

## Import the CLIP artifacts 
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler

## Helper functions

def load_artifacts():
    '''
    A function to load all diffusion artifacts
    '''
    vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")
    unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")
    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)    
    return vae, unet, tokenizer, text_encoder, scheduler

def load_image(p):
    '''
    Function to load images from a defined path
    '''
    return Image.open(p).convert('RGB').resize((512,512))

def pil_to_latents(image):
    '''
    Function to convert image to latents
    '''
    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0
    init_image = init_image.to(device="cuda", dtype=torch.float16) 
    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215
    return init_latent_dist

def latents_to_pil(latents):
    '''
    Function to convert latents to images
    '''
    latents = (1 / 0.18215) * latents
    with torch.no_grad():
        image = vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
    images = (image * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]
    return pil_images

def text_enc(prompts, maxlen=None):
    '''
    A function to take a texual promt and convert it into embeddings
    '''
    if maxlen is None: maxlen = tokenizer.model_max_length
    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") 
    return text_encoder(inp.input_ids.to("cuda"))[0].half()

vae, unet, tokenizer, text_encoder, scheduler = load_artifacts()
```

è®©æˆ‘ä»¬ä¹Ÿä¸‹è½½ä¸€ä¸ªå›¾åƒï¼Œæˆ‘ä»¬å°†ç”¨äºä»£ç å®ç°è¿‡ç¨‹ã€‚

```
p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')
init_img = load_image(p)
init_img
```

![](img/89268ca2fbe17090e9fe9ecaa85da8a1.png)

# 2 DiffEdit:çº¯ç²¹çš„å®ç°

è®©æˆ‘ä»¬ä»æŒ‰ç…§ä½œè€…çš„å»ºè®®å®ç°è¿™ç¯‡è®ºæ–‡å¼€å§‹ï¼Œå› æ­¤æ˜¯çº¯ç²¹çš„å®ç°ã€‚

# 2.1 é®ç½©åˆ›å»º:DiffEdit è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥

![](img/a382e1dc0624fff1ec4639e610d165ab.png)

å›¾ 4:`DiffEdit`è®ºæ–‡çš„ç¬¬ä¸€æ­¥ã€‚ä¿¡ç”¨â€” [è®ºæ–‡](https://arxiv.org/pdf/2210.11427.pdf)

è®ºæ–‡ä¸­æœ‰å¯¹æ­¥éª¤ 1 æ›´è¯¦ç»†çš„è§£é‡Šï¼Œä¸‹é¢æ˜¯æåˆ°çš„å…³é”®éƒ¨åˆ†â€”â€”
1ã€‚ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æ¡ä»¶å¯¹å›¾åƒå»å™ªï¼Œä¸€ä¸ªä½¿ç”¨å‚è€ƒæ–‡æœ¬ï¼Œå¦ä¸€ä¸ªä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬ï¼Œå¹¶ä»ç»“æœä¸­å–å·®ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯åœ¨ä¸åŒçš„éƒ¨åˆ†æœ‰æ›´å¤šçš„å˜åŒ–ï¼Œè€Œä¸æ˜¯åœ¨å›¾åƒçš„èƒŒæ™¯ä¸­ã€‚
2ã€‚é‡å¤æ­¤å·®åˆ†è¿‡ç¨‹ 10 æ¬¡
3ã€‚å¹³å‡è¿™äº›å·®å¼‚ï¼Œå¹¶å¯¹é®ç½©è¿›è¡ŒäºŒå€¼åŒ–

> æ³¨æ„â€”â€”è’™ç‰ˆåˆ›å»ºçš„ç¬¬ä¸‰æ­¥(å¹³å‡å’ŒäºŒå€¼åŒ–)åœ¨æ–‡ç« ä¸­æ²¡æœ‰è§£é‡Šæ¸…æ¥šï¼Œæˆ‘åšäº†å¾ˆå¤šå®éªŒæ‰å¼„æ¸…æ¥šã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å°†å°è¯•å®Œå…¨æŒ‰ç…§æ‰€æåˆ°çš„æ¥å®ç°è¿™ç¯‡è®ºæ–‡ã€‚æˆ‘ä»¬å°†ä¸ºæ­¤ä»»åŠ¡ä¿®æ”¹ [prompt_2_img_i2i](https://aayushmnit.com/posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline) å‡½æ•°ï¼Œä»¥è¿”å› latentsï¼Œè€Œä¸æ˜¯é‡æ–°ç¼©æ”¾å’Œè§£ç çš„å»å™ªå›¾åƒã€‚

```
def prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512):
    """
    Diffusion process to convert prompt to image
    """
    # Converting textual prompts to embedding
    text = text_enc(prompts) 

    # Adding an unconditional prompt , helps in the generation process
    if not neg_prompts: uncond =  text_enc([""], text.shape[1])
    else: uncond =  text_enc(neg_prompt, text.shape[1])
    emb = torch.cat([uncond, text])

    # Setting the seed
    if seed: torch.manual_seed(seed)

    # Setting number of steps in scheduler
    scheduler.set_timesteps(steps)

    # Convert the seed image to latent
    init_latents = pil_to_latents(init_img)

    # Figuring initial time step based on strength
    init_timestep = int(steps * strength) 
    timesteps = scheduler.timesteps[-init_timestep]
    timesteps = torch.tensor([timesteps], device="cuda")

    # Adding noise to the latents 
    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)
    init_latents = scheduler.add_noise(init_latents, noise, timesteps)
    latents = init_latents

    # Computing the timestep to start the diffusion loop
    t_start = max(steps - init_timestep, 0)
    timesteps = scheduler.timesteps[t_start:].to("cuda")

    # Iterating through defined steps
    for i,ts in enumerate(tqdm(timesteps)):
        # We need to scale the i/p latents to match the variance
        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)

        # Predicting noise residual using U-Net
        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)

        # Performing Guidance
        pred = u + g*(t-u)

        # Conditioning  the latents
        #latents = scheduler.step(pred, ts, latents).pred_original_sample
        latents = scheduler.step(pred, ts, latents).prev_sample

    # Returning the latent representation to output an array of 4x64x64
    return latents.detach().cpu()
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ¶ä½œä¸€ä¸ª`create_mask`å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†è·å–ä¸€ä¸ªåˆå§‹å›¾åƒã€å¼•ç”¨æç¤ºå’Œå¸¦æœ‰æˆ‘ä»¬éœ€è¦é‡å¤è¿™äº›æ­¥éª¤çš„æ¬¡æ•°çš„æŸ¥è¯¢æç¤ºã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…å»ºè®® n=10ï¼Œå¼ºåº¦ä¸º 0.5ï¼Œåœ¨ä»–ä»¬çš„å®éªŒä¸­æ•ˆæœå¾ˆå¥½ã€‚å› æ­¤ï¼Œè¯¥å‡½æ•°çš„é»˜è®¤å€¼è¢«è°ƒæ•´ä¸ºã€‚`create_mask`åŠŸèƒ½æ‰§è¡Œä»¥ä¸‹æ­¥éª¤-
1ã€‚åˆ›å»ºä¸¤ä¸ªå»å™ªçš„æ½œåœ¨å€¼ï¼Œä¸€ä¸ªä»¥å‚è€ƒæ–‡æœ¬ä¸ºæ¡ä»¶ï¼Œå¦ä¸€ä¸ªä»¥æŸ¥è¯¢æ–‡æœ¬ä¸ºæ¡ä»¶ï¼Œå–è¿™ä¸¤ä¸ªæ½œåœ¨å€¼çš„å·®
2ã€‚é‡å¤æ­¤æ­¥éª¤ n æ¬¡
3ã€‚å–è¿™äº›å·®å¼‚çš„å¹³å‡å€¼å¹¶æ ‡å‡†åŒ–
4ã€‚é€‰æ‹©é˜ˆå€¼ 0.5 è¿›è¡ŒäºŒå€¼åŒ–å¹¶åˆ›å»ºä¸€ä¸ªé®ç½©

```
def create_mask(init_img, rp, qp, n=10, s=0.5):
    ## Initialize a dictionary to save n iterations
    diff = {}

    ## Repeating the difference process n times
    for idx in range(n):
        ## Creating denoised sample using reference / original text
        orig_noise = prompt_2_img_i2i(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]
        ## Creating denoised sample using query / target text
        query_noise = prompt_2_img_i2i(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]
        ## Taking the difference 
        diff[idx] = (np.array(orig_noise)-np.array(query_noise))

    ## Creating a mask placeholder
    mask = np.zeros_like(diff[0])

    ## Taking an average of 10 iterations
    for idx in range(n):
        ## Note np.abs is a key step
        mask += np.abs(diff[idx])  

    ## Averaging multiple channels 
    mask = mask.mean(0)

    ## Normalizing 
    mask = (mask - mask.mean()) / np.std(mask)

    ## Binarizing and returning the mask object
    return (mask > 0).astype("uint8")

mask = create_mask(init_img=init_img, rp=["a horse image"], qp=["a zebra image"], n=10)
```

è®©æˆ‘ä»¬åœ¨å›¾åƒä¸Šå¯è§†åŒ–ç”Ÿæˆçš„é®ç½©ã€‚

```
plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'
plt.imshow(
    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size
    cmap='cividis', 
    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  
)
```

![](img/26188ac0b192988e7f6bd1305a6d7b06.png)

å›¾ 5:æˆ‘ä»¬çš„é©¬å›¾åƒçš„æ©è”½å¯è§†åŒ–ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œåˆ¶ä½œçš„é¢å…·å¾ˆå¥½çš„è¦†ç›–äº†é©¬çš„éƒ¨åˆ†ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚

# 2.2 æ©è”½æ‰©æ•£:DiffEdit è®ºæ–‡çš„æ­¥éª¤ 2 å’Œ 3ã€‚

![](img/58c77afa23523bd9269b9b9826d1ee7e.png)

å›¾ 6:`DiffEdit`è®ºæ–‡ä¸­çš„ç¬¬äºŒæ­¥å’Œç¬¬ä¸‰æ­¥ã€‚ä¿¡ç”¨â€” [è®ºæ–‡](https://arxiv.org/pdf/2210.11427.pdf)

æ­¥éª¤ 2 å’Œ 3 éœ€è¦åœ¨åŒä¸€ä¸ªå¾ªç¯ä¸­å®ç°ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä½œè€…æ˜¯è¯´æ ¹æ®éå±è”½éƒ¨åˆ†çš„å‚è€ƒæ–‡æœ¬å’Œå±è”½éƒ¨åˆ†çš„æŸ¥è¯¢æ–‡æœ¬æ¥è°ƒèŠ‚æ½œåœ¨äº‹ä»¶ã€‚
ä½¿ç”¨è¿™ä¸ªç®€å•çš„å…¬å¼å°†è¿™ä¸¤ä¸ªéƒ¨åˆ†ç»„åˆèµ·æ¥ï¼Œä»¥åˆ›å»ºç»„åˆçš„æ½œåœ¨å®¢æˆ·-

![](img/cf632b532fa905e82c4d36f424b2dacb.png)

```
def prompt_2_img_diffedit(rp, qp, init_img, mask, g=7.5, seed=100, strength =0.7, steps=70, dim=512):
    """
    Diffusion process to convert prompt to image
    """
    # Converting textual prompts to embedding
    rtext = text_enc(rp) 
    qtext = text_enc(qp)

    # Adding an unconditional prompt , helps in the generation process
    uncond =  text_enc([""], rtext.shape[1])
    emb = torch.cat([uncond, rtext, qtext])

    # Setting the seed
    if seed: torch.manual_seed(seed)

    # Setting number of steps in scheduler
    scheduler.set_timesteps(steps)

    # Convert the seed image to latent
    init_latents = pil_to_latents(init_img)

    # Figuring initial time step based on strength
    init_timestep = int(steps * strength) 
    timesteps = scheduler.timesteps[-init_timestep]
    timesteps = torch.tensor([timesteps], device="cuda")

    # Adding noise to the latents 
    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)
    init_latents = scheduler.add_noise(init_latents, noise, timesteps)
    latents = init_latents

    # Computing the timestep to start the diffusion loop
    t_start = max(steps - init_timestep, 0)
    timesteps = scheduler.timesteps[t_start:].to("cuda")

    # Converting mask to torch tensor
    mask = torch.tensor(mask, dtype=unet.dtype).unsqueeze(0).unsqueeze(0).to("cuda")

    # Iterating through defined steps
    for i,ts in enumerate(tqdm(timesteps)):
        # We need to scale the i/p latents to match the variance
        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)

        # Predicting noise residual using U-Net
        with torch.no_grad(): u, rt, qt = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)

        # Performing Guidance
        rpred = u + g*(rt-u)
        qpred = u + g*(qt-u)

        # Conditioning  the latents
        rlatents = scheduler.step(rpred, ts, latents).prev_sample
        qlatents = scheduler.step(qpred, ts, latents).prev_sample
        latents = mask*qlatents + (1-mask)*rlatents

    # Returning the latent representation to output an array of 4x64x64
    return latents_to_pil(latents)
```

è®©æˆ‘ä»¬å°†ç”Ÿæˆçš„å›¾åƒå¯è§†åŒ–

```
output = prompt_2_img_diffedit(
    rp = ["a horse image"], 
    qp=["a zebra image"],
    init_img=init_img, 
    mask = mask, 
    g=7.5, seed=100, strength =0.5, steps=70, dim=512)

## Plotting side by side
fig, axs = plt.subplots(1, 2, figsize=(12, 6))
for c, img in enumerate([init_img, output[0]]): 
    axs[c].imshow(img)
    if c == 0 : axs[c].set_title(f"Initial image ")
    else: axs[c].set_title(f"DiffEdit output")
```

![](img/bfa1c3e48b34ce1aed500b8e0a66a506.png)

å›¾ 7: DiffEdit è¾“å‡ºå¯è§†åŒ–

è®©æˆ‘ä»¬ä¸ºé®ç½©å’Œæ‰©æ•£è¿‡ç¨‹åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚

```
def diffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=70, dim=512):

    ## Step 1: Create mask
    mask = create_mask(init_img=init_img, rp=rp, qp=qp)

    ## Step 2 and 3: Diffusion process using mask
    output = prompt_2_img_diffedit(
        rp = rp, 
        qp=qp, 
        init_img=init_img, 
        mask = mask, 
        g=g, 
        seed=seed,
        strength =strength, 
        steps=steps, 
        dim=dim)
    return mask , output
```

è®©æˆ‘ä»¬ä¹Ÿä¸º`DiffEdit`åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–å‡½æ•°ï¼Œæ˜¾ç¤ºåŸå§‹è¾“å…¥å›¾åƒã€å±è”½å›¾åƒå’Œæœ€ç»ˆè¾“å‡ºå›¾åƒã€‚

```
def plot_diffEdit(init_img, output, mask):
    ## Plotting side by side
    fig, axs = plt.subplots(1, 3, figsize=(12, 6))

    ## Visualizing initial image
    axs[0].imshow(init_img)
    axs[0].set_title(f"Initial image")

    ## Visualizing initial image
    axs[2].imshow(output[0])
    axs[2].set_title(f"DiffEdit output")

    ## Visualizing the mask 
    axs[1].imshow(np.array(init_img), cmap='gray') 
    axs[1].imshow(
        Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size
        cmap='cividis', 
        alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  
    )
    axs[1].set_title(f"DiffEdit mask")
```

è®©æˆ‘ä»¬åœ¨ä¸€äº›å›¾åƒä¸Šæµ‹è¯•è¿™ä¸ªå‡½æ•°ã€‚

```
p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')
init_img = load_image(p)
mask, output = diffEdit(
  init_img, 
  rp = ["a horse image"], 
  qp=["a zebra image"]
)
plot_diffEdit(init_img, output, mask)
```

![](img/a49aaa1cc580edc4e466053831aadd5d.png)

å›¾ 8: Purist å®ç°è¾“å‡ºç¤ºä¾‹

å¤ªå¥½äº†ï¼Œè®©æˆ‘ä»¬è¯•è¯•å¦ä¸€ä¸ªã€‚

```
p = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')
init_img = load_image(p)
mask, output = diffEdit(
  init_img, 
  rp = ['Bowl of Strawberries'], 
  qp=['Bowl of Grapes']
)
plot_diffEdit(init_img, output, mask)
```

![](img/7fa0afcd31383e4919c73f64cd764d36.png)

å›¾ 9: Purist å®ç°è¾“å‡ºç¤ºä¾‹

# 3 FastDiffEdit:ä¸€ä¸ªæ›´å¿«çš„ DiffEdit å®ç°

ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°äº† purist çš„å®ç°ï¼Œæˆ‘å»ºè®®æˆ‘ä»¬å¯ä»¥åœ¨é€Ÿåº¦å’Œæ›´å¥½çš„ç»“æœæ–¹é¢å¯¹åŸå§‹çš„ DiffEdit è¿‡ç¨‹è¿›è¡Œä¸€äº›æ”¹è¿›ã€‚æˆ‘ä»¬å§‘ä¸”ç§°è¿™äº›æ”¹è¿›ä¸º`FastDiffEdit`ã€‚

# 3.1 é®ç½©åˆ›å»º:å¿«é€Ÿ DiffEdit é®ç½©è¿‡ç¨‹

æˆ‘å¯¹å½“å‰çš„è’™ç‰ˆæ–¹å¼æœ€å¤§çš„é—®é¢˜æ˜¯å®ƒå¤ªèŠ±æ—¶é—´äº†(åœ¨ 4500 GPU ä¸Šå¤§çº¦ 50 ç§’)ã€‚æˆ‘çš„è§‚ç‚¹æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿è¡Œä¸€ä¸ªå®Œæ•´çš„æ‰©æ•£å¾ªç¯æ¥å¯¹å›¾åƒè¿›è¡Œé™å™ªï¼Œè€Œåªéœ€åœ¨ä¸€æ¬¡æ‹æ‘„ä¸­ä½¿ç”¨åŸå§‹æ ·æœ¬çš„ U-net é¢„æµ‹ï¼Œå¹¶å°†é‡å¤æ¬¡æ•°å¢åŠ åˆ° 20 æ¬¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†è®¡ç®—ä» 10*25 = 250 æ­¥æé«˜åˆ° 20 æ­¥(å°‘ 12x ä¸ªå¾ªç¯)ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦è¡Œå¾—é€šã€‚

```
def prompt_2_img_i2i_fast(prompts, init_img, g=7.5, seed=100, strength =0.5, steps=50, dim=512):
    """
    Diffusion process to convert prompt to image
    """
    # Converting textual prompts to embedding
    text = text_enc(prompts) 

    # Adding an unconditional prompt , helps in the generation process
    uncond =  text_enc([""], text.shape[1])
    emb = torch.cat([uncond, text])

    # Setting the seed
    if seed: torch.manual_seed(seed)

    # Setting number of steps in scheduler
    scheduler.set_timesteps(steps)

    # Convert the seed image to latent
    init_latents = pil_to_latents(init_img)

    # Figuring initial time step based on strength
    init_timestep = int(steps * strength) 
    timesteps = scheduler.timesteps[-init_timestep]
    timesteps = torch.tensor([timesteps], device="cuda")

    # Adding noise to the latents 
    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)
    init_latents = scheduler.add_noise(init_latents, noise, timesteps)
    latents = init_latents

    # We need to scale the i/p latents to match the variance
    inp = scheduler.scale_model_input(torch.cat([latents] * 2), timesteps)
    # Predicting noise residual using U-Net
    with torch.no_grad(): u,t = unet(inp, timesteps, encoder_hidden_states=emb).sample.chunk(2)

    # Performing Guidance
    pred = u + g*(t-u)

    # Zero shot prediction
    latents = scheduler.step(pred, timesteps, latents).pred_original_sample

    # Returning the latent representation to output an array of 4x64x64
    return latents.detach().cpu()
```

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„å±è”½å‡½æ•°ï¼Œå®ƒå¯ä»¥æ¥å—æˆ‘ä»¬çš„`prompt_2_img_i2i_fast`å‡½æ•°ã€‚

```
def create_mask_fast(init_img, rp, qp, n=20, s=0.5):
    ## Initialize a dictionary to save n iterations
    diff = {}

    ## Repeating the difference process n times
    for idx in range(n):
        ## Creating denoised sample using reference / original text
        orig_noise = prompt_2_img_i2i_fast(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]
        ## Creating denoised sample using query / target text
        query_noise = prompt_2_img_i2i_fast(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]
        ## Taking the difference 
        diff[idx] = (np.array(orig_noise)-np.array(query_noise))

    ## Creating a mask placeholder
    mask = np.zeros_like(diff[0])

    ## Taking an average of 10 iterations
    for idx in range(n):
        ## Note np.abs is a key step
        mask += np.abs(diff[idx])  

    ## Averaging multiple channels 
    mask = mask.mean(0)

    ## Normalizing 
    mask = (mask - mask.mean()) / np.std(mask)

    ## Binarizing and returning the mask object
    return (mask > 0).astype("uint8")
```

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ–°çš„è’™ç‰ˆå‡½æ•°æ˜¯å¦èƒ½äº§ç”Ÿä¸€ä¸ªå¥½çš„è’™ç‰ˆã€‚

```
p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')
init_img = load_image(p)
mask = create_mask_fast(init_img=init_img, rp=["a horse image"], qp=["a zebra image"], n=20)
plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'
plt.imshow(
    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size
    cmap='cividis', 
    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  
)
```

![](img/ea85fff307fe72b9c89f037d7fa00f47.png)

å›¾ 10: `FastDiffEdit`é®è”½æˆ‘ä»¬çš„é©¬çš„å½¢è±¡ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢æ‰€çœ‹åˆ°çš„ï¼Œåœ¨æˆ‘çš„æœºå™¨ä¸Šï¼Œå±è”½å¾—åˆ°äº†æ”¹è¿›ï¼Œè®¡ç®—æ—¶é—´ä»å¤§çº¦ 50 ç§’å‡å°‘åˆ°å¤§çº¦ 10 ç§’(æé«˜äº† 5 å€ï¼).

è®©æˆ‘ä»¬é€šè¿‡æ·»åŠ  cv2 æŠ€å·§æ¥æ”¹è¿›æˆ‘ä»¬çš„é®ç½©ã€‚è¿™å°†åªæ˜¯å¹³æ»‘æ©è”½å¤šä¸€ç‚¹ç‚¹ã€‚

```
import cv2
def improve_mask(mask):
    mask  = cv2.GaussianBlur(mask*255,(3,3),1) > 0
    return mask.astype('uint8')

mask = improve_mask(mask)
plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'
plt.imshow(
    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size
    cmap='cividis', 
    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) > 0)  
)
```

![](img/5c8d76afefa3a42ce7e9ca58ecbf44a4.png)

å›¾ 11:ä½¿ç”¨ cv2 é«˜æ–¯æ¨¡ç³ŠæŠ€å·§æ”¹è¿›äº†æˆ‘ä»¬çš„é©¬å›¾åƒçš„`FastDiffEdit`æ©è”½å¯è§†åŒ–ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œé®ç½©å˜å¾—æ›´åŠ å¹³æ»‘ï¼Œè¦†ç›–äº†æ›´å¤šçš„åŒºåŸŸã€‚

# 3.2 æ©è”½æ‰©æ•£:æ›¿æ¢ä¸ºğŸ¤—ä¿®è¡¥ç®¡é“

å› æ­¤ï¼Œä¸æ˜¯ä½¿ç”¨æˆ‘ä»¬çš„å‡½æ•°æ¥æ‰§è¡Œæ©è”½æ‰©æ•£ï¼Œè€Œæ˜¯æœ‰ä¸€ä¸ªç‰¹æ®Šçš„ç®¡é“ğŸ¤—`diffusers`åº“åä¸º`inpaint`ç®¡é“ã€‚å®ƒé‡‡ç”¨æŸ¥è¯¢æç¤ºã€åˆå§‹å›¾åƒå’Œç”Ÿæˆçš„é®ç½©æ¥ç”Ÿæˆè¾“å‡ºå›¾åƒã€‚è®©æˆ‘ä»¬ä»è£…å…¥`inpaint`ç®¡é“å¼€å§‹ã€‚

```
from diffusers import StableDiffusionInpaintPipeline
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    revision="fp16",
    torch_dtype=torch.float16,
).to("cuda")
```

è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬ç”Ÿæˆçš„è’™ç‰ˆå’Œå›¾åƒä¿®å¤ç®¡é“ã€‚

```
pipe(
    prompt=["a zebra image"], 
    image=init_img, 
    mask_image=Image.fromarray(mask*255).resize((512,512)), 
    generator=torch.Generator("cuda").manual_seed(100),
    num_inference_steps = 20
).images[0]
image
```

![](img/b780ec9c6d42db7707d6febf71e5197f.png)

å›¾ 12:æ²¹æ¼†ç®¡é“è¾“å‡ºã€‚

æ­£å¦‚æˆ‘ä»¬ä¸Šé¢çœ‹åˆ°çš„ï¼Œä¿®å¤ç®¡é“åˆ›å»ºäº†ä¸€ä¸ªæ›´çœŸå®çš„æ–‘é©¬å›¾åƒã€‚è®©æˆ‘ä»¬ä¸ºé®ç½©å’Œæ‰©æ•£è¿‡ç¨‹åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚

```
def fastDiffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=20, dim=512):

    ## Step 1: Create mask
    mask = create_mask_fast(init_img=init_img, rp=rp, qp=qp, n=20)

    ## Improve masking using CV trick
    mask = improve_mask(mask)

    ## Step 2 and 3: Diffusion process using mask
    output = pipe(
        prompt=qp, 
        image=init_img, 
        mask_image=Image.fromarray(mask*255).resize((512,512)), 
        generator=torch.Generator("cuda").manual_seed(100),
        num_inference_steps = steps
    ).images
    return mask , output
```

è®©æˆ‘ä»¬åœ¨ä¸€äº›å›¾åƒä¸Šæµ‹è¯•è¿™ä¸ªå‡½æ•°ã€‚

```
p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&dl=pexels-helena-lopes-1996333.jpg&fm=jpg&_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')
init_img = load_image(p)
mask, output = fastDiffEdit(init_img, rp = ["a horse image"], qp=["a zebra image"])
plot_diffEdit(init_img, output, mask)
```

![](img/306fea9b50282cc5baea2b37bce9de6d.png)

å›¾ 13: `FastDiffEdit`è¾“å‡ºç¤ºä¾‹

å¤ªå¥½äº†ï¼Œè®©æˆ‘ä»¬è¯•è¯•å¦ä¸€ä¸ªã€‚

```
p = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')
init_img = load_image(p)
mask, output = fastDiffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])
plot_diffEdit(init_img, output, mask)
```

![](img/85e83579ff8a94530d009d407cfc83df.png)

å›¾ 14: `FastDiffEdit`è¾“å‡ºç¤ºä¾‹

# 4 ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä½œè€…æåˆ°çš„`DiffEdit`è®ºæ–‡ï¼Œç„¶åæˆ‘ä»¬å¯¹åˆ›å»º`FastDiffEdit`çš„æ–¹æ³•æå‡ºäº†æ”¹è¿›ï¼Œå°†è®¡ç®—é€Ÿåº¦æé«˜äº† 5 å€ã€‚

æˆ‘å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»å®ƒï¼Œå¹¶éšæ—¶ä½¿ç”¨æˆ‘çš„ä»£ç ï¼Œå¹¶å°è¯•ç”Ÿæˆæ‚¨çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå¦‚æœå¯¹ä»£ç æˆ–åšå®¢å¸–å­æœ‰ä»»ä½•åé¦ˆï¼Œè¯·éšæ—¶è”ç³» LinkedIn æˆ–ç»™æˆ‘å‘ç”µå­é‚®ä»¶ï¼Œåœ°å€æ˜¯ aayushmnit@gmail.comã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ç½‘ç«™ä¸Šé˜…è¯»åšå®¢çš„æ—©æœŸå‘å¸ƒã€aayushmnit.com[Aayush agr awal-åšå®¢](https://aayushmnit.com/blog.html)ã€‚
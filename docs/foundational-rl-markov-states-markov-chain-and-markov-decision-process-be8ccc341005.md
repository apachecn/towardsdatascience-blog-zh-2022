# åŸºç¡€ RL:é©¬å°”å¯å¤«çŠ¶æ€ã€é©¬å°”å¯å¤«é“¾å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/foundational-rl-markov-states-markov-chain-and-markov-decision-process-be8ccc341005>

## å¼ºåŒ–å­¦ä¹ ä¹‹è·¯

![](img/3df8de33fac8dbc355e14dd2403f4439.png)

ä½œè€…ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…· Dreamstudio ç”Ÿæˆçš„å°é¢ç…§ç‰‡(æˆæƒä¸º[https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)

R å¼ºåŒ–å­¦ä¹ (RL)æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ï¼Œåœ¨è¿™ç§æœºå™¨å­¦ä¹ ä¸­ï¼Œä»£ç†é€šè¿‡è¯•é”™æ¥å­¦ä¹ ä¸å…¶ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»¥ä½¿å›æŠ¥æœ€å¤§åŒ–ã€‚å®ƒä¸åŒäºç›‘ç£å­¦ä¹ ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œä»£ç†æ ¹æ®æ ‡è®°çš„ç¤ºä¾‹è¿›è¡Œè®­ç»ƒï¼Œä¹Ÿä¸åŒäºæ— ç›‘ç£å­¦ä¹ ï¼Œåœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œä»£ç†å­¦ä¹ è¯†åˆ«æœªæ ‡è®°æ•°æ®ä¸­çš„æ¨¡å¼ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»£ç†äººå­¦ä¹ åœ¨ä¸€ä¸ªç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨ä»¥è·å¾—æœ€å¤§çš„å›æŠ¥ï¼Œå¦‚èµ¢å¾—åˆ†æ•°æˆ–èµ¢å¾—æ¸¸æˆã€‚

> å¼ºåŒ–å­¦ä¹ å¯¹äºå¹¿æ³›çš„åº”ç”¨æ˜¯æœ‰ç”¨çš„ï¼ŒåŒ…æ‹¬æœºå™¨äººã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ¸¸æˆã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘æ„å»ºäº†ä¸€äº›åŸºæœ¬æ¦‚å¿µæ¥ç†è§£å¼ºåŒ–å­¦ä¹ ã€‚åœ¨[çš„ä¸‹ä¸€ç¯‡æ–‡ç« ](/foundational-rl-solving-markov-decision-process-d90b7e134c0b)ä¸­ï¼Œæˆ‘ä¼šè°ˆåˆ°é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹çš„è§£å†³æ–¹æ¡ˆã€‚

[](/foundational-rl-solving-markov-decision-process-d90b7e134c0b)  

# ä»£ç†äººã€è¡ŒåŠ¨ã€å¥–åŠ±å’Œç›®æ ‡

åœ¨ RL ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä»£ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨æŸç§ç®—æ³•è®­ç»ƒå®ƒé‡‡å–æŸäº›è¡ŒåŠ¨ï¼Œä½¿å›æŠ¥æœ€å¤§åŒ–ï¼Œä»¥è¾¾åˆ°æœ€ç»ˆç›®æ ‡ã€‚æœ€ç»ˆç›®æ ‡å¯èƒ½æ˜¯éå¸¸é¥è¿œçš„æœªæ¥ï¼Œæˆ–è€…ä¸æ–­å˜åŒ–(å°±åƒè‡ªä¸»å¯¼èˆªä¸€æ ·)ã€‚

# é©¬å°”å¯å¤«çŠ¶æ€

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒçŠ¶æ€æ˜¯æŒ‡ä¸»ä½“æ‰€å¤„çš„å½“å‰æƒ…å¢ƒæˆ–ç¯å¢ƒã€‚å®ƒè¡¨ç¤ºä»£ç†åœ¨ç»™å®šæ—¶é—´ç‚¹æ‹¥æœ‰çš„å…³äºå…¶ç¯å¢ƒçš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œè‡ªä¸»è½¦è¾†çš„ä½ç½®å’Œé€Ÿåº¦å¯ä»¥æ˜¯ RL é—®é¢˜ä¸­çš„çŠ¶æ€ã€‚ä»£ç†ä½¿ç”¨çŠ¶æ€ä¿¡æ¯æ¥å†³å®šä¸‹ä¸€ä¸ªæ—¶é—´æ­¥é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨æ¥æœ€å¤§åŒ–å¥–åŠ±ã€‚

åœ¨ RL ä¸­ï¼Œæˆ‘ä»¬å…³å¿ƒé©¬å°”å¯å¤«çŠ¶æ€ï¼Œå…¶ä¸­çŠ¶æ€å…·æœ‰æ‰€æœ‰æœªæ¥çŠ¶æ€ä»…ä¾èµ–äºå½“å‰çŠ¶æ€çš„æ€§è´¨ã€‚è¿™æ„å‘³ç€ä¸»ä½“ä¸éœ€è¦è®°ä½å®ƒä¸ç¯å¢ƒäº¤äº’çš„å…¨éƒ¨å†å²æ¥åšå†³å®šã€‚ç›¸åï¼Œå®ƒå¯ä»¥ç®€å•åœ°å…³æ³¨å½“å‰çŠ¶æ€å¹¶æ®æ­¤é‡‡å–è¡ŒåŠ¨ã€‚è¿™ä½¿å¾—å­¦ä¹ è¿‡ç¨‹æ›´åŠ æœ‰æ•ˆï¼Œå› ä¸ºä»£ç†ä¸å¿…å­˜å‚¨å’Œå¤„ç†å¤§é‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒä½¿ä»£ç†çš„è¡Œä¸ºæ›´å¯é¢„æµ‹ï¼Œå› ä¸ºå®ƒå®Œå…¨ç”±å½“å‰çŠ¶æ€å†³å®šã€‚è¿™åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½å¾ˆæœ‰ç”¨ï¼Œæ¯”å¦‚æœºå™¨äººå’Œæ§åˆ¶ç³»ç»Ÿã€‚

æˆ‘ä»¬å¯ä»¥å°†è½¦è¾†çš„é©¬å°”å¯å¤«çŠ¶æ€ç¼–ç å¦‚ä¸‹:

```
# define the states of the vehicle
STOPPED = 0
MOVING_FORWARD = 1
MOVING_BACKWARD = 2

# define the actions of the vehicle
STOP = 0
MOVE_FORWARD = 1
MOVE_BACKWARD = 2

# define the Markov state of the vehicle
class VehicleMarkovState:
  def __init__(self, state, action):
    self.state = state
    self.action = action

# define a function to encode the Markov state of the vehicle
def encode_markov_state(vehicle_state, vehicle_action):
  return VehicleMarkovState(vehicle_state, vehicle_action)

# example: encode the Markov state of a vehicle that is moving forward
markov_state = encode_markov_state(MOVING_FORWARD, MOVE_FORWARD)
print(markov_state.state)  # prints 1 (MOVING_FORWARD)
print(markov_state.action)  # prints 1 (MOVE_FORWARD)
```

# é©¬å°”å¯å¤«é“¾

é©¬å°”å¯å¤«é“¾æ˜¯ä¸€ä¸ªæœ‰é™çŠ¶æ€æœºï¼Œå…¶ä¸­æ¯ä¸ªçŠ¶æ€éƒ½æ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«çŠ¶æ€ã€‚é©¬å°”å¯å¤«é“¾ç”±è®¸å¤šçŠ¶æ€ç»„æˆï¼Œå…·æœ‰ä»ä¸€ä¸ªçŠ¶æ€åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„è½¬ç§»æ¦‚ç‡ã€‚åœ¨é©¬å°”å¯å¤«é“¾ä¸­ï¼Œè½¬ç§»åˆ°ç‰¹å®šçŠ¶æ€çš„æ¦‚ç‡å–å†³äºå½“å‰çŠ¶æ€å’Œç»è¿‡çš„æ—¶é—´ï¼Œè€Œä¸ç”¨æ‹…å¿ƒè¿‡å»å‘ç”Ÿäº†ä»€ä¹ˆã€‚

é©¬å°”å¯å¤«é“¾ä¸åŒäºéšæœºè¿‡ç¨‹ï¼Œå› ä¸ºåœ¨éšæœºè¿‡ç¨‹ä¸­ï¼Œç°åœ¨å‘ç”Ÿçš„äº‹æƒ…å–å†³äºè¿‡å»å‘ç”Ÿçš„äº‹æƒ…ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€è¿‘å‘ç”Ÿçš„äº‹æƒ…ã€‚

è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªä¾‹å­:

![](img/dc45562ddfd3b97f6bc6efe155c59c40.png)

å›¾ä¸€ã€‚ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œå›¾ç‰‡ç”±ä½œè€…æä¾›

æˆ‘ä»¬æœ‰ä¸¤ä¸ªé©¬å°”å¯å¤«çŠ¶æ€ Aï¼Œå’Œ Bï¼Œä» A åˆ° B çš„è½¬ç§»æ¦‚ç‡æ˜¯ 0.7ï¼Œä» B åˆ° A çš„è½¬ç§»æ¦‚ç‡æ˜¯ 0.9ï¼Œä» B åˆ° B çš„è½¬ç§»æ¦‚ç‡æ˜¯ 0.1ï¼Œä» A åˆ° A çš„è½¬ç§»æ¦‚ç‡æ˜¯ 0.3ã€‚è¿™ä¸ªæƒ³æ³•å¦‚å›¾ 1 æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥ç”¨ Python å¯¹æ­¤è¿›è¡Œç¼–ç ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
# define the states of the Markov chain
A = 0
B = 1

# define the transition probabilities
transition_probs = [[0.3, 0.7],  # transition probabilities from A
                    [0.9, 0.1]]  # transition probabilities from B

# define a class to represent the Markov chain
class MarkovChain:
  def __init__(self, states, transition_probs):
    self.states = states
    self.transition_probs = transition_probs

# define a function to encode the Markov chain
def encode_markov_chain(markov_states, markov_transition_probs):
  return MarkovChain(markov_states, markov_transition_probs)

# example: encode the Markov chain
markov_chain = encode_markov_chain([A, B], transition_probs)
print(markov_chain.states)  # prints [0, 1]
print(markov_chain.transition_probs)  # prints [[0.3, 0.7], [0.9, 0.1]]
```

# é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æˆ– MDP æ˜¯é©¬å°”å¯å¤«é“¾çš„å»¶ä¼¸ã€‚åœ¨ MDPï¼Œæ ¹æ®æŸç§åŠ¨ä½œ**å’Œ**ï¼ŒçŠ¶æ€ä»ä¸€ä¸ªé©¬å°”å¯å¤«çŠ¶æ€è½¬æ¢åˆ°å¦ä¸€ä¸ªé©¬å°”å¯å¤«çŠ¶æ€ã€‚è¿™ç§è½¬å˜ä¼šå¸¦æ¥ç›¸åº”çš„å›æŠ¥ã€‚MDP æ˜¯ä¸€ä¸ª 4 å…ƒç»„æ¨¡å‹(ğ“¢ã€ğ“ã€ğ“Ÿã€ğ“¡)ï¼Œå…¶ä¸­ *s* âˆˆ ğ“¢æ˜¯ä¸€ä¸ªçŠ¶æ€ï¼Œ *a* âˆˆ ğ“æ˜¯å½“ä»£ç†æ˜¯ä¸€ä¸ªçŠ¶æ€ *s* æ—¶é‡‡å–çš„ä¸€ä¸ªåŠ¨ä½œï¼Œğ“Ÿ *(s' | sï¼Œa)* æ˜¯åœ¨åŠ¨ä½œ *a* (æˆ–ä¸€äº›åŠ¨ä½œ)çš„å½±å“ä¸‹ä» *s* è½¬ç§»åˆ°çŠ¶æ€*sâ€™*çš„è½¬ç§»æ¦‚ç‡çŸ©é˜µ

**ç­–ç•¥å‡½æ•°:**åœ¨ RL æ–‡çŒ®ä¸­é€šå¸¸ç”¨Ï€è¡¨ç¤ºçš„ç­–ç•¥å‡½æ•°è§„å®šäº†ä»çŠ¶æ€ç©ºé—´ğ“¢åˆ°åŠ¨ä½œç©ºé—´ğ“.çš„æ˜ å°„

MDP å¯ä»¥ç”¨æ¥æ¨¡æ‹Ÿè‡ªåŠ¨é©¾é©¶æ±½è½¦çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒMDP çš„çŠ¶æ€å¯èƒ½è¡¨ç¤ºæ±½è½¦å’Œç¯å¢ƒä¸­å…¶ä»–å¯¹è±¡(å¦‚å…¶ä»–æ±½è½¦å’Œéšœç¢ç‰©)çš„ä¸åŒä½ç½®å’Œé€Ÿåº¦ã€‚MDP çš„åŠ¨ä½œå¯èƒ½ä»£è¡¨è‡ªåŠ¨é©¾é©¶æ±½è½¦å¯ä»¥é‡‡å–çš„ä¸åŒåŠ¨ä½œï¼Œä¾‹å¦‚åŠ é€Ÿã€åˆ¹è½¦æˆ–è½¬å¼¯ã€‚MDP çš„å¥–åŠ±å¯èƒ½ä»£è¡¨ä¸åŒè¡Œä¸ºçš„ä»·å€¼æˆ–æ•ˆç”¨ï¼Œä¾‹å¦‚é¿å…ç¢°æ’æˆ–å¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°ã€‚ä½¿ç”¨ MDPï¼Œè‡ªåŠ¨é©¾é©¶æ±½è½¦å¯ä»¥å­¦ä¹ é‡‡å–æœ€å¤§åŒ–å…¶å›æŠ¥çš„è¡ŒåŠ¨ï¼Œä¾‹å¦‚é¿å…ç¢°æ’å’Œå¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°ã€‚

æœ¬æ–‡ä¸ºå¼€å§‹å¼ºåŒ–å­¦ä¹ æä¾›äº†ç¬¬ä¸€æ‰‹åŸºç¡€ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è®¨è®ºæ›´å¤šçš„æ¦‚å¿µï¼Œæ¯”å¦‚ä»·å€¼å‡½æ•°ã€åŠ¨æ€è§„åˆ’ã€å¦‚ä½•æ±‚è§£é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä»¥åŠéƒ¨åˆ†å¯è§‚æµ‹æ€§ MDPã€‚

ç¬¬äºŒéƒ¨åˆ†:[https://towards data science . com/fundamental-rl-solving-Markov-decision-process-d 90 b 7 e 134 c0b](/foundational-rl-solving-markov-decision-process-d90b7e134c0b)

[](/foundational-rl-solving-markov-decision-process-d90b7e134c0b)  

# å‚è€ƒ

1.  ç½—çº³å¾·Â·éœåå¾·(1960)ã€‚ [*åŠ¨æ€è§„åˆ’å’Œé©¬å°”å¯å¤«è¿‡ç¨‹*](http://web.mit.edu/dimitrib/www/dpchapter.pdf) (PDF)ã€‚éº»çœç†å·¥å­¦é™¢å‡ºç‰ˆç¤¾ã€‚
2.  å¼ºåŒ–å­¦ä¹ å’Œéšæœºä¼˜åŒ–:è¿ç»­å†³ç­–çš„ç»Ÿä¸€æ¡†æ¶ã€‚)ï¼Œå¨åˆ©(2022)ã€‚ç²¾è£…æœ¬ã€‚ISBN 9781119815051ã€‚

> ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿç»™æˆ‘ä¹°æ¯å’–å•¡ã€‚
> 
> å–œæ¬¢æˆ‘çš„ä½œå“å—ï¼ŸåŠ å…¥æˆ‘çš„[é‚®ä»¶åˆ—è¡¨](https://rahulbhadani.medium.com/subscribe)ã€‚
> 
> æƒ³äº†è§£æ›´å¤š STEM ç›¸å…³è¯é¢˜ï¼ŸåŠ å…¥[ä»‹è´¨](https://rahulbhadani.medium.com/membership)
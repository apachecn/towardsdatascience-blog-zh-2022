# æ–‡æœ¬åˆ†æ 101 â€”è¯äº‘å’Œæƒ…æ„Ÿåˆ†æ

> åŸæ–‡ï¼š<https://towardsdatascience.com/text-analytics-101-word-cloud-and-sentiment-analysis-2c3ade81c7e8>

## ä¸€ç¯‡æè¿°æ–‡æœ¬å¤„ç†åŸºç¡€çŸ¥è¯†ä»¥åŠå¦‚ä½•ä» Twitter API çš„æ•°æ®ä¸­è·å¾—æ´å¯ŸåŠ›çš„æ–‡ç« 

æˆ‘ä»¬ä» API è·å¾—çš„ tweets æ•°æ®æ˜¯éç»“æ„åŒ–çš„ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸åŒçš„è¯­è¨€ã€‚è¿™ä¸æ–¹ä¾¿æœºå™¨å­¦ä¹ æˆ–è€…ç»Ÿè®¡åˆ†æã€‚æˆ‘ä»¬å°†æ‰§è¡ŒæŒ–æ˜å’Œè‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ¥è¯„ä¼°æ–‡æœ¬æ•°æ®çš„æƒ…æ„Ÿã€‚æˆ‘ä»¬å°†åœ¨è¿™æ¬¡æ—…ç¨‹ä¸­ä½¿ç”¨æˆ‘ä»¬å‹å¥½çš„ Jupyter ç¬”è®°æœ¬å’Œ python:)

![](img/11223f294ecc620b4e4734fc3631ac7d.png)

æ–‡æœ¬æŒ–æ˜â€”æŒ‰ä½œè€…åˆ†ç±»çš„å›¾åƒ

æˆ‘ä»¬å¯ä»¥æŠŠå®ƒåˆ†æˆå››ä¸ªæ­¥éª¤:

1.  ä½¿ç”¨ tweepy ä» Twitter ä¸­æå–æ•°æ®
2.  æ•°æ®æ¸…ç†å’Œå¤„ç†
3.  ä½¿ç”¨ wordcloud è¿›è¡Œå¯è§†åŒ–
4.  æƒ…æ„Ÿåˆ†æ

# æ•°æ®æå–

ç¬¬ä¸€ä»¶äº‹æ˜¯åœ¨ Twitter ä¸Šæ³¨å†Œä¸€ä¸ªå¼€å‘è€…è´¦æˆ·ï¼Œå¹¶è®¿é—® Twitter APIã€‚API ä½¿ç”¨ OAuth å®¢æˆ·ç«¯è¿›è¡Œè®¤è¯ï¼Œè¿™æ„å‘³ç€æ‚¨å¿…é¡»ç”Ÿæˆä¸€ä¸ª[æ‰¿è½½ä»¤ç‰Œ](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens)æˆ–è€…ä½¿ç”¨å®¢æˆ·ç«¯ id/secretã€‚æˆ‘å·²ç»ä½¿ç”¨â€œtweepyâ€æ¥è®¿é—® API å¹¶ä» twitter è·å–æ•°æ®ã€‚

é‚£ä¹ˆæˆ‘ä»¬è¦åˆ†æä»€ä¹ˆå‘¢ï¼Ÿï¼Ÿå—¯â€¦è®©æˆ‘ä»¬æ¥çœ‹çœ‹æ¨ç‰¹æ˜¯æ€ä¹ˆçœ‹å¾…â€œè¡°é€€â€çš„å§ï¼ï¼â€”è¿™å°†æ˜¯æˆ‘ä»¬çš„æœç´¢æŸ¥è¯¢ã€‚æˆ‘ä»¬å°†åˆ¶å®šä¸€ä¸ªè¯äº‘ï¼Œå¹¶æ£€æŸ¥å‘¨å›´çš„æƒ…ç»ªã€‚

```
**#Data Extraction**
import tweepy
query = '#recession -is:retweet'
tw_clnt=tweepy.Client(bearer_token='AAAABCJJGJG')
tweets=tweepy.Paginator(tw_clnt.search_recent_tweets,query,max_results=100).flatten(limit=5000)
df=pd.DataFrame(tweets)
df.head(2)
```

![](img/4351f2fb9732a55ed5ee5737ce9b5f71.png)

åœ¨æ•°æ®å¸§ä¸­æ•è·çš„æ¨æ–‡â€”â€”å›¾ç‰‡ç”±ä½œè€…æä¾›

å•Šï¼æˆ‘ä»¬å–œæ¬¢æ•°æ®æ¡†ğŸ¤©æ›´æœ‰é“ç†ã€‚æ‰¾å‡º df ä¸­æ˜¯å¦æœ‰ç©ºå€¼çš„æ—¶é—´ã€‚

```
**#Check for nulls/blank fields**
df.id.count(), df.isnull().sum()
```

5000 æ¡è®°å½•ï¼Œè°¢å¤©è°¢åœ°æ²¡æœ‰ç©ºè®°å½•ğŸ˜€

# æ–‡æœ¬å¤„ç†

ç°åœ¨ï¼Œæˆ‘ä»¬å°†æ¸…ç†å’Œæ ¼å¼åŒ– tweet æ–‡æœ¬â€”â€”åˆ é™¤æåŠ(ä¾‹å¦‚:@abc423)ã€åª’ä½“é“¾æ¥ã€è½¬æ¢ä¸ºå°å†™å¹¶åˆ é™¤æ¢è¡Œç¬¦ã€‚æˆ‘å»ºè®®æˆ‘ä»¬ä¸è¦åˆ é™¤æ ‡ç­¾ï¼Œå› ä¸ºå¾ˆå¤šæ—¶å€™é‡è¦çš„æƒ…æ„Ÿ/ä¿¡æ¯éšè—åœ¨æ ‡ç­¾#dontignorehashtags ä¸­ğŸ˜œ

```
**#Remove special characters/links**
import re
def tweet_cleaner(x):
    text=re.sub("[@&][A-Za-z0-9_]+","", x)     # Remove mentions
    text=re.sub(r"http\S+","", text)           # Remove media links
return  pd.Series([text])df[['plain_text']] = df.text.apply(tweet_cleaner)**#Convert all text to lowercase**
df.plain_text = df.plain_text.str.lower()**#Remove newline character**
df.plain_text = df.plain_text.str.replace('\n', '')**#Replacing any empty strings with null**
df = df.replace(r'^\s*$', np.nan, regex=True)
if df.isnull().sum().plain_text == 0:
   print ('no empty strings')
else:
   df.dropna(inplace=True)
```

æˆ‘ä»¬å°†æŠŠæ ¼å¼è‰¯å¥½çš„æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªæ–°çš„åˆ—ä¸­â€”â€”â€œplain _ textâ€

![](img/7a02af796e00dab063076c69fca13308.png)

æ ¼å¼è‰¯å¥½çš„æ–‡æœ¬-ä½œè€…æä¾›çš„å›¾åƒ

æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥æ˜¯ä½¿ç”¨â€œæ£€æµ‹â€æ¥æ£€æµ‹è¯­è¨€ï¼Œæ‰¾å‡ºæ˜¯å¦æœ‰è‹±è¯­ä»¥å¤–è¯­è¨€çš„æ¨æ–‡ã€‚å¦‚æœ tweet ä¸­çš„æ–‡æœ¬åªæœ‰æ•°å­—æˆ–æ ‡ç‚¹ç¬¦å·ï¼Œè¿™ä¸ªåº“å°±ä¼šå¤±è´¥ã€‚è¿™æ ·çš„æ¨æ–‡(åªæœ‰æ•°å­—)å¯¹æˆ‘ä»¬çš„åˆ†ææ¯«æ— ç”¨å¤„ï¼Œå› æ­¤è¿™äº›â€œä¾‹å¤–â€è®°å½•å¯ä»¥åˆ é™¤ã€‚

```
**#detect language of tweets**from langdetect import detect
def detect_textlang(text):
    try:
        src_lang = detect(text)
        if src_lang =='en':
            return 'en'
        else:
        #return "NA"    
            return src_lang
    except:
        return "NA"
df['text_lang']=df.plain_text.apply(detect_textlang)
```

![](img/b842c8eabdd29ad2605bfc6899e8ff52.png)

è¥¿ç­ç‰™è¯­æ¨æ–‡â€”ä½œè€…å›¾ç‰‡

å“¦ï¼æœ‰è¥¿ç­ç‰™è¯­çš„æ¨æ–‡ï¼Œå¯èƒ½è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„â€¦æˆ‘ä»¬å°†æŒ‰è¯­è¨€å¯¹æ‰€æœ‰æ¨æ–‡è¿›è¡Œåˆ†ç»„ï¼Œå¹¶æŸ¥çœ‹å‰ 10 åã€‚

```
**# Group tweets by language and list the top 10**
import matplotlib.pyplot as plt
plt.figure(figsize=(4,3))
df.groupby(df.text_lang).plain_text.count().sort_values(ascending=False).head(10).plot.bar()
plt.show()
```

![](img/98f14ad3b01add9a062517a77c4eeafe.png)

æŒ‰è¯­è¨€å¯¹æ¨æ–‡åˆ†ç»„â€”â€”æŒ‰ä½œè€…å¯¹å›¾ç‰‡åˆ†ç»„

æœ‰è·å…°è¯­ã€åœŸè€³å…¶è¯­ç­‰è®°å½•..æ‰€ä»¥æˆ‘ä»¬ä¼šç”¨è°·æ­Œç¿»è¯‘å°†è¿™äº›æ¨æ–‡ç¿»è¯‘æˆè‹±è¯­ã€‚å¦‚æœæœ‰è¶…è¿‡ 100â€“200 æ¡è®°å½•ï¼Œç¿»è¯‘éœ€è¦æ—¶é—´ï¼›å¦‚æœæ²¡æœ‰åŠæ—¶æ”¶åˆ°å“åº”ï¼Œå®ƒå¯èƒ½ä¼šè¶…æ—¶[é”™è¯¯â€”è¯»å–æ“ä½œè¶…æ—¶]ã€‚å› æ­¤ï¼Œåœ¨â€œç¿»è¯‘â€å‡½æ•°è°ƒç”¨ä¹‹å‰åº”ç”¨è¯­è¨€è¿‡æ»¤å™¨æ˜¯æ˜æ™ºçš„ã€‚

```
**#Translate to English**
from googletrans import Translator
def translate_text(lang,text):
    translator= Translator()
    trans_text = translator.translate(text, src=lang).text
    return trans_text df['translated_text']=df.apply(lambda x: x.plain_text if x.text_lang == 'en' else translate_text(x.text_lang, x.plain_text), axis=1)df.translated_text = df.translated_text.str.lower()
```

![](img/79d29a88e3a6347e37c773a030dc0552.png)

å¸¦æœ‰ç¿»è¯‘æ–‡æœ¬çš„æ•°æ®æ¡†â€”æŒ‰ä½œè€…åˆ†ç±»çš„å›¾ç‰‡

# æ•°æ®å¯è§†åŒ–

é…·:)ç°åœ¨æˆ‘ä»¬çš„æºæ•°æ®å‡ ä¹å‡†å¤‡å¥½äº†ï¼Œåªæ˜¯æˆ‘ä»¬éœ€è¦å»æ‰' #recession 'ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬çš„æŸ¥è¯¢ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯å»ºç«‹ä¸€ä¸ªè¯äº‘ï¼Œå®ƒå¯ä»¥ç»™å‡ºå…³äºè¡°é€€çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯é‡å¤é‚£ä¸ªè¯ï¼æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›é€šç”¨è¯ï¼Œå¦‚â€œå°†â€ï¼Œâ€œå»â€ï¼Œâ€œå·²ç»â€ï¼Œâ€œå°†â€ç­‰ã€‚å‡ºç°åœ¨æˆ‘ä»¬çš„è¯äº‘ä¸­ã€‚Nltk çš„â€œåœç”¨å­—è¯â€æä¾›äº†æ‰€æœ‰æ­¤ç±»å­—è¯çš„åˆ—è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä»æˆ‘ä»¬çš„â€œtranslated_textâ€ä¸­æ’é™¤ã€‚

```
**#Remove un-important words from text**
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
query_words={'recession', '#' }
stop_words.update(query_words)
for word in query_words:
    df.translated_text = df.translated_text.str.replace(word, '')**#Creating word cloud**
from wordcloud import WordCloud, ImageColorGenerator
wc=WordCloud(stopwords=stop_words, collocations=False, max_font_size=55, max_words=25, background_color="black")
wc.generate(' '.join(df.translated_text))
plt.figure(figsize=(10,12))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
```

![](img/2e2af394901b3806a40f0e08bcd139ff.png)

â€œè¡°é€€â€ä¸€è¯äº‘â€”â€”ä½œè€…å›¾ç‰‡

ç»™ä½ ğŸ‘

æ ¹æ®æ¨ç‰¹æ•°æ®ï¼Œåœ¨ç»æµè¡°é€€çš„èƒŒæ™¯ä¸‹ï¼Œäººä»¬éƒ½åœ¨è°ˆè®ºé€šè´§è†¨èƒ€ã€T2ã€è£å‘˜å’Œå·¥ä½œâ€”â€”è¿™æ˜¯çœŸçš„ï¼ç‰¹åˆ«å…³æ³¨è‚¡ç¥¨å¸‚åœºã€ä½æˆ¿å¸‚åœºå’Œå¯†ç å¸‚åœºã€‚æ­¤å¤–ï¼Œè‹±å›½çš„çŠ¶å†µå’Œç¾è”å‚¨çš„å†³å®šä¹Ÿç»å¸¸è¢«æèµ·ã€‚æˆ‘ä¿æŒäº† 25 ä¸ªå•è¯çš„æœ€å¤§è®¡æ•°ï¼Œä½ å¯ä»¥å¢åŠ å®ƒä»¥è·å¾—æ›´å¤šçš„è§è§£ã€‚

# æƒ…æ„Ÿåˆ†æ

è®©æˆ‘ä»¬ä½¿ç”¨ Vader perspective analyzer æ¥æ¢ç´¢è¿™äº›æ¨æ–‡çš„æ€»ä½“æƒ…ç»ªã€‚è¿™ä¸ªåº“è¿”å›ä¸€ä¸ªç§°ä¸ºææ€§çš„æ•°å­—ï¼ŒèŒƒå›´åœ¨-1 å’Œ 1 ä¹‹é—´ï¼›-1 è¡¨ç¤ºæœ€æ¶ˆæçš„æƒ…ç»ªï¼Œ1 è¡¨ç¤ºæœ€ç§¯æçš„æƒ…ç»ªã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›æƒ…ç»ªåˆ†ä¸ºâ€œæ¶ˆæâ€ã€â€œç§¯æâ€å’Œâ€œä¸­æ€§â€ä¸‰ç±»ã€‚

```
**#Sentiment Check**
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer=SentimentIntensityAnalyzer()        
df['polarity']=[analyzer.polarity_scores(text)['compound'] for text in df.translated_text]
def get_sentiment(polarity):
    if polarity < 0.0:
        return 'Negative'
    elif polarity > 0.2:
        return 'Positive'
    else:
        return 'Neutral'
df['sentiment']=df2.polarity.apply(get_sentiment)
plt.figure(figsize=(3,3))
df.sentiment.value_counts().plot.bar()
```

![](img/f2e4c9f46c25015b1f2e64a91fe04c14.png)

æ­£é¢æƒ…ç»ªã€è´Ÿé¢æƒ…ç»ªå’Œä¸­æ€§æƒ…ç»ªçš„æ•°é‡â€”â€”ä½œè€…å›¾ç‰‡

å“¦ï¼æ­£å¦‚æ‰€æ–™ï¼Œè´Ÿé¢æƒ…ç»ªçš„æ•°é‡æœ€å¤šï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªè´Ÿé¢æ¨æ–‡æ ·æœ¬å’Œä¸€äº›æ­£é¢æ¨æ–‡ã€‚

![](img/7f47b192410ea896349b167883151453.png)

æ¶ˆææƒ…ç»ªâ€”â€”ä½œè€…çš„å½¢è±¡

![](img/99665955f24029b45c7300b422948841.png)

ç§¯æçš„æƒ…ç»ªâ€”â€”ä½œè€…çš„å½¢è±¡

åœ¨è¿‡æ»¤ç§¯ææƒ…ç»ªæ–¹é¢ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›å…³äºå¦‚ä½•åœ¨è¡°é€€æ—¶æœŸè£…å¤‡è‡ªå·±çš„å»ºè®®ï¼æ˜¯çš„ï¼Œè¿™äº›ç¡®å®æ˜¯ç§¯æçš„è§‚ç‚¹..

# ç»“è®º

å¹²å¾—å¥½ï¼è¿™ç§æŠ€æœ¯åœ¨è¥é”€åˆ†æä¸­æœ‰é‡è¦çš„åº”ç”¨ï¼Œå…¶ä¸­å¯¹å“ç‰Œ/äº§å“çš„å®¢æˆ·è¯„è®ºè¿›è¡Œè¯„ä¼°ã€‚ä½†æœ‰æ—¶(æ­£å¦‚ä½ åœ¨ä¸€æ¡ç§¯æçš„æ¨æ–‡ä¸­çœ‹åˆ°çš„â€”â€”â€œå“‡â€â€”â€”ç”¨æˆ·æ–‡æœ¬å¯èƒ½ä¼šæœ‰ç‚¹è¯¯å¯¼ï¼Œå› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œâ€œå“‡â€å¹¶ä¸æ„å‘³ç€ç§¯æã€‚è¿™æ ·çš„ç»†å¾®å·®åˆ«ä¸æ˜“å¯Ÿè§‰ã€‚å› æ­¤ï¼Œä¸ºäº†è·å¾—æ›´å‡†ç¡®çš„æƒ…æ„Ÿç»“æœï¼Œå»ºè®®ä½¿ç”¨ 2-3 ä¸ªè½¯ä»¶åŒ…è¿›è¡Œåˆ†æï¼Œå¦‚ TextBlob/Transformers ä»¥åŠ VADERï¼Œå¹¶è·å¾—ææ€§çš„åŠ æƒåˆ†æ•°ã€‚

è¿˜æœ‰å…¶ä»–æ–¹æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œä¾‹å¦‚é€šè¿‡çŸ¢é‡åŒ–ï¼Œä½†è¿™ä¹Ÿå–å†³äºæ‚¨çš„æ•°æ®åŠå…¶å±æ€§ã€‚

å—¯ï¼ˆè¡¨ç¤ºè¸Œèº‡ç­‰ï¼‰..æ‰€ä»¥æˆ‘ä»¬å·²ç»åˆ°äº† 101 è¯¾çš„ç»“å°¾ï¼›)æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« ä¿¡æ¯ä¸°å¯Œï¼Œå·©å›ºäº†æ‚¨å¯¹æ–‡æœ¬åˆ†æçš„ç†è§£ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—®æˆ‘çš„ç¬”è®°æœ¬[ã€‚ç”¨ä½ é€‰æ‹©çš„æœç´¢è¯(å“ç‰Œ/ä¸ªæ€§/ä¸»é¢˜)æ¥å°è¯•è¿™ä¸ªç»ƒä¹ ï¼Œå¹¶ä¸æˆ‘åˆ†äº«ä½ çš„ç»“æœï¼](https://github.com/sacharya225/data-expts/blob/master/Tweets_Analysis_Recession-AV.ipynb)
# åŸºäºç¥ç»ç½‘ç»œçš„ç°ä»£æ¨èç³»ç»Ÿ

> åŸæ–‡ï¼š<https://towardsdatascience.com/modern-recommendation-systems-with-neural-networks-3cc06a6ded2c>

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

ä½œè€…å›¾ç‰‡

## ä½¿ç”¨ Python å’Œ TensorFlow æ„å»ºæ··åˆæ¨¡å‹

## æ‘˜è¦

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Python å’Œ TensorFlow æ„å»ºå…·æœ‰ç¥ç»ç½‘ç»œçš„ç°ä»£æ¨èç³»ç»Ÿã€‚

![](img/7572ec45d456c1ad3c0f76dfb88e806b.png)

äºšå†å±±å¤§Â·æ²™æ‰˜å¤«åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

[**æ¨èç³»ç»Ÿ**](https://en.wikipedia.org/wiki/Recommender_system) æ˜¯é¢„æµ‹ç”¨æˆ·å¯¹å¤šä¸ªäº§å“åå¥½çš„æ¨¡å‹ã€‚å®ƒä»¬è¢«ç”¨äºå„ç§é¢†åŸŸï¼Œå¦‚è§†é¢‘å’ŒéŸ³ä¹æœåŠ¡ã€ç”µå­å•†åŠ¡å’Œç¤¾äº¤åª’ä½“å¹³å°ã€‚

æœ€å¸¸è§çš„æ–¹æ³•åˆ©ç”¨äº§å“ç‰¹å¾(åŸºäºå†…å®¹)ã€ç”¨æˆ·ç›¸ä¼¼æ€§(åä½œè¿‡æ»¤)ã€ä¸ªäººä¿¡æ¯(åŸºäºçŸ¥è¯†)ã€‚ç„¶è€Œï¼Œéšç€ç¥ç»ç½‘ç»œçš„æ—¥ç›Šæ™®åŠï¼Œå…¬å¸å·²ç»å¼€å§‹å°è¯•å°†å®ƒä»¬ç»“åˆåœ¨ä¸€èµ·çš„æ–°çš„æ··åˆæ¨èç³»ç»Ÿã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•ä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªç°ä»£æ¨èç³»ç»Ÿã€‚æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

<https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_recommendation.ipynb>  

æˆ‘å°†ä½¿ç”¨ç”± [GroupLens Research](https://en.wikipedia.org/wiki/GroupLens_Research) åˆ›å»ºçš„ **MovieLens** æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ•°ç™¾åç”¨æˆ·è¯„ä»·çš„æ•°åƒéƒ¨ç”µå½±(é“¾æ¥å¦‚ä¸‹)ã€‚

<https://grouplens.org/datasets/movielens/latest/>  

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   è®¾ç½®:å¯¼å…¥åŒ…ã€è¯»å–æ•°æ®ã€é¢„å¤„ç†
*   å†·å¯åŠ¨é—®é¢˜
*   ä½¿ç”¨ *tensorflow* å’Œ *numpy* çš„åŸºäºå†…å®¹çš„æ–¹æ³•
*   ä½¿ç”¨ *tensorflow/keras çš„ä¼ ç»ŸååŒè¿‡æ»¤å’Œç¥ç»ååŒè¿‡æ»¤*
*   å…·æœ‰ *tensorflow/keras* çš„æ··åˆ(ä¸Šä¸‹æ–‡æ„ŸçŸ¥)æ¨¡å‹

## è®¾ç½®

é¦–å…ˆï¼Œæˆ‘å°†å¯¼å…¥ä»¥ä¸‹**åŒ…**:

```
**## for data**
import **pandas** as pd
import **numpy** as np
import **re**
from **datetime** import datetime**## for plotting**
import **matplotlib**.pyplot as plt
import **seaborn** as sns**## for machine learning**
from **sklearn** import metrics, preprocessing**## for deep learning**
from **tensorflow**.keras import models, layers, utils  **#(2.6.0)**
```

ç„¶åæˆ‘å°†è¯»å–**æ•°æ®**ï¼ŒåŒ…æ‹¬äº§å“æ•°æ®(æœ¬ä¾‹ä¸­ä¸ºç”µå½±)å’Œç”¨æˆ·æ•°æ®ã€‚

```
dtf_products = pd.read_excel("data_movies.xlsx", sheet_name="products")
```

![](img/cefa0b4716301be67ca4cf380c8dcd7c.png)

ä½œè€…å›¾ç‰‡

åœ¨ product è¡¨ä¸­ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªé¡¹ç›®ï¼Œå³è¾¹çš„ä¸¤åˆ—åŒ…å«å®ƒçš„ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§æ˜¯é™æ€çš„(æ‚¨å¯ä»¥å°†å…¶è§†ä¸ºç”µå½±å…ƒæ•°æ®)ã€‚è®©æˆ‘ä»¬è¯»å–ç”¨æˆ·æ•°æ®:

```
dtf_users = pd.read_excel("data_movies.xlsx", sheet_name="users").head(10000)
```

![](img/2ac93d4e7b85c52d20994950f7b11070.png)

ä½œè€…å›¾ç‰‡

è¿™ä¸ªè¡¨æ ¼çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€å¯¹ç”¨æˆ·-äº§å“ï¼Œæ˜¾ç¤ºç”¨æˆ·å¯¹äº§å“çš„è¯„åˆ†ï¼Œè¿™æ˜¯**ç›®æ ‡å˜é‡**ã€‚æ˜¾ç„¶ï¼Œå¹¶ä¸æ˜¯æ¯ä¸ªç”¨æˆ·éƒ½çœ‹è¿‡æ‰€æœ‰çš„äº§å“ã€‚äº‹å®ä¸Šï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ¨èç³»ç»Ÿã€‚ä»–ä»¬å¿…é¡»é¢„æµ‹ç”¨æˆ·ä¼šç»™æ–°äº§å“ä»€ä¹ˆæ ·çš„è¯„ä»·ï¼Œå¦‚æœé¢„æµ‹çš„è¯„ä»·æ˜¯é«˜/ç§¯æçš„ï¼Œé‚£ä¹ˆå°±æ¨èå®ƒã€‚æ­¤å¤–ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€äº›å…³äºç›®æ ‡å˜é‡ä¸Šä¸‹æ–‡çš„ä¿¡æ¯(å½“ç”¨æˆ·ç»™å‡ºè¯„çº§æ—¶)ã€‚

è®©æˆ‘ä»¬åšä¸€äº›**æ•°æ®æ¸…ç†**å’Œ**ç‰¹å¾å·¥ç¨‹**æ¥æ›´å¥½åœ°ç†è§£æˆ‘ä»¬æ‹¥æœ‰ä»€ä¹ˆä»¥åŠæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨å®ƒã€‚

```
**# Products**
dtf_products = dtf_products[~dtf_products["genres"].isna()]dtf_products["product"] = range(0,len(dtf_products))dtf_products["name"] = dtf_products["title"].apply(lambda x: re.sub("[\(\[].*?[\)\]]", "", x).strip())dtf_products["date"] = dtf_products["title"].apply(lambda x: int(x.split("(")[-1].replace(")","").strip()) 
if "(" in x else np.nan)dtf_products["date"] = dtf_products["date"].fillna(9999)
dtf_products["old"] = dtf_products["date"].apply(lambda x: 1 if x < 2000 else 0) **# Users**
dtf_users["user"] = dtf_users["userId"].apply(lambda x: x-1)dtf_users["timestamp"] = dtf_users["timestamp"].apply(lambda x: datetime.fromtimestamp(x))dtf_users["daytime"] = dtf_users["timestamp"].apply(lambda x: 1 if 6<int(x.strftime("%H"))<20 else 0)dtf_users["weekend"] = dtf_users["timestamp"].apply(lambda x: 1 if x.weekday() in [5,6] else 0)dtf_users = dtf_users.merge(dtf_products[["movieId","product"]], how="left")dtf_users = dtf_users.rename(columns={"rating":"y"}) **# Clean**
dtf_products = dtf_products[["product","name","old","genres"]].set_index("product")dtf_users = 
dtf_users[["user","product","daytime","weekend","y"]]
```

![](img/c7a645228c92fa1407dfdf82d3c36d4b.png)

ä½œè€…å›¾ç‰‡

è¯·æ³¨æ„ï¼Œæˆ‘ä»*æ—¶é—´æˆ³*åˆ—ä¸­æå–äº† 2 ä¸ªä¸Šä¸‹æ–‡å˜é‡:*ç™½å¤©*å’Œ*å‘¨æœ«*ã€‚æˆ‘å°†æŠŠå®ƒä»¬ä¿å­˜åˆ°æ•°æ®å¸§ä¸­ï¼Œå› ä¸ºæˆ‘ä»¬ä»¥åå¯èƒ½éœ€è¦å®ƒä»¬ã€‚

```
dtf_context = dtf_users[["user","product","daytime","weekend"]]
```

å…³äºäº§å“ï¼Œä¸‹ä¸€æ­¥æ˜¯åˆ›å»º*äº§å“-ç‰¹æ€§*çŸ©é˜µ:

```
tags = [i.split("|") for i in dtf_products["genres"].unique()]
columns = list(set([i for lst in tags for i in lst]))
columns.remove('(no genres listed)')for col in columns:
    dtf_products[col] = dtf_products["genres"].apply(lambda x: 1 if col in x else 0)
```

![](img/a17b89941f4b71ba34c7ed9aaf1883f1.png)

ä½œè€…å›¾ç‰‡

çŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œå› ä¸ºå¤§å¤šæ•°äº§å“æ²¡æœ‰æ‰€æœ‰çš„åŠŸèƒ½ã€‚è®©æˆ‘ä»¬æŠŠå®ƒå½¢è±¡åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æƒ…å†µã€‚

```
fig, ax = plt.subplots(figsize=(20,5))
sns.heatmap(dtf_products==0, vmin=0, vmax=1, cbar=False, ax=ax).set_title("Products x Features")
plt.show()
```

![](img/96f9296f78f4c0e12c506cb698b305c1.png)

ä½œè€…å›¾ç‰‡

å¯¹äº*ç”¨æˆ·-äº§å“*çŸ©é˜µï¼Œç¨€ç–æ€§å˜å¾—æ›´ç³Ÿ:

```
tmp = dtf_users.copy()
dtf_users = tmp.pivot_table(index="user", columns="product", values="y")
missing_cols = list(set(dtf_products.index) - set(dtf_users.columns))
for col in missing_cols:
    dtf_users[col] = np.nan
dtf_users = dtf_users[sorted(dtf_users.columns)]
```

![](img/45e5ddc814bcf76c92f4a0d4ebfba0ec.png)

ä½œè€…å›¾ç‰‡

![](img/aef73ade0bae05837c0573a1e117471b.png)

ä½œè€…å›¾ç‰‡

æ·±å…¥æ¨¡å‹ä¹‹å‰çš„æœ€åä¸€æ­¥æ˜¯**é¢„å¤„ç†**ã€‚å› ä¸ºæˆ‘ä»¬å°†å¤„ç†ç¥ç»ç½‘ç»œï¼Œæ‰€ä»¥ç¼©æ”¾æ•°æ®æ€»æ˜¯å¥½çš„å®è·µã€‚

```
dtf_users = pd.DataFrame(preprocessing.MinMaxScaler(feature_range=(0.5,1)).fit_transform(dtf_users.values), 
columns=dtf_users.columns, index=dtf_users.index)
```

![](img/92d7a365e2b503f186dc85efb4035976.png)

ä½œè€…å›¾ç‰‡

æœ€åï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†æˆ*è®­ç»ƒ*å’Œ*æµ‹è¯•*ç»„ã€‚æˆ‘å°†å‚ç›´æ‹†åˆ†æ•°æ®é›†ï¼Œè¿™æ ·æ‰€æœ‰ç”¨æˆ·éƒ½å°†å‚åŠ *åŸ¹è®­*å’Œ*æµ‹è¯•*ï¼Œè€Œ 80%çš„äº§å“ç”¨äºåŸ¹è®­ï¼Œ20%ç”¨äºæµ‹è¯•ã€‚åƒè¿™æ ·:

![](img/8223cb789f50349c59f7ef086718e72f.png)

ä½œè€…å›¾ç‰‡

```
split = int(0.8*dtf_users.shape[1])
dtf_train = dtf_users.loc[:, :split-1]
dtf_test = dtf_users.loc[:, split:]
```

å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹äº†â€¦ä¹Ÿè®¸å§ã€‚

## å†·å¯åŠ¨

æƒ³è±¡ä¸€ä¸‹ï¼Œæ‹¥æœ‰ä¸€ä¸ªå…¨æ–°çš„ç±»ä¼¼ç½‘é£çš„åº”ç”¨ç¨‹åºï¼Œç¬¬ä¸€ä¸ªç”¨æˆ·è®¢é˜…ã€‚æˆ‘ä»¬éœ€è¦èƒ½å¤Ÿæä¾›å»ºè®®ï¼Œè€Œä¸ä¾èµ–äºç”¨æˆ·ä»¥å‰çš„äº’åŠ¨ï¼Œå› ä¸ºè¿˜æ²¡æœ‰è®°å½•ã€‚å½“ä¸€ä¸ªç”¨æˆ·(æˆ–ä¸€ä¸ªäº§å“)æ˜¯æ–°çš„ï¼Œæˆ‘ä»¬å°±æœ‰äº† [**å†·å¯åŠ¨é—®é¢˜**](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)) ã€‚è¿™ä¸ªç³»ç»Ÿæ— æ³•åœ¨ç”¨æˆ·å’Œäº§å“ä¹‹é—´å»ºç«‹ä»»ä½•è”ç³»ï¼Œå› ä¸ºå®ƒæ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸»è¦çš„æŠ€æœ¯æ˜¯**åŸºäºçŸ¥è¯†çš„æ–¹æ³•**:ä¾‹å¦‚ï¼Œè¯¢é—®ç”¨æˆ·çš„åå¥½ä»¥åˆ›å»ºåˆå§‹ç®€æ¡£ï¼Œæˆ–è€…ä½¿ç”¨äººå£ç»Ÿè®¡ä¿¡æ¯(å³é’å°‘å¹´çš„é«˜ä¸­èŠ‚ç›®å’Œå„¿ç«¥çš„å¡é€š)ã€‚

å¦‚æœåªæœ‰å‡ ä¸ªç”¨æˆ·ï¼Œå¯ä»¥ä½¿ç”¨åŸºäºå†…å®¹çš„æ–¹æ³•ã€‚ç„¶åï¼Œå½“æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„è¯„çº§(å³æ¯ä¸ªç”¨æˆ·è‡³å°‘ 10 ä¸ªäº§å“ï¼Œæ€»ç”¨æˆ·è¶…è¿‡ 100 ä¸ª)æ—¶ï¼Œå¯ä»¥åº”ç”¨æ›´å¤æ‚çš„æ¨¡å‹ã€‚

## åŸºäºå†…å®¹

[**åŸºäºå†…å®¹çš„æ–¹æ³•**](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering) æ˜¯åŸºäºäº§å“å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ*ç”¨æˆ· A* å–œæ¬¢*äº§å“ 1ã€*å’Œ*äº§å“ 2* ä¸*äº§å“ 1* ç›¸ä¼¼ï¼Œé‚£ä¹ˆ*ç”¨æˆ· A* å¯èƒ½ä¹Ÿä¼šå–œæ¬¢*äº§å“ 2* ã€‚å¦‚æœä¸¤ä¸ªäº§å“å…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾ï¼Œåˆ™å®ƒä»¬æ˜¯ç›¸ä¼¼çš„ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œè¿™ç§æƒ³æ³•æ˜¯ç”¨æˆ·å®é™…ä¸Šå¯¹äº§å“çš„åŠŸèƒ½è€Œä¸æ˜¯äº§å“æœ¬èº«è¿›è¡Œè¯„çº§ã€‚æ¢ä¸ªè§’åº¦è¯´ï¼Œå¦‚æœæˆ‘å–œæ¬¢éŸ³ä¹å’Œè‰ºæœ¯ç›¸å…³çš„äº§å“ï¼Œé‚£æ˜¯å› ä¸ºæˆ‘å–œæ¬¢é‚£äº›ç‰¹æ€§(éŸ³ä¹å’Œè‰ºæœ¯)ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°è®¡æˆ‘æœ‰å¤šå–œæ¬¢å…·æœ‰ç›¸åŒåŠŸèƒ½çš„å…¶ä»–äº§å“ã€‚è¿™ç§æ–¹æ³•æœ€é€‚åˆäºå·²çŸ¥äº§å“æ•°æ®ä½†ä¸çŸ¥é“ç”¨æˆ·æ•°æ®çš„æƒ…å†µã€‚

![](img/28ef1a281838ad749151380fa345d98a.png)

ä½œè€…å›¾ç‰‡

è®©æˆ‘ä»¬ä»æ•°æ®ä¸­æŒ‘é€‰ä¸€ä¸ªç”¨æˆ·ä½œä¸ºæˆ‘ä»¬ç¬¬ä¸€ä¸ªå·²ç»ä½¿ç”¨äº†è¶³å¤Ÿå¤šäº§å“çš„ç”¨æˆ·çš„ä¾‹å­ï¼Œè®©æˆ‘ä»¬åˆ›å»º*è®­ç»ƒ*å’Œ*æµ‹è¯•*å‘é‡ã€‚

```
**# Select a user**
i = 1
train = dtf_train.iloc[i].to_frame(name="y")
test = dtf_test.iloc[i].to_frame(name="y")**# add all the test products but hide the y**
tmp = test.copy()
tmp["y"] = np.nan
train = train.append(tmp)
```

ç°åœ¨æˆ‘ä»¬éœ€è¦ä¼°è®¡ç”¨æˆ·ç»™æ¯ä¸ªç‰¹æ€§çš„æƒé‡ã€‚æˆ‘ä»¬æœ‰*ç”¨æˆ·-äº§å“*å‘é‡å’Œ*äº§å“-ç‰¹æ€§*çŸ©é˜µã€‚

```
**# shapes**
usr = train[["y"]].fillna(0).values.T
prd = dtf_products.drop(["name","genres"],axis=1).values
print("Users", usr.shape, " x  Products", prd.shape)
```

![](img/8bef4742e25be874f48324bdaa9dab09.png)

é€šè¿‡å°†è¿™ä¸¤ä¸ªå¯¹è±¡ç›¸ä¹˜ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ª*ç”¨æˆ·ç‰¹å¾*å‘é‡ï¼Œå…¶ä¸­åŒ…å«äº†ç”¨æˆ·ç»™æ¯ä¸ªç‰¹å¾çš„ä¼°è®¡æƒé‡ã€‚è¿™äº›æƒé‡åº”é‡æ–°åº”ç”¨äº*äº§å“-åŠŸèƒ½*çŸ©é˜µï¼Œä»¥è·å¾—é¢„æµ‹è¯„çº§ã€‚

```
**# usr_ft(users,fatures) = usr(users,products) x prd(products,features)**
usr_ft = np.dot(usr, prd)**# normalize**
weights = usr_ft / usr_ft.sum()**# predicted rating(users,products) = weights(users,fatures) x prd.T(features,products)**
pred = np.dot(weights, prd.T)test = test.merge(pd.DataFrame(pred[0], columns=["yhat"]), how="left", left_index=True, right_index=True).reset_index()
test = test[~test["y"].isna()]
test
```

![](img/d93771c54b4874a2b4854d093df61475.png)

ä½œè€…å›¾ç‰‡

å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä½¿ç”¨ç®€å•çš„ *numpy å¼€å‘äº†è¿™ä¸ªç®€å•çš„æ–¹æ³•ã€‚*åªä½¿ç”¨ raw *tensorflow* ä¹Ÿå¯ä»¥åšåˆ°è¿™ä¸€ç‚¹:

```
import tensorflow as tf**# usr_ft(users,fatures) = usr(users,products) x prd(products,features)**
usr_ft = tf.matmul(usr, prd)**# normalize**
weights = usr_ft / tf.reduce_sum(usr_ft, axis=1, keepdims=True)**# rating(users,products) = weights(users,fatures) x prd.T(features,products)**
pred = tf.matmul(weights, prd.T)
```

å¦‚ä½•**è¯„ä»·**æˆ‘ä»¬çš„é¢„æµ‹æ¨èï¼Ÿæˆ‘é€šå¸¸ä½¿ç”¨[ç²¾ç¡®åº¦](https://en.wikipedia.org/wiki/Accuracy_and_precision)å’Œ[å¹³å‡å€’æ•°æ’å(MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) ã€‚åè€…æ˜¯ä¸€ç§ç»Ÿè®¡åº¦é‡ï¼Œç”¨äºè¯„ä¼°ä»»ä½•æŒ‰æ­£ç¡®æ¦‚ç‡æ’åºçš„å¯èƒ½å“åº”åˆ—è¡¨ã€‚

```
def **mean_reciprocal_rank**(y_test, predicted):
    score = []
    for product in y_test:
        mrr = 1 / (list(predicted).index(product) + 1) if product 
        in predicted else 0
        score.append(mrr)
    return np.mean(score)
```

è¯·æ³¨æ„ï¼ŒæŒ‡æ ‡ä¼šæ ¹æ®æˆ‘ä»¬æ¨èçš„äº§å“æ•°é‡è€Œå˜åŒ–ã€‚å› ä¸ºæˆ‘ä»¬å°†é¢„æµ‹çš„*å‰ k ä¸ª*é¡¹ç›®ä¸*æµ‹è¯•*é›†ä¸­çš„é¡¹ç›®è¿›è¡Œæ¯”è¾ƒï¼Œæ‰€ä»¥é¡ºåºä¹Ÿå¾ˆé‡è¦ã€‚

```
print("--- user", i, "---")top = 5
y_test = test.sort_values("y", ascending=False)["product"].values[:top]
print("y_test:", y_test)predicted = test.sort_values("yhat", ascending=False)["product"].values[:top]
print("predicted:", predicted)true_positive = len(list(set(y_test) & set(predicted)))
print("true positive:", true_positive, "("+str(round(true_positive/top*100,1))+"%)")
print("accuracy:", str(round(**metrics**.**accuracy_score**(y_test,predicted)*100,1))+"%")
print("mrr:", **mean_reciprocal_rank**(y_test, predicted))
```

![](img/e7114e8eb021cedf86b74ee93f56d12a.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬å¾—åˆ°äº† 4 ä¸ªæ­£ç¡®çš„äº§å“ï¼Œä½†è®¢å•ä¸åŒ¹é…ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå‡†ç¡®æ€§å’Œ MRR ä½ã€‚

```
**# See predictions details**
test.merge(
       dtf_products[["name","old","genres"]], left_on="product", 
       right_index=True
).sort_values("yhat", ascending=False)
```

![](img/ba99e07c6b040182c85eeb23f32dcb0c.png)

ä½œè€…å›¾ç‰‡

## ååŒè¿‡æ»¤

<https://en.wikipedia.org/wiki/Collaborative_filtering>**ååŒè¿‡æ»¤æ˜¯åŸºäºç›¸ä¼¼ç”¨æˆ·å–œæ¬¢ç›¸ä¼¼äº§å“çš„å‡è®¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ*ç”¨æˆ· A* å–œæ¬¢*äº§å“ 1* ï¼Œå¹¶ä¸”*ç”¨æˆ· B* ä¸*ç”¨æˆ· A* ç›¸ä¼¼ï¼Œé‚£ä¹ˆ*ç”¨æˆ· B* å¯èƒ½ä¹Ÿä¼šå–œæ¬¢*äº§å“ 1* ã€‚ä¸¤ä¸ªç”¨æˆ·å–œæ¬¢ç›¸ä¼¼çš„äº§å“ï¼Œå°±æ˜¯ç›¸ä¼¼çš„ã€‚**

**![](img/9428cd019a74a65697dfc068567e6634.png)**

**ä½œè€…å›¾ç‰‡**

**è¿™ç§æ–¹æ³•ä¸éœ€è¦äº§å“ç‰¹æ€§å°±èƒ½èµ·ä½œç”¨ï¼Œç›¸åï¼Œå®ƒéœ€è¦æ¥è‡ªè®¸å¤šç”¨æˆ·çš„è®¸å¤šè¯„çº§ã€‚ç»§ç»­æˆ‘ä»¬å¹³å°çš„ä¾‹å­ï¼Œæƒ³è±¡æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªç”¨æˆ·ä¸å†å­¤å•ï¼Œæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„ç”¨æˆ·æ¥åº”ç”¨è¿™ä¸ªæ¨¡å‹ã€‚**

**å½“ç½‘é£åœ¨ 2009 å¹´ä¸¾åŠäº†ä¸€åœºæœ€ä½³ç®—æ³•çš„å…¬å¼€æ¯”èµ›ï¼Œäººä»¬æå‡ºäº†å‡ ç§å®ç°æ–¹å¼æ—¶ï¼ŒååŒè¿‡æ»¤å¼€å§‹æµè¡Œèµ·æ¥ã€‚å®ƒä»¬å¯ä»¥åˆ†ä¸ºä¸¤ç±»:**

*   ****åŸºäºè®°å¿†â€”** ç”¨ç›¸å…³åº¦å¯»æ‰¾ç›¸ä¼¼ç”¨æˆ·ï¼Œ[ä½™å¼¦ç›¸ä¼¼åº¦](https://en.wikipedia.org/wiki/Cosine_similarity)ï¼Œä»¥åŠ[èšç±»](https://en.wikipedia.org/wiki/Cluster_analysis)ã€‚**
*   ****åŸºäºæ¨¡å‹â€”** é€šè¿‡åº”ç”¨ç›‘ç£æœºå™¨å­¦ä¹ å’Œ[çŸ©é˜µåˆ†è§£](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems))æ¥é¢„æµ‹ç”¨æˆ·å°†å¦‚ä½•è¯„ä»·æŸä¸ªäº§å“ï¼ŒçŸ©é˜µåˆ†è§£å°†å¤§çš„*ç”¨æˆ·-äº§å“*çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªè¾ƒå°çš„å› å­ï¼Œåˆ†åˆ«ä»£è¡¨*ç”¨æˆ·*çŸ©é˜µå’Œ*äº§å“*çŸ©é˜µã€‚**

**åœ¨ Python ä¸­ï¼Œå¯¹ç”¨æˆ·æœ€å‹å¥½çš„åŒ…æ˜¯ [*surprise*](https://pypi.org/project/scikit-surprise/) ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„åº“ï¼Œç”¨äºæ„å»ºå’Œåˆ†æå…·æœ‰æ˜¾å¼è¯„çº§æ•°æ®çš„æ¨èç³»ç»Ÿ(ç±»ä¼¼äº *scikit-learn* )ã€‚å®ƒæ—¢å¯ä»¥ç”¨äºåŸºäºè®°å¿†çš„æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥ç”¨äºåŸºäºæ¨¡å‹çš„æ–¹æ³•ã€‚æˆ–è€…ï¼Œå¯ä»¥ä½¿ç”¨ *tensorflow/keras* ä¸ºæ›´å¤æ‚çš„åŸºäºæ¨¡å‹çš„æ–¹æ³•åˆ›å»ºåµŒå…¥ï¼Œè¿™æ­£æ˜¯æˆ‘è¦åšçš„ã€‚**

**é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸‹å½¢å¼çš„æ•°æ®:**

```
train = dtf_train.stack(dropna=True).reset_index().rename(columns={0:"y"})
train.head()
```

**![](img/0c498be51096e62f2f28cc1f0f03483a.png)**

**ä½œè€…çš„å›¾ç‰‡(å¯¹æµ‹è¯•é›†åšåŒæ ·çš„äº‹æƒ…)**

**ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨ç¥ç»ç½‘ç»œçš„åµŒå…¥å±‚æ¥åˆ›å»º*ç”¨æˆ·*å’Œ*äº§å“*çŸ©é˜µã€‚é‡è¦çš„æ˜¯è¦ç†è§£è¾“å…¥æ˜¯ç”¨æˆ·-äº§å“å¯¹ï¼Œè¾“å‡ºæ˜¯è¯„çº§ã€‚å½“é¢„æµ‹ä¸€å¯¹æ–°çš„ç”¨æˆ·-äº§å“æ—¶ï¼Œè¯¥æ¨¡å‹å°†åœ¨*ç”¨æˆ·*åµŒå…¥ç©ºé—´ä¸­æŸ¥æ‰¾ç”¨æˆ·ï¼Œåœ¨*äº§å“*ç©ºé—´ä¸­æŸ¥æ‰¾äº§å“ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦æå‰æŒ‡å®šç”¨æˆ·å’Œäº§å“çš„æ€»æ•°ã€‚**

```
embeddings_size = 50
usr, prd = dtf_users.shape[0], dtf_users.shape[1] **# Users (1,embedding_size)**
xusers_in = layers.Input(name="xusers_in", shape=(1,))xusers_emb = layers.Embedding(name="xusers_emb", input_dim=usr, output_dim=embeddings_size)(xusers_in)xusers = layers.Reshape(name='xusers', target_shape=(embeddings_size,))(xusers_emb) **# Products (1,embedding_size)**
xproducts_in = layers.Input(name="xproducts_in", shape=(1,))xproducts_emb = layers.Embedding(name="xproducts_emb", input_dim=prd, output_dim=embeddings_size)(xproducts_in)xproducts = layers.Reshape(name='xproducts', target_shape=(embeddings_size,))(xproducts_emb) **# Product (1)**
xx = layers.Dot(name='xx', normalize=True, axes=1)([xusers, xproducts]) **# Predict ratings (1)**
y_out = layers.Dense(name="y_out", units=1, activation='linear')(xx) **# Compile**
model = models.Model(inputs=[xusers_in,xproducts_in], outputs=y_out, name="CollaborativeFiltering")
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_percentage_error'])
```

**è¯·æ³¨æ„ï¼Œæˆ‘é€šè¿‡ä½¿ç”¨[å¹³å‡ç»å¯¹è¯¯å·®](https://en.wikipedia.org/wiki/Mean_absolute_error)ä½œä¸ºæŸå¤±ï¼Œå°†æ­¤ç”¨ä¾‹è§†ä¸ºå›å½’é—®é¢˜ï¼Œå³ä½¿æˆ‘ä»¬æ¯•ç«Ÿä¸éœ€è¦åˆ†æ•°æœ¬èº«ï¼Œè€Œæ˜¯éœ€è¦é¢„æµ‹äº§å“çš„æ’åºã€‚**

```
utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)
```

**![](img/4482301bd037f458ecc00ad83794209c.png)**

**ä½œè€…å›¾ç‰‡**

**è®©æˆ‘ä»¬**è®­ç»ƒå’Œæµ‹è¯•**æ¨¡å‹ã€‚**

```
**# Train**
training = model.fit(x=[train["user"], train["product"]], y=train["y"], epochs=100, batch_size=128, shuffle=True, verbose=0, validation_split=0.3)model = training.model**# Test** test["yhat"] = model.predict([test["user"], test["product"]])
test
```

**![](img/e40d3fa0ddd5ddaf4aa6a91affec8b22.png)**

**ä½œè€…å›¾ç‰‡**

**æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯”è¾ƒä¸ºæˆ‘ä»¬äº²çˆ±çš„ç¬¬ä¸€ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ¨èæ¥è¯„ä¼°è¿™äº›é¢„æµ‹(ä¸å‰é¢çš„ä»£ç ç›¸åŒ):**

**![](img/bec1c8310062bb0f5a6902994a3c6cde.png)**

**ä½œè€…å›¾ç‰‡**

**ç›®å‰ï¼Œæ‰€æœ‰æœ€å…ˆè¿›çš„æ¨èç³»ç»Ÿéƒ½åˆ©ç”¨äº†æ·±åº¦å­¦ä¹ ã€‚ç‰¹åˆ«åœ°ï¼Œ**ç¥ç»ååŒè¿‡æ»¤** (2017)ç»“åˆäº†æ¥è‡ªç¥ç»ç½‘ç»œçš„éçº¿æ€§å’ŒçŸ©é˜µåˆ†è§£ã€‚è¯¥æ¨¡å‹ä¸ä»…ç”¨äºä¼ ç»Ÿçš„ååŒè¿‡æ»¤ï¼Œè¿˜ç”¨äºå®Œå…¨è¿æ¥çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä»è€Œå……åˆ†åˆ©ç”¨åµŒå…¥ç©ºé—´ã€‚é¢å¤–çš„éƒ¨åˆ†åº”è¯¥æ•è·çŸ©é˜µåˆ†è§£å¯èƒ½é—æ¼çš„æ¨¡å¼å’Œç‰¹å¾ã€‚**

**![](img/ed9c06c34351c7c1d0dc5ea27c745b3e.png)**

**ä½œè€…å›¾ç‰‡**

**ç”¨ Python çš„æœ¯è¯­æ¥è¯´:**

```
embeddings_size = 50
usr, prd = dtf_users.shape[0], dtf_users.shape[1]**# Input layer**
xusers_in = layers.Input(name="xusers_in", shape=(1,))
xproducts_in = layers.Input(name="xproducts_in", shape=(1,)) **# A) Matrix Factorization
## embeddings and reshape**
cf_xusers_emb = layers.Embedding(name="cf_xusers_emb", input_dim=usr, output_dim=embeddings_size)(xusers_in)
cf_xusers = layers.Reshape(name='cf_xusers', target_shape=(embeddings_size,))(cf_xusers_emb)**## embeddings and reshape**
cf_xproducts_emb = layers.Embedding(name="cf_xproducts_emb", input_dim=prd, output_dim=embeddings_size)(xproducts_in)
cf_xproducts = layers.Reshape(name='cf_xproducts', target_shape=(embeddings_size,))(cf_xproducts_emb)**## product**
cf_xx = layers.Dot(name='cf_xx', normalize=True, axes=1)([cf_xusers, cf_xproducts]) **# B) Neural Network**
**## embeddings and reshape**
nn_xusers_emb = layers.Embedding(name="nn_xusers_emb", input_dim=usr, output_dim=embeddings_size)(xusers_in)
nn_xusers = layers.Reshape(name='nn_xusers', target_shape=(embeddings_size,))(nn_xusers_emb)**## embeddings and reshape**
nn_xproducts_emb = layers.Embedding(name="nn_xproducts_emb", input_dim=prd, output_dim=embeddings_size)(xproducts_in)
nn_xproducts = layers.Reshape(name='nn_xproducts', target_shape=(embeddings_size,))(nn_xproducts_emb)**## concat and dense**
nn_xx = layers.Concatenate()([nn_xusers, nn_xproducts])
nn_xx = layers.Dense(name="nn_xx", units=int(embeddings_size/2), activation='relu')(nn_xx) **# Merge A & B**
y_out = layers.Concatenate()([cf_xx, nn_xx])
y_out = layers.Dense(name="y_out", units=1, activation='linear')(y_out)**# Compile**
model = models.Model(inputs=[xusers_in,xproducts_in], outputs=y_out, name="Neural_CollaborativeFiltering")
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_percentage_error'])
```

**![](img/d42ffbfec76e53de9f59a4f4eae7f1f5.png)**

**utils.plot_model(modelï¼Œto_file='model.png 'ï¼Œshow_shapes=Trueï¼Œshow_layer_names=True)**

**æ‚¨å¯ä»¥ä½¿ç”¨ä¸ä»¥å‰ç›¸åŒçš„ä»£ç è¿è¡Œå®ƒï¼Œå¹¶æ£€æŸ¥å®ƒæ˜¯å¦æ¯”ä¼ ç»Ÿçš„ååŒè¿‡æ»¤æ‰§è¡Œå¾—æ›´å¥½ã€‚**

**![](img/1e2389449724900f99eb5fede20b2792.png)**

**ä½œè€…å›¾ç‰‡**

## **æ··åˆæ¨¡å‹**

**è®©æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ç°å®ä¸–ç•Œæä¾›äº†ä»€ä¹ˆæ ·çš„æ•°æ®:**

*   ****ç›®æ ‡å˜é‡** â€”è¯„çº§å¯ä»¥æ˜¯æ˜¾å¼çš„(å³ç”¨æˆ·ç•™ä¸‹åé¦ˆ)æˆ–éšå¼çš„(å³å¦‚æœç”¨æˆ·çœ‹å®Œæ•´éƒ¨ç”µå½±ï¼Œåˆ™å‡è®¾æ˜¯æ­£é¢åé¦ˆ)ï¼Œæ— è®ºå¦‚ä½•å®ƒä»¬éƒ½æ˜¯å¿…è¦çš„ã€‚**
*   ****äº§å“ç‰¹å¾** â€”é¡¹ç›®(å³ç”µå½±ç±»å‹)çš„æ ‡ç­¾å’Œæè¿°ï¼Œä¸»è¦ç”¨äºåŸºäºå†…å®¹çš„æ–¹æ³•ã€‚**
*   ****ç”¨æˆ·èµ„æ–™** â€”å…³äºç”¨æˆ·çš„æè¿°æ€§ä¿¡æ¯å¯ä»¥æ˜¯äººå£ç»Ÿè®¡ä¿¡æ¯(å³æ€§åˆ«å’Œå¹´é¾„)æˆ–è¡Œä¸ºä¿¡æ¯(å³åå¥½ã€åœ¨å±å¹•ä¸Šçš„å¹³å‡æ—¶é—´ã€æœ€é¢‘ç¹çš„ä½¿ç”¨æ—¶é—´)ï¼Œé€šå¸¸ç”¨äºåŸºäºçŸ¥è¯†çš„æ¨èã€‚**
*   ****ä¸Šä¸‹æ–‡** â€”å…³äºè¯„çº§æƒ…å†µçš„é™„åŠ ä¿¡æ¯(å³æ—¶é—´ã€åœ°ç‚¹ã€æœç´¢å†å²)ï¼Œé€šå¸¸ä¹ŸåŒ…å«åœ¨åŸºäºçŸ¥è¯†çš„æ¨èä¸­ã€‚**

**ç°ä»£æ¨èç³»ç»Ÿåœ¨é¢„æµ‹æˆ‘ä»¬çš„å£å‘³æ—¶ï¼Œä¼šå°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚ä¾‹å¦‚ï¼ŒYouTube æ¨èä¸‹ä¸€ä¸ªè§†é¢‘æ—¶ï¼Œä½¿ç”¨äº†è°·æ­ŒçŸ¥é“çš„å…³äºä½ çš„æ‰€æœ‰ä¿¡æ¯ï¼Œè€Œè°·æ­ŒçŸ¥é“çš„å¾ˆå¤šã€‚**

**åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘æœ‰äº§å“ç‰¹æ€§å’Œå…³äºç”¨æˆ·ä½•æ—¶ç»™å‡ºè¯„çº§çš„æ•°æ®ï¼Œæˆ‘å°†æŠŠå®ƒä»¬ç”¨ä½œä¸Šä¸‹æ–‡(æˆ–è€…ï¼Œå®ƒä¹Ÿå¯ä»¥ç”¨æ¥å»ºç«‹ç”¨æˆ·ç®€æ¡£)ã€‚**

```
features = dtf_products.drop(["genres","name"], axis=1).columns
print(features)context = dtf_context.drop(["user","product"], axis=1).columns
print(context)
```

**![](img/4f8446f5dc40eaa0ad0fdefe356be766.png)**

**ä½œè€…å›¾ç‰‡**

**è®©æˆ‘ä»¬å°†é¢å¤–ä¿¡æ¯æ·»åŠ åˆ°*è®­ç»ƒ*é›†åˆä¸­:**

```
train = dtf_train.stack(dropna=True).reset_index().rename(columns={0:"y"})**## add features**
train = train.merge(dtf_products[features], how="left", left_on="product", right_index=True)**## add context**
train = train.merge(dtf_context, how="left")
```

**![](img/eec5e4f9c9d5cc4d20520ae8f39e6801.png)**

**ä½œè€…å›¾ç‰‡**

**è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å¯¹*æµ‹è¯•*é›†åšåŒæ ·çš„äº‹æƒ…ï¼Œä½†æ˜¯å¦‚æœæ‚¨æƒ³è¦æ¨¡æ‹ŸçœŸå®çš„ç”Ÿäº§ï¼Œæ‚¨åº”è¯¥ä¸ºä¸Šä¸‹æ–‡æ’å…¥ä¸€ä¸ªé™æ€å€¼ã€‚ç®€å•æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬åœ¨å‘¨ä¸€æ™šä¸Šä¸ºæˆ‘ä»¬å¹³å°çš„ç”¨æˆ·åšé¢„æµ‹ï¼Œä¸Šä¸‹æ–‡å˜é‡åº”è¯¥æ˜¯*ç™½å¤©=0* å’Œ*å‘¨æœ«=0* ã€‚**

**ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†æ„å»º**ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ··åˆæ¨¡å‹**çš„æ‰€æœ‰è¦ç´ ã€‚ç¥ç»ç½‘ç»œçš„çµæ´»æ€§å…è®¸æˆ‘ä»¬æ·»åŠ ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„ä¸œè¥¿ï¼Œæ‰€ä»¥æˆ‘å°†é‡‡ç”¨ç¥ç»ååŒè¿‡æ»¤ç½‘ç»œç»“æ„ï¼Œå¹¶å°½å¯èƒ½å¤šåœ°åŒ…å«æ¨¡å—ã€‚**

**![](img/b9d127a0483b9167a1c4dc13cb029f61.png)**

**ä½œè€…å›¾ç‰‡**

**å°½ç®¡ä»£ç çœ‹èµ·æ¥å¾ˆéš¾ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨å·²ç»ä½¿ç”¨çš„åŸºç¡€ä¸Šå¢åŠ äº†å‡ å±‚ã€‚**

```
embeddings_size = 50
usr, prd = dtf_users.shape[0], dtf_users.shape[1]
feat = len(features)
ctx = len(context) **################### COLLABORATIVE FILTERING ########################
# Input layer**
xusers_in = layers.Input(name="xusers_in", shape=(1,))
xproducts_in = layers.Input(name="xproducts_in", shape=(1,))**# A) Matrix Factorization
## embeddings and reshape**
cf_xusers_emb = layers.Embedding(name="cf_xusers_emb", input_dim=usr, output_dim=embeddings_size)(xusers_in)
cf_xusers = layers.Reshape(name='cf_xusers', target_shape=(embeddings_size,))(cf_xusers_emb)**## embeddings and reshape**
cf_xproducts_emb = layers.Embedding(name="cf_xproducts_emb", input_dim=prd, output_dim=embeddings_size)(xproducts_in)
cf_xproducts = layers.Reshape(name='cf_xproducts', target_shape=(embeddings_size,))(cf_xproducts_emb)**## product**
cf_xx = layers.Dot(name='cf_xx', normalize=True, axes=1)([cf_xusers, cf_xproducts])**# B) Neural Network
## embeddings and reshape**
nn_xusers_emb = layers.Embedding(name="nn_xusers_emb", input_dim=usr, output_dim=embeddings_size)(xusers_in)
nn_xusers = layers.Reshape(name='nn_xusers', target_shape=(embeddings_size,))(nn_xusers_emb)**## embeddings and reshape**
nn_xproducts_emb = layers.Embedding(name="nn_xproducts_emb", input_dim=prd, output_dim=embeddings_size)(xproducts_in)
nn_xproducts = layers.Reshape(name='nn_xproducts', target_shape=(embeddings_size,))(nn_xproducts_emb)**## concat and dense**
nn_xx = layers.Concatenate()([nn_xusers, nn_xproducts])
nn_xx = layers.Dense(name="nn_xx", units=int(embeddings_size/2), activation='relu')(nn_xx) **######################### CONTENT BASED ############################
# Product Features**
features_in = layers.Input(name="features_in", shape=(feat,))
features_x = layers.Dense(name="features_x", units=feat, activation='relu')(features_in) **######################## KNOWLEDGE BASED ###########################
# Context**
contexts_in = layers.Input(name="contexts_in", shape=(ctx,))
context_x = layers.Dense(name="context_x", units=ctx, activation='relu')(contexts_in) **########################## OUTPUT ##################################
# Merge all**
y_out = layers.Concatenate()([cf_xx, nn_xx, features_x, context_x])
y_out = layers.Dense(name="y_out", units=1, activation='linear')(y_out)**# Compile**
model = models.Model(inputs=[xusers_in,xproducts_in, features_in, contexts_in], outputs=y_out, name="Hybrid_Model")
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_percentage_error'])
```

**![](img/b94b881b3fce60e5f0b6095a649e7c9d.png)**

**utils.plot_model(modelï¼Œto_file='model.png 'ï¼Œshow_shapes=Trueï¼Œshow_layer_names=True)**

**è¿™ç§æ··åˆæ¨¡å‹éœ€è¦æ›´å¤šçš„è¾“å…¥ï¼Œæ‰€ä»¥ä¸è¦å¿˜è®°è¾“å…¥æ–°æ•°æ®:**

```
**# Train**
training = model.fit(x=[train["user"], train["product"], **train[features]**, **train[context]**], y=train["y"], 
                     epochs=100, batch_size=128, shuffle=True, verbose=0, validation_split=0.3)model = training.model**# Test**
test["yhat"] = model.predict([test["user"], test["product"], **test[features]**, **test[context]**])
```

**![](img/ce83a3cfd409d7d3a96b4d2719b9aba4.png)**

**ä½œè€…å›¾ç‰‡**

**ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå¯¹äºè¯¥ç‰¹å®šç”¨æˆ·ï¼Œæ··åˆæ¨¡å‹è·å¾—äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼Œå› ä¸ºä¸‰ä¸ªé¢„æµ‹äº§å“å…·æœ‰åŒ¹é…çš„è®¢å•ã€‚**

## **ç»“è®º**

**è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªæ•™ç¨‹ï¼Œå±•ç¤ºäº†å¦‚ä½•ç”¨ç¥ç»ç½‘ç»œè®¾è®¡å’Œæ„å»ºæ¨èç³»ç»Ÿã€‚æˆ‘ä»¬çœ‹åˆ°äº†åŸºäºæ•°æ®å¯ç”¨æ€§çš„ä¸åŒç”¨ä¾‹:å¯¹å•ç”¨æˆ·åœºæ™¯åº”ç”¨åŸºäºå†…å®¹çš„æ–¹æ³•ï¼Œå¯¹å¤šç”¨æˆ·äº§å“åº”ç”¨åä½œè¿‡æ»¤ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬äº†è§£äº†å¦‚ä½•ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ”¹è¿›ä¼ ç»ŸæŠ€æœ¯ï¼Œå¹¶æ„å»ºå¯ä»¥åŒ…å«ä¸Šä¸‹æ–‡å’Œä»»ä½•å…¶ä»–é™„åŠ ä¿¡æ¯çš„ç°ä»£æ··åˆæ¨èç³»ç»Ÿã€‚**

**æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚**

> **ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ**

> **æœ¬æ–‡æ˜¯ç³»åˆ—**ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ **çš„ä¸€éƒ¨åˆ†ï¼Œå‚è§:**

**</deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0>  </machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec>  </machine-learning-with-python-regression-complete-tutorial-47268e546cea>  </clustering-geospatial-data-f0584f0b04ec> **
# ä½¿ç”¨ PyMC3 çš„è´å¶æ–¯å›å½’

> åŸæ–‡ï¼š<https://towardsdatascience.com/bayesian-regression-using-pymc3-e279ccc81d20>

## å¦‚ä½•ä½¿ç”¨ PyMC3 åŒ…åœ¨ Python ä¸­å®ç°è´å¶æ–¯å›å½’

![](img/8da1b56def8c4a58f04620d379dce2af.png)

Joachim SchnÃ¼rle åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šçš„ç…§ç‰‡

# èƒŒæ™¯

[**PyMC3**](https://www.pymc.io/welcome.html) (ç°åœ¨ç®€ç§° PyMC)æ˜¯ä¸€ä¸ªè´å¶æ–¯å»ºæ¨¡åŒ…ï¼Œä½¿æˆ‘ä»¬ä½œä¸ºæ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿè½»æ¾åœ°æ‰§è¡Œ [**è´å¶æ–¯æ¨ç†**](https://en.wikipedia.org/wiki/Bayesian_inference) ã€‚

åœ¨å¼•æ“ç›–ä¸‹ï¼ŒPyMC3 ä½¿ç”¨[**ã€MCMCã€‘**](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)**çš„æ–¹æ³•è®¡ç®—åéªŒåˆ†å¸ƒã€‚ç°åœ¨ï¼Œè¿™ç§æ–¹æ³•éå¸¸å¤æ‚ï¼Œéœ€è¦å¦å¤–ä¸€ç¯‡æ–‡ç« æ¥å…¨é¢ä»‹ç»ã€‚å› æ­¤ï¼Œæˆ‘åœ¨è¿™é‡Œé“¾æ¥äº†ä¸€ä¸ªå¾ˆå¥½åœ°è§£é‡Šäº†è¿™ä¸ªè¯é¢˜çš„å¸–å­[ã€‚](/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50)**

**æ‚¨å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆå°è£…ä½¿ç”¨ MCMCï¼Ÿå—¯è¿™æ˜¯ä¸ºäº†ç»•è¿‡ [**è®¡ç®—**](https://www.reddit.com/r/askmath/comments/cghi5c/what_is_an_intractable_integral_and_why_this/) **[**å½’ä¸€åŒ–å¸¸æ•°**](https://en.wikipedia.org/wiki/Normalizing_constant#Bayes'_theorem) ä¸­çš„ [**è´å¶æ–¯å®šç†**](https://en.wikipedia.org/wiki/Bayes'_theorem)****

**![](img/18feaecf9728b45456c78d0768346a6c.png)**

**ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚**

**å…¶ä¸­ *P(H | D)* æ˜¯ [**åéªŒ**](https://en.wikipedia.org/wiki/Posterior_probability) ï¼Œ *P(H)* æ˜¯ [**å…ˆéªŒ**](https://en.wikipedia.org/wiki/Prior_probability) ï¼Œ *P(D | H)* æ˜¯ [**ä¼¼ç„¶**](https://en.wikipedia.org/wiki/Likelihood_function) ï¼Œ *P(D)* æ˜¯å½’ä¸€åŒ–å¸¸æ•°ï¼Œå®šä¹‰ä¸º:**

**![](img/b98f8d5ecdb0845861836aa96d433dd0.png)**

**ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚**

**å¯¹äºè®¸å¤šé—®é¢˜æ¥è¯´ï¼Œè¿™ä¸ªç§¯åˆ†è¦ä¹ˆæ²¡æœ‰å°é—­å½¢å¼çš„è§£ï¼Œè¦ä¹ˆæ— æ³•è®¡ç®—ã€‚å› æ­¤ï¼Œåƒ MCMC è¿™æ ·çš„æ–¹æ³•è¢«å¼€å‘å‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å…è®¸æˆ‘ä»¬ä½¿ç”¨è´å¶æ–¯æ–¹æ³•ã€‚**

**å¦‚æœä½ ä¸ç†Ÿæ‚‰è´å¶æ–¯å®šç†ï¼Œæˆ‘æ¨èä½ çœ‹çœ‹æˆ‘ä»¥å‰å…³äºè¿™ä¸ªä¸»é¢˜çš„æ–‡ç« :**

**[](https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333)  

è¿˜æœ‰å¦ä¸€ç§å«åš [**å…±è½­å…ˆéªŒ**](https://en.wikipedia.org/wiki/Conjugate_prior) çš„æ–¹æ³•ä¹Ÿè§£å†³äº†è¿™ä¸ªæ£˜æ‰‹çš„é—®é¢˜ï¼Œä½†æ˜¯å®ƒçš„å¯å¡‘æ€§ä¸å¦‚ MCMCã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå…±è½­å…ˆéªŒçš„çŸ¥è¯†ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¥å‰çš„æ–‡ç« :

[](/bayesian-conjugate-priors-simply-explained-747218be0f70)  

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ PyMC3 åŒ…å®ç° [**è´å¶æ–¯çº¿æ€§å›å½’**](https://en.wikipedia.org/wiki/Bayesian_linear_regression) ï¼Œå¹¶å¿«é€Ÿæµè§ˆä¸€ä¸‹å®ƒä¸æ™®é€šçš„ [**çº¿æ€§å›å½’**](https://en.wikipedia.org/wiki/Linear_regression) æœ‰ä½•ä¸åŒã€‚å¦‚æœä½ æƒ³çŸ¥é“å¦‚ä½•ä»é›¶å¼€å§‹å®ç°è´å¶æ–¯çº¿æ€§å›å½’ï¼Œé‚£ä¹ˆçœ‹çœ‹æˆ‘æœ€è¿‘çš„å¸–å­:

[](/bayesian-regression-from-scratch-a1fe19ff64c)  

# æ¦‚è¿°:è´å¶æ–¯ä¸é¢‘ç‡ä¸»ä¹‰å›å½’

ä»æœ¬è´¨ä¸Šæ¥è¯´ï¼Œ [**Frequentist**](https://en.wikipedia.org/wiki/Frequentist_inference) å’Œ Bayesian å›å½’æ–¹æ³•ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºå®ƒä»¬å¦‚ä½•å¤„ç†å‚æ•°ã€‚åœ¨é¢‘ç‡ç»Ÿè®¡ä¸­ï¼Œçº¿æ€§å›å½’æ¨¡å‹çš„å‚æ•°æ˜¯å›ºå®šçš„ï¼Œè€Œåœ¨è´å¶æ–¯ç»Ÿè®¡ä¸­ï¼Œå®ƒä»¬æ˜¯éšæœºå˜é‡ã€‚

é¢‘ç‡ä¸»ä¹‰è€…ä½¿ç”¨[](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)**çš„æ–¹æ³•æ¥æ¨å¯¼çº¿æ€§å›å½’æ¨¡å‹çš„å€¼ã€‚MLE çš„ç»“æœæ˜¯æ¯ä¸ªå‚æ•°çš„å•ä¸€å›ºå®šå€¼ã€‚**

**ç„¶è€Œï¼Œåœ¨è´å¶æ–¯ä¸–ç•Œä¸­ï¼Œä½ çš„å‚æ•°æœ‰ä¸€ä¸ªå€¼çš„åˆ†å¸ƒï¼Œå®ƒä»¬å¯èƒ½æœ‰ä¸€å®šçš„æ¦‚ç‡ã€‚ç„¶åä½¿ç”¨æ›´å¤šçš„æ•°æ®æ›´æ–°è¿™ä¸ªåˆ†å¸ƒï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ›´åŠ ç¡®å®šå‚æ•°çš„å–å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º [**è´å¶æ–¯æ›´æ–°**](https://en.wikipedia.org/wiki/Bayesian_inference) ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šä¿¡æ¯:**

**[](/bayesian-updating-simply-explained-c2ed3e563588)  

è¿™æ˜¯å¯¹è´å¶æ–¯å’Œé¢‘ç‡ä¸»ä¹‰å›å½’ä¹‹é—´çš„ä¸»è¦åŒºåˆ«çš„ä¸€ç¥ã€‚å¦‚æœä½ æƒ³è¦æ›´æ·±å…¥çš„è§‚ç‚¹ï¼Œæœ‰å¾ˆå¤šèµ„æºå¯ä»¥æ¯”æˆ‘è§£é‡Šå¾—æ›´å¥½ï¼

# ä½¿ç”¨ PyMC3

## åŒ…è£…

é¦–å…ˆï¼Œæˆ‘ä»¬è£…å…¥æˆ‘ä»¬çš„åŒ…:

```
# Import pyMC3 and also arviz for visualisation
import pymc3 as pm
import arviz as az# Import the other core data science packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from scipy.stats import norm
import statsmodels.formula.api as smf
```

ä½ å¯èƒ½éœ€è¦å®‰è£… PyMC3 å’Œ[T5 ArviZ](https://www.arviz.org/en/latest/)**ã€‚ä¸ºæ­¤ï¼Œåªéœ€éµå¾ªä»–ä»¬ç½‘ç«™ä¸Šçš„å®‰è£…è¯´æ˜ã€‚**

## æ•°æ®

ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ sklearn çš„ *make_regression* å‡½æ•°ç”Ÿæˆä¸€äº›æ•°æ®:

```
# Generate data
x, y = datasets.make_regression(n_samples=10_000,
                                n_features=1,
                                noise=10,
                                bias=5)# Create the dataframe
data = pd.DataFrame(list(zip(x.flatten(), y)),columns =['x', 'y'])# Plot the data
fig, ax = plt.subplots(figsize=(9,5))
ax.scatter(data['x'], data['y'])
ax.ticklabel_format(style='plain')
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.show()
```

![](img/c0853b6d2ccf9c42ec2e851053e6a799.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

## é¢‘ç‡å›å½’çº¿

æˆ‘ä»¬ä½¿ç”¨ [**ã€æ™®é€šæœ€å°äºŒä¹˜æ³•(OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) çš„æ–¹æ³•ç»˜åˆ¶é¢‘ç‡çº¿æ€§å›å½’çº¿:

```
# OLS line
formula = 'y ~ x'
results = smf.ols(formula, data=data).fit()
results.params# Get our equation of the OLS line
inter = results.params['Intercept']
slope = results.params['x']
x_vals = np.arange(min(x), max(x), 0.1)
ols_line = inter + slope * x_vals# Plot the target against our feature with the OLS regression
fig, ax = plt.subplots(figsize=(9,5))
ax.scatter(data['x'], data['y'])
ax.plot(x_vals, ols_line,label='OLS Fit', color='red')
ax.ticklabel_format(style='plain')
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.legend(fontsize=16)
plt.show()
```

![](img/462c2f2b3f4f940dc6dc6ef61a914082.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

## PyMC3: 100 ä¸ªæ ·æœ¬

è¦ä½¿ç”¨ PyMC3ï¼Œæˆ‘ä»¬å¿…é¡»å¯åŠ¨ä¸€ä¸ªæ¨¡å‹ï¼Œé€‰æ‹©æˆ‘ä»¬çš„å…ˆéªŒï¼Œå¹¶å‘Šè¯‰æ¨¡å‹æˆ‘ä»¬çš„åéªŒåº”è¯¥æ˜¯ä»€ä¹ˆåˆ†å¸ƒ:

```
# Start our model
with pm.Model() as model_100: # Define the priors on each parameter:
    grad = pm.Uniform("grad",
                      lower=results.params['x']*0.5,
                      upper=results.params['x']*1.5)

    inter = pm.Uniform("inter",
                       lower=results.params['Intercept']*0.5,
                       upper=results.params['Intercept']*1.5)

    sigma = pm.Uniform("sigma",
                       lower=results.resid.std()*0.5,\
                       upper=results.resid.std()*1.5)

    # Linear regression line
    mean = inter + grad*data['x']

    # Describe the distribution of our conditional output
    y = pm.Normal('y', mu = mean, sd = sigma, observed = data['y']) # Run the sampling using pymc3 for 100 samples
    trace_100 = pm.sample(100,return_inferencedata=True)
```

è¿™æ®µä»£ç å°†è¿è¡Œ MCMC é‡‡æ ·å™¨æ¥è®¡ç®—æ¯ä¸ªå‚æ•°çš„åéªŒæ¦‚ç‡ã€‚åŒæ ·ï¼Œå¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£ MCMC æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè¯·å‚è€ƒèƒŒæ™¯éƒ¨åˆ†çš„é“¾æ¥ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥ç»˜åˆ¶æ¯ä¸ªå‚æ•°çš„åéªŒåˆ†å¸ƒ:

```
with model_100:
    az.plot_posterior(trace_100,
                      var_names=['grad', 'inter', 'sigma'],
                      textsize=18,
                      point_estimate='mean',
                      rope_color='black')
```

![](img/916a4d962982cb18879a989cbb8a497a.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

è¿™äº›åéªŒåˆ†å¸ƒçš„å¹³å‡å€¼ä¸ OLS ä¼°è®¡å€¼ç›¸åŒï¼Œä½†æ˜¯è¿™ä¸æ˜¯å‚æ•°å¯ä»¥é‡‡ç”¨çš„å”¯ä¸€å€¼ã€‚å¦‚ä½ æ‰€è§ï¼Œæœ‰å¾ˆå¤šå€¼ã€‚è¿™æ˜¯è´å¶æ–¯çº¿æ€§å›å½’èƒŒåçš„è¦ç‚¹ã€‚

HDI ä»£è¡¨ [**é«˜å¯†åº¦åŒºé—´**](https://stats.stackexchange.com/questions/148439/what-is-a-highest-density-region-hdr) ï¼Œæè¿°äº†æˆ‘ä»¬åœ¨å‚æ•°ä¼°è®¡ä¸­çš„ç¡®å®šæ€§ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºäººç±»å‘å±•æŒ‡æ•°çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘åœ¨[T21ã€è´å¶æ–¯å¯ä¿¡åŒºé—´ ä¸Šçš„åšæ–‡:](https://en.wikipedia.org/wiki/Credible_interval)

[](/bayesian-credible-intervals-simply-explained-24989c9259a3)  

è¿™ä¸ªæ¨¡æ‹Ÿåªä½¿ç”¨äº†æˆ‘ä»¬æ•°æ®ä¸­çš„ 100 ä¸ªæ ·æœ¬ã€‚ä¸è´å¶æ–¯æ–¹æ³•ä¸€æ ·ï¼Œæˆ‘ä»¬éšç€æ›´å¤šçš„æ•°æ®å˜å¾—æ›´åŠ ç¡®å®šã€‚

## PyMC3: 10ï¼Œ000 ä¸ªæ ·æœ¬

è®©æˆ‘ä»¬å†æ¬¡è¿è¡Œè¯¥è¿‡ç¨‹ï¼Œä½†ç°åœ¨ä½¿ç”¨ 10ï¼Œ000 ä¸ªæ ·æœ¬:

```
# Start our model
with pm.Model() as model_10_100: # Define the priors on each parameter:
    grad = pm.Uniform("grad",
                      lower=results.params['x']*0.5,
                      upper=results.params['x']*1.5)

    inter = pm.Uniform("inter",
                       lower=results.params['Intercept']*0.5,
                       upper=results.params['Intercept']*1.5)

    sigma = pm.Uniform("sigma",
                       lower=results.resid.std()*0.5,
                       upper=results.resid.std()*1.5)

    # Linear regression line
    mean = inter + grad*data['x']

    # Describe the distribution of our conditional output
    y = pm.Normal('y', mu = mean, sd = sigma, observed = data['y']) # Run the sampling using pymc3 for 10,000 samples
    trace_10_000 = pm.sample(10_000,return_inferencedata=True)
```

è®©æˆ‘ä»¬å†æ¬¡æŸ¥çœ‹å‚æ•°çš„åéªŒåˆ†å¸ƒ:

```
with model_10_100:
    az.plot_posterior(trace_10_000,
                      var_names=['grad', 'inter', 'sigma'],
                      textsize=18,
                      point_estimate='mean',
                      rope_color='black')
```

![](img/9aaaed10469bf07d0825bbd97240a5e8.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

å¹³å‡é¢„æµ‹æ²¡æœ‰æ”¹å˜ï¼Œä½†æ˜¯éšç€æˆ‘ä»¬æ›´åŠ ç¡®å®šå‚æ•°çš„åˆ†å¸ƒï¼Œæ€»ä½“ä¸Šåˆ†å¸ƒå˜å¾—æ›´åŠ å¹³æ»‘å’Œç´§å¯†ã€‚

## å¯†ç 

è¦æŸ¥çœ‹æˆ‘åœ¨è¿™ç¯‡æ–‡ç« ä¸­ä½¿ç”¨çš„å…¨éƒ¨ä»£ç ï¼Œè¯·ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹ GitHub ä¸Šçš„ç¬”è®°æœ¬:

[](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/pymc3_tutorial.ipynb)  

# ç»“è®º

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æµè§ˆäº†è´å¶æ–¯ç»Ÿè®¡çš„ä¸»è¦åŸåˆ™ï¼Œå¹¶è§£é‡Šäº†å®ƒå¦‚ä½•é‡‡å–ä¸åŒäºé¢‘ç‡ç»Ÿè®¡çš„çº¿æ€§å›å½’æ–¹æ³•ã€‚ç„¶åæˆ‘ä»¬çœ‹äº†ä¸€ä¸ªåŸºæœ¬çš„ä¾‹å­ï¼Œå¦‚ä½•ä½¿ç”¨ PyMC3 åŒ…è¿›è¡Œè´å¶æ–¯å›å½’ã€‚

# å’Œæˆ‘è”ç³»ï¼

*   è¦åœ¨åª’ä½“ä¸Šé˜…è¯»æ— é™çš„æ•…äº‹ï¼Œè¯·åŠ¡å¿…åœ¨æ­¤æ³¨å†Œï¼T3*ğŸ’œ*
*   [*å½“æˆ‘åœ¨è¿™é‡Œå‘å¸ƒæ³¨å†Œé‚®ä»¶é€šçŸ¥æ—¶ï¼Œå¯ä»¥è·å¾—æ›´æ–°ï¼*](/subscribe/@egorhowell) ğŸ˜€
*   [*é¢†è‹±*](https://www.linkedin.com/in/egor-howell-092a721b3/) ğŸ‘”
*   [*æ¨ç‰¹*](https://twitter.com/EgorHowell) ğŸ–Š
*   [*github*](https://github.com/egorhowell)*ğŸ–¥*
*   *[](https://www.kaggle.com/egorphysics)**ğŸ…***

> ***(æ‰€æœ‰è¡¨æƒ…ç¬¦å·ç”± [OpenMoji](https://openmoji.org/) è®¾è®¡â€”â€”å¼€æºè¡¨æƒ…ç¬¦å·å’Œå›¾æ ‡é¡¹ç›®ã€‚è®¸å¯è¯: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/#)*******
# è´å¶æ–¯å›å½’ä»é›¶å¼€å§‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/bayesian-regression-from-scratch-a1fe19ff64c>

## ä½¿ç”¨ Python ä»åŸºæœ¬åŸç†å¯¼å‡ºè´å¶æ–¯çº¿æ€§å›å½’

![](img/49bb02b1c8963841db10b4a6397a0dcf.png)

å…‹åˆ©å§†Â·ç©†è¨åˆ©è«å¤«åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šçš„ç…§ç‰‡

# ä»‹ç»

[**çº¿æ€§å›å½’**](https://en.wikipedia.org/wiki/Linear_regression) æ˜¯æ•°æ®ç§‘å­¦ä¸­æœ€çŸ¥åçš„ç®—æ³•ï¼Œç„¶è€Œå®ƒæœ‰ä¸æ­¢ä¸€ä¸ªç‰ˆæœ¬ã€‚å¤§å¤šæ•°äººä½¿ç”¨çš„ç‰ˆæœ¬æ¥è‡ªç»Ÿè®¡å­¦å®¶<https://en.wikipedia.org/wiki/Frequentist_inference>**çš„è§£é‡Šï¼Œä½†è¿˜æœ‰ä¸€ä¸ªç‰ˆæœ¬æ¥è‡ªè´å¶æ–¯<https://en.wikipedia.org/wiki/Bayesian_statistics>**å­¦æ´¾ã€‚****

****åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å¤ä¹  [**è´å¶æ–¯å®šç†**](https://en.wikipedia.org/wiki/Bayes'_theorem) ï¼Œé¢‘æ•°ç»Ÿè®¡å’Œè´å¶æ–¯ç»Ÿè®¡çš„åŒºåˆ«ï¼Œæœ€åç”¨ Python ä»å¤´å¼€å§‹å®ç° [**è´å¶æ–¯çº¿æ€§å›å½’**](https://en.wikipedia.org/wiki/Bayesian_linear_regression) ã€‚****

> ****æ³¨æ„:åœ¨æ•´ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å‡è®¾è¯»è€…å¯¹è´å¶æ–¯ç»Ÿè®¡å’Œçº¿æ€§å›å½’æœ‰åŸºæœ¬çš„äº†è§£ã€‚æˆ‘ç¡®å®é‡è¿°äº†è¿™äº›è¯é¢˜ï¼Œä½†æ˜¯æ²¡æœ‰æ·±å…¥åˆ°ä¸€ä¸ªå…¨æ–°çš„è¯»è€…å¯èƒ½å®Œå…¨æŒæ¡å®ƒä»¬çš„ç¨‹åº¦ã€‚****

# ****è´å¶æ–¯æ¨ç†é‡å†™****

## ****è´å¶æ–¯å®šç†****

****è´å¶æ–¯å®šç†å†™å¦‚ä¸‹:****

****![](img/18feaecf9728b45456c78d0768346a6c.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

*   *******P(H)*** æ˜¯ [**ä¹‹å‰çš„**](https://en.wikipedia.org/wiki/Prior_probability)****
*   *******ã€D | Hã€‘***æ˜¯ [**çš„å¯èƒ½æ€§**](https://en.wikipedia.org/wiki/Likelihood_function)****
*   *******ã€P(H | D)ã€‘***æ˜¯ [**çš„åè·¯**](https://en.wikipedia.org/wiki/Posterior_probability)****
*   *******ã€P(D)ã€‘***æ˜¯ [**å½’ä¸€åŒ–å¸¸æ•°**](https://en.wikipedia.org/wiki/Normalizing_constant#Bayes'_theorem) ï¼Œå…¶ä¸­æ˜¯å¯èƒ½æ€§ä¸å…ˆéªŒçš„ä¹˜ç§¯ä¹‹å’Œï¼Œä¹Ÿç§°ä¸º [**å…¨æ¦‚ç‡å®šå¾‹**](https://en.wikipedia.org/wiki/Law_of_total_probability) :****

****![](img/94e7b2033ce34bf81d65ba638d8b364d.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

****å¦‚æœä½ ä¸ç†Ÿæ‚‰è´å¶æ–¯å®šç†ï¼Œæˆ‘å¼ºçƒˆæ¨èä½ çœ‹çœ‹æˆ‘ä»¥å‰å…³äºè¿™ä¸ªä¸»é¢˜çš„æ–‡ç« :****

****<https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333>  

## è´å¶æ–¯æ›´æ–°

è´å¶æ–¯å®šç†ç”¨äºæ ¹æ®æ–°æ•°æ®æ›´æ–°æˆ‘ä»¬å¯¹æŸä¸€äº‹ä»¶çš„ä¿¡å¿µï¼Œä½¿ç”¨ä»¥ä¸‹å…¬å¼:

![](img/621962575384296a6543142db0a72f26.png)

ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚

åœ¨æˆ‘ä»¬è®¡ç®—åéªŒæ¦‚ç‡åï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè·å¾—å…³äºæˆ‘ä»¬è¯•å›¾å»ºæ¨¡çš„æ–°æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ—§çš„åéªŒæ•°æ®ä½œä¸ºæ–°çš„å…ˆéªŒæ•°æ®ï¼Œç”¨è¿™ä¸ªæ–°æ•°æ®è®¡ç®—æ–°çš„åéªŒæ•°æ®ã€‚è¿™ä¸ªç”¨æ–°æ•°æ®æ›´æ–°å…ˆéªŒçŸ¥è¯†çš„è¿‡ç¨‹ç§°ä¸ºè´å¶æ–¯æ›´æ–°ã€‚è¿™å°±æ˜¯[**è´å¶æ–¯æ¨ç†**](https://en.wikipedia.org/wiki/Bayesian_statistics) çš„æœ¬è´¨æ‰€åœ¨ã€‚

ä½ å¯ä»¥åœ¨æˆ‘æœ€è¿‘çš„ä¸€ç¯‡æ–‡ç« ä¸­è¯»åˆ°æ›´å¤šå…³äºè´å¶æ–¯æ›´æ–°çš„å†…å®¹:

</bayesian-updating-simply-explained-c2ed3e563588>  

# å›å½’ç†è®º

## çº¿æ€§å›å½’

[**å›å½’**](https://en.wikipedia.org/wiki/Regression_analysis) æ—¨åœ¨ä¼°è®¡æŸä¸€ç‰¹æ€§çš„æ•ˆæœï¼Œ*ã€xã€‘ï¼Œ*:**

**![](img/94486684d981fcc0fd865ae83b2f1a93.png)**

**ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚**

**å…¶ä¸­ ***Î²_0*** ä¸ºæˆªè·ï¼Œ ***Î²_1*** ä¸ºæ ‡å®šç›®æ ‡ä¸ç‰¹å¾å…³ç³»çš„ç³»æ•°ï¼Œ ***Îµ*** ä¸ºè¯¯å·®é¡¹ï¼Œåœ¨çº¿æ€§å›å½’ä¸­éµå¾ª [**æ­£æ€åˆ†å¸ƒ**](https://en.wikipedia.org/wiki/Normal_distribution) :**

**![](img/ad04480ad683f198d5a2d42af348431a.png)**

**ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚**

**å…¶ä¸­ ***Ïƒ*** ä¸º [**æ ‡å‡†å·®**](https://en.wikipedia.org/wiki/Standard_deviation) ã€‚**

**çº¿æ€§å›å½’çš„ç›®çš„æ˜¯ç¡®å®šæè¿°ç‰¹å¾ã€ ***x*** å’Œç›®æ ‡ã€ ***y*** ä¹‹é—´å…³ç³»çš„å‚æ•° ***Î²_0ã€Î²_1*** å’Œ***ã€Ïƒã€‘***çš„æœ€ä½³å€¼ã€‚**

> **æ³¨æ„:æˆ‘ç›¸ä¿¡å¤§å¤šæ•°é˜…è¯»è¿™ç¯‡æ–‡ç« çš„äººéƒ½çŸ¥é“ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Œå¦‚æœä»–ä»¬ä¸çŸ¥é“çš„è¯ï¼Œé‚£ä¹ˆæœ‰å¾ˆå¤šèµ„æºå¯ä»¥æ¯”æˆ‘æ›´å¥½åœ°å‘ä½ è§£é‡Šå®ƒï¼**

## **é¢‘ç¹ä¸»ä¹‰è§‚ç‚¹**

**å¯»æ‰¾çº¿æ€§å›å½’æ¨¡å‹å‚æ•°çš„æœ€å¹¿ä¸ºäººçŸ¥çš„æ–¹æ³•æ¥è‡ªç»Ÿè®¡å­¦çš„é¢‘ç‡è§‚ç‚¹ã€‚**

**é¢‘ç‡ä¸»ä¹‰è€…çš„è§‚ç‚¹é‡‡ç”¨[**ã€OLSã€‘**](https://en.wikipedia.org/wiki/Ordinary_least_squares)çš„æ™®é€šæœ€å°äºŒä¹˜æ³•ï¼Œé€šè¿‡[**ã€RSSã€‘**](https://en.wikipedia.org/wiki/Residual_sum_of_squares)**çš„æ®‹å·®å¹³æ–¹å’Œæ¥ä¼°è®¡å‚æ•°:****

****![](img/0654bcabaa0c23710e00904bbac53403.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

****å…¶ä¸­ ***y*** æ˜¯å®é™…å€¼ï¼Œè€Œ ***Å·*** æ˜¯æ¥è‡ªæˆ‘ä»¬çš„æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œå…¶ä¸€èˆ¬å½¢å¼ä¸º:****

****![](img/1d5883213522a6084a27518bc4b84316.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

****å…¶ä¸­ ***X*** å’Œ ***Î²*** æ˜¯ç‰¹å¾å’Œå‚æ•°çš„æ•°ç»„ã€‚****

****è¿™ä¸ª OLS æ–¹ç¨‹çš„ä¸€èˆ¬è§£æ˜¯:****

****![](img/f0be5aaf2e7e8f2a2ff3d16287817497.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

> ****è¿™ä¸ªè§£å†³æ–¹æ¡ˆçš„å®Œæ•´æ¨å¯¼å¯ä»¥åœ¨[è¿™é‡Œ](https://en.wikipedia.org/wiki/Ordinary_least_squares#Simple_linear_regression_model)æ‰¾åˆ°ã€‚****

****Frequentist æ–¹æ³•çš„å…³é”®æ˜¯æˆ‘ä»¬ä¸ºæ¯ä¸ªå‚æ•°è·å¾—ä¸€ä¸ª*å•ä¸€å›ºå®šå€¼*ã€‚****

****æœ€ç»ˆçš„æ¨¡å‹åœ¨æŸç§ç¨‹åº¦ä¸Šå‡è®¾æˆ‘ä»¬æ­£åœ¨å»ºæ¨¡çš„æ•°æ®å¿…é¡»æ¥è‡ªè¿™äº›å›ºå®šçš„å‚æ•°ã€‚ç„¶è€Œï¼Œä¸å¯èƒ½è·å¾—æ‰€æœ‰æ•°æ®ï¼Œå› æ­¤å‡è®¾è¿™äº›*å•å€¼*å‚æ•° 100%æ­£ç¡®ä¼¼ä¹æ˜¯æ„šè ¢çš„ã€‚å¦ä¸€ç§è¡¨è¿°æ–¹å¼æ˜¯ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥æ¨å¯¼å‡ºå‚æ•°çš„æœ‰æ„ä¹‰çš„å•ä¸€å€¼ã€‚****

## ****è´å¶æ–¯è§‚ç‚¹****

****å¦ä¸€ç§è§‚ç‚¹è®¤ä¸ºï¼Œå‚æ•°å‘ˆç°å€¼çš„åˆ†å¸ƒï¼Œå…¶ä¸­ä¸€äº›æ¯”å¦ä¸€äº›æ›´æœ‰å¯èƒ½ã€‚å®ƒè€ƒè™‘äº†å¯èƒ½äº§ç”Ÿè§‚æµ‹æ•°æ®çš„å‡ ä¸ªä¼¼æ˜¯è€Œéçš„å‚æ•°ç»„åˆã€‚****

****æˆ‘ä»¬æœ‰ä¸€ä¸ªæˆ‘ä»¬è®¤ä¸ºå‚æ•°å¯èƒ½æ˜¯ä»€ä¹ˆçš„åˆå§‹è§†å›¾/èŒƒå›´ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥è®¤ä¸ºæˆªè·åŒæ ·å¯èƒ½æ˜¯ 0 åˆ° 10 ä¹‹é—´çš„ä»»ä½•æ•°å­—ã€‚è¿™æ˜¯æˆ‘ä»¬å‚æ•°çš„å…ˆéªŒã€‚****

****ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è§‚æµ‹æ•°æ®æ›´æ–°å…ˆéªŒï¼Œä½¿ç”¨è´å¶æ–¯å®šç†ä¸ºæ¯ä¸ªå‚æ•°åˆ›å»ºåéªŒåˆ†å¸ƒï¼Œå¦‚ä¸‹æ‰€ç¤º:****

****![](img/edb675464e8f27b7e1462b0e2f4710a7.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

****è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¦‚ä½•æ›´æ–°æˆªè·çš„ä¾‹å­ã€‚****

****ä¹Ÿå°±æ˜¯è¯´ç›®æ ‡ï¼Œ***ã€yã€‘***ï¼Œç°åœ¨æ˜¯æ•°æ®ä¸Šéšæœºåˆ†å¸ƒçš„å˜é‡ï¼Œ ***x*** ï¼Œå‚æ•° ***Î²_0ï¼Œ*** ***Î²_1ï¼ŒÏƒ:*******

****![](img/6867566502064d802592580597dbcce9.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

****å› æ­¤æ¯ä¸ªç›®æ ‡å˜é‡çš„å¯èƒ½æ€§æ˜¯æ­£æ€åˆ†å¸ƒçš„ [**ã€æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‘**](https://en.wikipedia.org/wiki/Probability_density_function) :****

****![](img/bd38d4e3487f614ff3e68cfff90276c2.png)****

****ä½œè€…åœ¨ LaTeX ä¸­ç”Ÿæˆçš„æ–¹ç¨‹ã€‚****

*******y_i*** çš„æ‰€æœ‰å•ä¸ªå¯èƒ½æ€§çš„ä¹˜ç§¯äº§ç”Ÿå½“å‰æ¨¡å‹åŠå…¶ç»™å®šå‚æ•°çš„æ€»å¯èƒ½æ€§ã€‚****

> ****è¿™ä¸ªä¼¼ç„¶å‡½æ•°å’Œè´å¶æ–¯å›å½’ä½œä¸ºä¸€ä¸ªæ•´ä½“çš„æ›´å¯†é›†å’Œå®Œæ•´çš„æ¨å¯¼å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°[ã€‚](https://statswithr.github.io/book/introduction-to-bayesian-regression.html)****

****è§‚å¯Ÿè´å¶æ–¯æ–¹æ³•çš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹å¼æ˜¯ï¼Œå½“æˆ‘ä»¬è·å¾—æ›´å¤šæ•°æ®æ—¶ï¼Œæˆ‘ä»¬æ›´æ–°å‚æ•°çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å˜å¾—æ›´åŠ ç¡®å®šå‚æ•°åº”è¯¥æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¾ˆå¯èƒ½æ˜¯ OLS å¸¸å®¢çš„ä¼°è®¡ï¼Œä½†è¿™ä¸æ˜¯ä¿è¯ã€‚****

****è¿™æ˜¯é¢‘ç‡ä¸»ä¹‰è€…å’Œè´å¶æ–¯ç»Ÿè®¡ä¹‹é—´çš„å·®å¼‚çš„å¿«é€Ÿè¿è¡Œã€‚å¦‚æœä½ æƒ³æ›´å¥½åœ°ç†è§£ï¼Œé‚£é‡Œæœ‰å¾ˆå¤šèµ„æºï¼Œä½†æ˜¯æˆ‘å–œæ¬¢è¿™ä¸ªåšå®¢å¸–å­ï¼****

# ****Python ä¸­çš„è´å¶æ–¯å›å½’****

****ç°åœ¨è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ä¸ºä¸€ä¸ªç®€å•çš„æ¨¡å‹å®ç°è´å¶æ–¯çº¿æ€§å›å½’ï¼Œå…¶ä¸­æˆ‘ä»¬æœ‰ä¸€ä¸ªç‰¹å¾ï¼****

## ****ç”Ÿæˆæ•°æ®****

****æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ sklearn çš„ *make_regression* å‡½æ•°åœ¨ Python ä¸­ç”Ÿæˆä¸€äº›æ•°æ®:****

```
****# Import packages
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets# Generate data
x, y = datasets.make_regression(n_samples=100,
                                n_features=1,
                                noise=10)# Plot data
fig, ax = plt.subplots(figsize=(9,5))
ax.scatter(x, y)
ax.ticklabel_format(style='plain')
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.show()****
```

****![](img/1089d072540555c5f5ea2d4cff4e7e61.png)****

****ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚****

## ****æ™®é€šæœ€å°äºŒä¹˜æ³•****

****æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ statsmodel è½¯ä»¶åŒ…ï¼Œé€šè¿‡ OLS æ–¹æ³•æ¥ä¼°è®¡é¢‘ç‡å›å½’çº¿:****

```
****# Packages
import statsmodels.formula.api as smf# Create a dataframe
data = pd.DataFrame(list(zip(x.flatten(), y)), columns =['x', 'y'])# Calculating the slope and intercept
formula = 'y ~ x'
results = smf.ols(formula, data=data).fit()# Get our equation of the OLS line
intercept = results.params['Intercept']
slope = results.params['x']
x_vals = np.arange(min(x), max(x), 0.1)
ols_line = slope*x_vals + intercept# Plot the OLS line
fig, ax = plt.subplots(figsize=(9,5))
ax.scatter(data['x'], data['y'])
ax.plot(x_vals, ols_line,label='OLS Fit', color='red')
ax.ticklabel_format(style='plain')
plt.xlabel('x',fontsize=18)
plt.ylabel('y',fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.legend(fontsize=16)
plt.show()****
```

****![](img/c493ad6311a66ecee09d6f240d62ede2.png)****

****ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚****

****ä¸ºäº†æ¸…æ¥šèµ·è§ï¼Œæ–œç‡æ˜¯ ***Î²_1ï¼Œ*** æˆªè·æ˜¯ ***Î²_0*** ï¼Œsigma æ˜¯*è¿™å°±æ˜¯æˆ‘ä»¬ä¸Šé¢åœ¨ç†è®ºéƒ¨åˆ†ç”¨æ¥æè¿°å›å½’çº¿çš„ã€‚*****

*****è¿™æ˜¯é¢‘ç‡ä¸»ä¹‰è€…çš„è§£é‡Šï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨å¯¹æ¯ä¸ªå‚æ•°åªæœ‰å•ä¸€çš„ä¼°è®¡ã€‚æˆ‘ä»¬ç°åœ¨å°†æ‰§è¡Œè´å¶æ–¯ç‰ˆæœ¬ã€‚*****

## *****ä¼ é“è€…*****

*****é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç»™æˆ‘ä»¬çš„å‚æ•°åˆ†é…ä¸€äº›å…ˆéªŒåˆ†å¸ƒã€‚ä½¿ç”¨ OLS ä¼°è®¡çš„ç»“æœï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ª[ç»Ÿä¸€çš„æ— ä¿¡æ¯å…ˆéªŒ](https://stats.stackexchange.com/questions/27813/what-is-the-point-of-non-informative-priors)ï¼Œå…¶èŒƒå›´ä¸º OLS ä¼°è®¡çš„ 20%:*****

```
*****def make_uninformative_prior(name,data):
    """Function to make priors."""
    df = pd.DataFrame({name:data})
    df['Probability'] = 1/len(data)
    return df# Create a range of values for the slope
data_slope = np.linspace(results.params['x']*0.8,
                         results.params['x']*1.2, num=60)prior_slope = make_uninformative_prior('slope',data_slope)
prior_slope.head()# Create a range of values for the intercept
data_inter = np.linspace(results.params['Intercept']*0.8,
                         results.params['Intercept']*1.2, num=60)prior_inter = make_uninformative_prior('intercept',data_inter)
prior_inter.head()# Create a range of values for the sigma
data_sigma = np.linspace(results.resid.std()*0.8,
                         results.resid.std()*1.2, num=60)prior_sigma = make_uninformative_prior('sigma',data_sigma)
prior_sigma.head()*****
```

*****å¦‚æœæˆ‘ä»¬å¯¹å®é™…å‚æ•°æœ‰æ‰€äº†è§£ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šä½¿ç”¨ä¸åŒçš„å…ˆéªŒï¼Œå¯¹å‚æ•°çš„æŸäº›å€¼è¿›è¡Œä¸åŒçš„åŠ æƒã€‚å‰ç§‘å®Œå…¨æ˜¯ä¸»è§‚æ­¦æ–­çš„ã€‚è¿™é€šå¸¸æ˜¯åå¯¹è´å¶æ–¯ç»Ÿè®¡çš„ä¸€ä¸ª[è®ºç‚¹](https://stats.stackexchange.com/questions/381825/objective-vs-subjective-bayesian-paradigms)ï¼Œå› ä¸ºå®ƒå¯¼è‡´éå®¢è§‚æ¦‚ç‡ã€‚*****

*****æˆ‘ä»¬ç°åœ¨è®¡ç®—ä¸‰ä¸ªå‚æ•°çš„è”åˆåˆ†å¸ƒã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªç‰¹å®šçš„å‚æ•°ç»„åˆè§£é‡Šè§‚å¯Ÿåˆ°çš„æ•°æ®çš„å¯èƒ½æ€§æœ‰å¤šå¤§:*****

```
*****# Counter for the row index
counter = 0# Dataframe to store the combinations in
df = pd.DataFrame(columns=['slope','intercept','sigma','prior'])# Iterate through the slope
for slope in prior_slope['slope']:
    prob_slope = \
    prior_slope['Prior'].loc[prior_slope['slope'] == slope]

    # Iterate through the intercept
    for intercept in prior_inter['intercept']:
        prob_inter = \
        prior_inter['Prior'].loc[prior_inter['intercept'] \
                                 == intercept]

        # Iterate through the error
        for sigma in prior_sigma['sigma']:
            prob_sigma = \
            prior_sigma['Prior'].loc[prior_sigma['sigma'] == sigma]

            # Calculate the prior of this specific combination
            prob = \
            float(prob_slope)*float(prob_inter)*float(prob_sigma)

            # Insert the row of data
            df.loc[counter] = \
            [slope] + [intercept] + [sigma] + [prob]

            # Update row index
            counter += 1*****
```

> *****å¦‚æœè¿™ç§æ ¼å¼åœ¨ä½ çš„å±å¹•ä¸Šçœ‹èµ·æ¥ä¸èˆ’æœï¼Œæˆ‘å¾ˆæŠ±æ­‰ã€‚å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œæˆ‘å¼ºçƒˆæ¨èä½ åœ¨æˆ‘çš„ [GitHub repo è¿™é‡Œ](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/Bayesian_Regression.ipynb)æŸ¥çœ‹ä»£ç ï¼Œè®©å®ƒæ›´å®¹æ˜“è§£é‡Šå’Œç†è§£ï¼*****

*****æˆ‘å®Œå…¨çŸ¥é“ for å¾ªç¯ä¸æ˜¯æœ€ä¼˜çš„ï¼Œä½¿ç”¨ pandas å’Œ numpy çš„çŸ¢é‡åŒ–å®ç°ä¼šæ›´å¿«ã€‚ç„¶è€Œï¼Œæˆ‘è®¤ä¸ºä½¿ç”¨å¾ªç¯å¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼*****

*****å› ä¸ºæˆ‘ä»¬å¯¹æ¯ä¸ªå‚æ•°éƒ½æœ‰æœªçŸ¥çš„å…ˆéªŒï¼Œæ‰€ä»¥æ¯ä¸ªç»„åˆéƒ½æœ‰å®Œå…¨ç›¸åŒçš„å…ˆéªŒæ¦‚ç‡:*****

*****![](img/129fd554575eee1b6617150bbbe76837.png)*****

*****å›¾ç‰‡ä½œè€…ã€‚*****

## *****å¯èƒ½æ€§*****

*****å°±åƒæˆ‘ä»¬ä¹‹å‰è¯´çš„ï¼ŒåéªŒä¸å…ˆéªŒå’Œä¼¼ç„¶çš„ä¹˜ç§¯æˆæ­£æ¯”ã€‚å› æ­¤ï¼Œè¦è·å¾—æ¯ä¸ªå‚æ•°çš„åéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®æˆ‘ä»¬çš„è§‚æµ‹æ•°æ®è®¡ç®—å®ƒä»¬çš„ä¼¼ç„¶æ€§ã€‚*****

*****ä¸ºäº†è®¡ç®—å¯èƒ½æ€§ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå¯èƒ½çš„ç»„åˆ(æ¯ä¸€è¡Œ)å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æˆ‘å‰é¢æåˆ°çš„å…¬å¼è®¡ç®—æ®‹å·®æ¥æ‰¾åˆ°å¯èƒ½æ€§:*****

```
*****counter = 0
df['likelihood'] = df['prior']# Loop over the combination of values
for slope in prior_slope['slope']:
    for intercept in prior_inter['intercept']:        
        for sigma in prior_sigma['sigma']:

            # Compute the predictions from this line
            predictions = slope * data['x'] + intercept

            # Compute residual/errors of this line
            residual = data['y'] - predictions

            # Compute the likelihood function that we saw above
            likelihoods = norm(0, sigma).pdf(residual)

            # Compute the total likelihood
            df['likelihood'].loc[counter] = likelihoods.prod()
            counter += 1*****
```

*****è¿™éƒ¨åˆ†å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´æ¥è¿è¡Œï¼Œè¿™æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­ä½¿ç”¨è´å¶æ–¯æ–¹æ³•çš„é—®é¢˜ä¹‹ä¸€ã€‚è¦çŸ¥é“ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼ŒæŸ¥çœ‹ä¸€ä¸‹æˆ‘ä¹‹å‰å…³äºè´å¶æ–¯å…±è½­å…ˆéªŒçš„æ–‡ç« ï¼Œè¿™ç¯‡æ–‡ç« å‘Šè¯‰ä½ å®æ–½è´å¶æ–¯å®šç†çš„ä¸€äº›ç¼ºç‚¹:*****

*****</bayesian-conjugate-priors-simply-explained-747218be0f70>  

æˆ‘ä»¬çš„æ•°æ®æ¡†æ¶ç°åœ¨çœ‹èµ·æ¥åƒè¿™æ ·:

![](img/179d124727382062eff408b558669195.png)

å›¾ç‰‡ä½œè€…ã€‚

## è´å¶æ–¯æ›´æ–°

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¦‚ä¸‹æ‰§è¡Œæˆ‘ä»¬çš„è´å¶æ–¯æ›´æ–°:

```
df['posterior'] = df['prior'] * df['likelihood']
df['posterior'] = df['posterior']/df['posterior'].sum()
```

æˆ‘ä»¬å¾—åˆ°çš„æ•°æ®æ¡†æ¶æ˜¯:

![](img/850d16c9bf5bd07bf8ccbb48bd006a04.png)

å›¾ç‰‡ä½œè€…ã€‚

## è¾¹é™…åˆ†å¸ƒ

ä¸ºäº†è¾“å‡ºæ¯ä¸ªå‚æ•°çš„è¾¹é™…åéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶ä»–ä¸¤ä¸ªå‚æ•°çš„åéªŒè¿›è¡Œæ±‚å’Œã€‚ä¾‹å¦‚ï¼Œä¸ºäº†æ‰¾åˆ°æ–œç‡çš„è¾¹é™…åéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬æ€»ç»“å‡ºï¼Œå¯¹äºæ–œç‡çš„æ¯ä¸ªå€¼ ***ã€*ã€**çš„åéªŒè¶…è¿‡Ïƒå’Œæˆªè·(åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç§¯åˆ†):

```
slope_df = df.groupby('slope').sum()
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶æ–œç‡çš„åéªŒåˆ†å¸ƒ:

```
# Plot the posterior distribution of the slope
plt.figure(figsize=(8,5))
plt.plot(slope_df.index, slope_df.posterior, linewidth=3)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.xlabel('Slope Value', fontsize=18)
plt.ylabel('PDF', fontsize=18)
plt.axvline(results.params['x'], color='red', ls='--', label='OLS Estimate')
plt.legend(fontsize=16)
plt.show()
```

![](img/c7d36ba94f4d11638bda0da65ef5413f.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

> æ–œç‡ç°åœ¨æ˜¯ä¸€ä¸ªåˆ†å¸ƒï¼

æˆ‘ä»¬å¯ä»¥å¯¹æˆªè·å’Œè¯¯å·®é¡¹è¿›è¡Œç±»ä¼¼çš„è®¡ç®—:

```
intercept_df = df.groupby('intercept').sum()
sigma_df = df.groupby('sigma').sum()# Plot the posterior distribution of the Intercept
plt.figure(figsize=(8,5))
plt.plot(intercept_df.index, intercept_df.posterior, linewidth=3)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.xlabel('Intercept Value', fontsize=18)
plt.ylabel('PDF', fontsize=18)
plt.axvline(results.params['Intercept'], color='red', ls='--', label='OLS Estimate')
plt.legend(fontsize=16)
plt.show()# Plot the posterior distribution of sigma
plt.figure(figsize=(8,5))
plt.plot(sigma_df.index, sigma_df.posterior, linewidth=3)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.xlabel('Sigma Value', fontsize=18)
plt.ylabel('PDF', fontsize=18)
plt.axvline(results.resid.std(), color='red', ls='--', label='OLS Estimate')
plt.legend(fontsize=16)
plt.show()
```

![](img/4a4fa9853684ee0e4ed86063ecea385b.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

![](img/0ec34da865fb78f873b09b37937ff7a2.png)

ä½œè€…ç”¨ Python ç”Ÿæˆçš„å›¾ã€‚

OLS ä¼°è®¡å€¼æ˜¯è¿™äº›å‚æ•°æœ€å¯èƒ½çš„å€¼ï¼Œä½†ä¸æ˜¯å”¯ä¸€çš„å€¼ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ 216ï¼Œ000 ç§å…¶ä»–æ½œåœ¨ç»„åˆæ¥å»ºç«‹æ•°æ®æ¨¡å‹ï¼

## æ‘˜è¦

è¿™ä¸ªè®¡ç®—å¹¶ä¸æ˜¯é‚£ä¹ˆç›´æ¥ï¼Œå†™è¿™ç¯‡åšå®¢æœ€ç»ˆè®©æˆ‘ç†è§£äº†æ•´ä¸ªè´å¶æ–¯å›å½’è¿‡ç¨‹ã€‚æˆ‘å»ºè®®è¯»è€…åœ¨æˆ‘çš„ GitHub ä¸ŠæŸ¥çœ‹å®Œæ•´çš„ç¬”è®°æœ¬ï¼Œå¹¶åœ¨ä¸Šé¢ç©ä¸€ç©:

<https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/Bayesian_Regression.ipynb>  

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†è´å¶æ–¯å®šç†ï¼Œè§£é‡Šäº†é¢‘æ•°ç»Ÿè®¡å’Œè´å¶æ–¯ç»Ÿè®¡ä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œæœ€åä»å¤´å¼€å§‹è¿›è¡Œè´å¶æ–¯çº¿æ€§å›å½’ã€‚

æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œè¿™ä¸ªä¸»é¢˜åœ¨æ‚¨éœ€è¦å®Œå…¨æŒæ¡çš„å…ˆå†³çŸ¥è¯†æ–¹é¢ç›¸å½“å¹¿æ³›ã€‚å¦‚æœæˆ‘è¯•ç€å»é€‚åº”æ‰€æœ‰çš„èƒŒæ™¯è¯é¢˜ï¼Œè¿™ä¸ªåšå®¢çœŸçš„ä¼šæˆä¸ºæ•™ç§‘ä¹¦ï¼

å› æ­¤ï¼Œæˆ‘å»ºè®®é‚£äº›è¿˜æ²¡æœ‰å®Œå…¨ç†è§£æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„äº‹æƒ…çš„äººå¤ä¹ ä¸€ä¸‹è´å¶æ–¯ç»Ÿè®¡å’Œçº¿æ€§å›å½’ã€‚æœ‰è¿™ä¹ˆå¤šå…³äºè¿™äº›ä¸»é¢˜çš„èµ„æºï¼Œä¼šæ¯”æˆ‘æ•™å¾—æ›´å¥½ï¼

# å’Œæˆ‘è”ç³»ï¼

*   [*è¦æƒ³åœ¨åª’ä½“ä¸Šé˜…è¯»æ— é™çš„æ•…äº‹ï¼Œè¯·åŠ¡å¿…åœ¨è¿™é‡Œæ³¨å†Œï¼*](/@egorhowell/membership) ğŸ’œ
*   [*åœ¨æˆ‘å‘å¸ƒæ³¨å†Œé‚®ä»¶é€šçŸ¥æ—¶è·å¾—æ›´æ–°ï¼*T13*ğŸ˜€*](/subscribe/@egorhowell)
*   [*é¢†è‹±*](https://www.linkedin.com/in/egor-howell-092a721b3/) ğŸ‘”
*   [*ç¢ç¢å¿µ*](https://twitter.com/EgorHowell) ğŸ–Š
*   [*github*](https://github.com/egorhowell)*ğŸ–¥*
*   *<https://www.kaggle.com/egorphysics>**ğŸ…***

> ***(æ‰€æœ‰è¡¨æƒ…ç¬¦å·éƒ½æ˜¯ç”± [OpenMoji](https://openmoji.org/) è®¾è®¡çš„â€”â€”å¼€æºçš„è¡¨æƒ…ç¬¦å·å’Œå›¾æ ‡é¡¹ç›®ã€‚æ‰§ç…§: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/#)************
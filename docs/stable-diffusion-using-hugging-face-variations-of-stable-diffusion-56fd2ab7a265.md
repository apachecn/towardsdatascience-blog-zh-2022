# åˆ©ç”¨æ‹¥æŠ±é¢çš„ç¨³å®šæ‰©æ•£â€”â€”ç¨³å®šæ‰©æ•£çš„å˜åŒ–

> åŸæ–‡ï¼š<https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265>

## ä½¿ç”¨[æ‹¥æŠ±é¢éƒ¨æ‰©æ•£å™¨åº“](https://github.com/huggingface/diffusers)çš„è´Ÿé¢æç¤ºå’Œå›¾åƒåˆ°å›¾åƒç¨³å®šæ‰©æ•£ç®¡é“çš„ä»‹ç»

è¿™æ˜¯æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« çš„ç»­ç¯‡â€”â€”[ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£|ä½œè€…:Aayush agr awal | 2022 å¹´ 11 æœˆ|è¿ˆå‘æ•°æ®ç§‘å­¦(medium.com)](https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8)ã€‚

åœ¨å‰ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å›é¡¾äº†ç¨³å®šæ‰©æ•£çš„æ‰€æœ‰å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä»¥åŠå¦‚ä½•è®©`prompt to image`ç®¡é“å·¥ä½œã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ç¼–è¾‘`prompt to image`å‡½æ•°æ¥ä¸ºæˆ‘ä»¬çš„ç¨³å®šæ‰©æ•£ç®¡é“æ·»åŠ é¢å¤–çš„åŠŸèƒ½ï¼Œå³`Negative prompting`å’Œ`Image to Image`ç®¡é“ã€‚å¸Œæœ›è¿™å°†æä¾›è¶³å¤Ÿçš„åŠ¨åŠ›æ¥ç©è¿™ä¸ªå‡½æ•°å¹¶è¿›è¡Œæ‚¨çš„ç ”ç©¶ã€‚

![](img/26c1ceb006b5d462fec26fa14dd52628.png)

å›¾ 1:ä½¿ç”¨ prompt -
â€œåœ¨ä¸¤ä¸ªä¸åŒæ–¹å‘åˆ†å‰çš„é“è·¯â€çš„ç¨³å®šæ‰©æ•£ç”Ÿæˆçš„å›¾åƒ

# 1.å˜ä½“ 1:å¦å®šæç¤º

## 1.1 ä»€ä¹ˆæ˜¯è´Ÿé¢æç¤ºï¼Ÿ

å¦å®šæç¤ºæ˜¯æˆ‘ä»¬å¯ä»¥æ·»åŠ åˆ°æ¨¡å‹ä¸­çš„é™„åŠ åŠŸèƒ½ï¼Œç”¨æ¥å‘Šè¯‰ç¨³å®šæ‰©æ•£æ¨¡å‹æˆ‘ä»¬ä¸å¸Œæœ›åœ¨ç”Ÿæˆçš„å›¾åƒä¸­çœ‹åˆ°ä»€ä¹ˆã€‚è¿™ä¸ªç‰¹æ€§å¾ˆå—æ¬¢è¿ï¼Œå¯ä»¥ä»åŸå§‹ç”Ÿæˆçš„å›¾åƒä¸­åˆ é™¤ç”¨æˆ·ä¸æƒ³çœ‹åˆ°çš„ä»»ä½•å†…å®¹ã€‚

![](img/a5f0cf0d035bcbb82819e9bf25b20e24.png)

å›¾ 2:å¦å®šæç¤ºç¤ºä¾‹

## 1.2 é€šè¿‡ä»£ç ç†è§£è´Ÿé¢æç¤º

è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€éœ€çš„åº“å’ŒåŠ©æ‰‹å‡½æ•°å¼€å§‹ã€‚æ‰€æœ‰è¿™äº›éƒ½å·²ç»åœ¨ä¹‹å‰çš„[å¸–å­](https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8)ä¸­ä½¿ç”¨å’Œè§£é‡Šè¿‡äº†ã€‚

```
import torch, logging

## disable warnings
logging.disable(logging.WARNING)  

## Imaging  library
from PIL import Image
from torchvision import transforms as tfms

## Basic libraries
from fastdownload import FastDownload
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
%matplotlib inline
from IPython.display import display
import shutil
import os

## For video display
from IPython.display import HTML
from base64 import b64encode

## Import the CLIP artifacts 
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler

## Initiating tokenizer and encoder.
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")

## Initiating the VAE
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")

## Initializing a scheduler and Setting number of sampling steps
scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)
scheduler.set_timesteps(50)

## Initializing the U-Net model
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")

## Helper functions
def load_image(p):
    '''
    Function to load images from a defined path
    '''
    return Image.open(p).convert('RGB').resize((512,512))

def pil_to_latents(image):
    '''
    Function to convert image to latents
    '''
    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0
    init_image = init_image.to(device="cuda", dtype=torch.float16) 
    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215
    return init_latent_dist

def latents_to_pil(latents):
    '''
    Function to convert latents to images
    '''
    latents = (1 / 0.18215) * latents
    with torch.no_grad():
        image = vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
    images = (image * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]
    return pil_images

def text_enc(prompts, maxlen=None):
    '''
    A function to take a texual promt and convert it into embeddings
    '''
    if maxlen is None: maxlen = tokenizer.model_max_length
    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") 
    return text_encoder(inp.input_ids.to("cuda"))[0].half()
```

ç°åœ¨æˆ‘ä»¬è¦é€šè¿‡ä¼ é€’ä¸€ä¸ªé¢å¤–çš„å‡½æ•°`neg_prompts`æ¥æ”¹å˜`prompt_2_img`å‡½æ•°ã€‚å¦å®šæç¤ºçš„å·¥ä½œæ–¹å¼æ˜¯åœ¨é‡‡æ ·æ—¶ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ–‡æœ¬ä»£æ›¿ç©ºå­—ç¬¦ä¸²è¿›è¡Œæ— æ¡ä»¶åµŒå…¥(`uncond`)ã€‚

![](img/8caf58ac1ca0908894f9afc5a3052e98.png)

å›¾ 3:è´Ÿæç¤ºä»£ç å˜åŒ–

æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬åšè¿™ä¸ªæ”¹å˜å¹¶æ›´æ–°æˆ‘ä»¬çš„`prompt_2_img`å‡½æ•°ã€‚

```
def prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):
    """
    Diffusion process to convert prompt to image
    """

    # Defining batch size
    bs = len(prompts) 

    # Converting textual prompts to embedding
    text = text_enc(prompts) 

    # Adding negative prompt condition
    if not neg_prompts: uncond =  text_enc([""] * bs, text.shape[1])
    # Adding an unconditional prompt , helps in the generation process
    else: uncond =  text_enc(neg_prompts, text.shape[1])
    emb = torch.cat([uncond, text])

    # Setting the seed
    if seed: torch.manual_seed(seed)

    # Initiating random noise
    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))

    # Setting number of steps in scheduler
    scheduler.set_timesteps(steps)

    # Adding noise to the latents 
    latents = latents.to("cuda").half() * scheduler.init_noise_sigma

    # Iterating through defined steps
    for i,ts in enumerate(tqdm(scheduler.timesteps)):
        # We need to scale the i/p latents to match the variance
        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)

        # Predicting noise residual using U-Net
        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)

        # Performing Guidance
        pred = u + g*(t-u)

        # Conditioning  the latents
        latents = scheduler.step(pred, ts, latents).prev_sample

        # Saving intermediate images
        if save_int: 
            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')
            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')

    # Returning the latent representation to output an image of 3x512x512
    return latents_to_pil(latents)
```

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå‡½æ•°æ˜¯å¦å¦‚é¢„æœŸçš„é‚£æ ·å·¥ä½œã€‚

```
## Image without neg prompt
images = [None, None]
images[0] = prompt_2_img(prompts = ["A dog wearing a white hat"], neg_prompts=[""],steps=50, save_int=False)[0]
images[1] = prompt_2_img(prompts = ["A dog wearing a white hat"], neg_prompts=["White hat"],steps=50, save_int=False)[0]

## Plotting side by side
fig, axs = plt.subplots(1, 2, figsize=(12, 6))
for c, img in enumerate(images): 
    axs[c].imshow(img)
    if c == 0 : axs[c].set_title(f"A dog wearing a white hat")
    else: axs[c].set_title(f"Neg prompt - white hat")
```

![](img/e73a2278b4876ba5048031be3769a31f.png)

å›¾ 4:è´Ÿé¢æç¤ºçš„å¯è§†åŒ–ã€‚å·¦ä¾§ SD ç”Ÿæˆæç¤ºâ€œæˆ´ç™½å¸½å­çš„ç‹—â€,å³ä¾§ç›¸åŒæ ‡é¢˜ç”Ÿæˆå¦å®šæç¤ºâ€œç™½å¸½å­â€

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åŠŸèƒ½ï¼Œå¯ä»¥æ ¹æ®æ‚¨çš„å–œå¥½å¾®è°ƒå›¾åƒã€‚ä½ ä¹Ÿå¯ä»¥ç”¨å®ƒæ¥ç”Ÿæˆä¸€å¼ éå¸¸é€¼çœŸçš„è„¸ï¼Œå°±åƒè¿™ä¸ª [Reddit å¸–å­](https://www.reddit.com/r/StableDiffusion/comments/yqnh2c/closeup_photo_of_a_face_just_txt2img_and_lsdr/)ä¸€æ ·ã€‚è®©æˆ‘ä»¬è¯•è¯•-

```
prompt = ['Close-up photography of the face of a 30 years old man with brown eyes, (by Alyssa Monks:1.1), by Joseph Lorusso, by Lilia Alvarado, beautiful lighting, sharp focus, 8k, high res, (pores:0.1), (sweaty:0.8), Masterpiece, Nikon Z9, Award - winning photograph']
neg_prompt = ['lowres, signs, memes, labels, text, food, text, error, mutant, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, made by children, caricature, ugly, boring, sketch, lacklustre, repetitive, cropped, (long neck), facebook, youtube, body horror, out of frame, mutilated, tiled, frame, border, porcelain skin, doll like, doll']
images = prompt_2_img(prompts = prompt, neg_prompts=neg_prompt, steps=50, save_int=False)
images[0]
```

![](img/7088aff5ea5ea90d218f2242ec3733c8.png)

å›¾ 5:ä½¿ç”¨è´Ÿé¢æç¤ºç”Ÿæˆçš„å›¾åƒã€‚

ç›¸å½“æ•´æ´ï¼æˆ‘å¸Œæœ›è¿™èƒ½ç»™ä½ ä¸€äº›æƒ³æ³•ï¼Œå…³äºå¦‚ä½•å¼€å§‹ä½ è‡ªå·±çš„ç¨³å®šæ‰©æ•£çš„å˜åŒ–ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ç¨³å®šæ‰©æ•£çš„å¦ä¸€ç§å˜åŒ–ã€‚

# 2.å˜ä½“ 2:å›¾åƒåˆ°å›¾åƒç®¡é“

## 2.1 ä»€ä¹ˆæ˜¯å›¾åƒåˆ°å›¾åƒç®¡é“ï¼Ÿ

å¦‚ä¸Šæ‰€è¿°ï¼Œ`prompt_2_img`å‡½æ•°å¼€å§‹ä»éšæœºé«˜æ–¯å™ªå£°ä¸­ç”Ÿæˆå›¾åƒï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªåˆå§‹ç§å­å›¾åƒæ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿè¿™æ­£æ˜¯å›¾åƒåˆ°å›¾åƒç®¡é“çš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆå§‹ç§å­å›¾åƒå°†å®ƒä¸ä¸€äº›å™ªå£°æ··åˆ(è¿™å¯ä»¥ç”±ä¸€ä¸ª`strength`å‚æ•°æ¥å¼•å¯¼)ï¼Œç„¶åè¿è¡Œæ‰©æ•£å¾ªç¯ï¼Œè€Œä¸æ˜¯çº¯ç²¹ä¾èµ–äºè¾“å‡ºå›¾åƒçš„æ–‡æœ¬è°ƒèŠ‚ã€‚

![](img/09b08c3cc5482fb3c937cf121e064efe.png)

å›¾ 6:å›¾åƒåˆ°å›¾åƒç®¡é“ç¤ºä¾‹ã€‚

# 2.2 é€šè¿‡ä»£ç ç†è§£å›¾åƒåˆ°å›¾åƒçš„æç¤º

ç°åœ¨æˆ‘ä»¬è¦æ”¹å˜ä¸Šé¢å®šä¹‰çš„`prompt_2_img`å‡½æ•°ã€‚æˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬çš„`prompt_2_img_i2i`å‡½æ•°-
1 å¼•å…¥å¦å¤–ä¸¤ä¸ªå‚æ•°ã€‚`init_img`:å®ƒå°†æ˜¯åŒ…å«ç§å­å›¾åƒ
2 çš„`Image`å¯¹è±¡ã€‚`strength`:è¯¥å‚æ•°å– 0 åˆ° 1 ä¹‹é—´çš„å€¼ã€‚å€¼è¶Šé«˜ï¼Œæœ€ç»ˆå›¾åƒçœ‹èµ·æ¥å°±è¶Šä¸åƒç§å­å›¾åƒã€‚

```
def prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512, save_int=False):
    """
    Diffusion process to convert prompt to image
    """
    # Converting textual prompts to embedding
    text = text_enc(prompt) 

    # Adding negative prompt condition
    if not neg_prompts: uncond =  text_enc([""] * bs, text.shape[1])
    # Adding an unconditional prompt , helps in the generation process
    else: uncond =  text_enc(neg_prompts, text.shape[1])
    emb = torch.cat([uncond, text])

    # Setting the seed
    if seed: torch.manual_seed(seed)

    # Setting number of steps in scheduler
    scheduler.set_timesteps(steps)

    # Convert the seed image to latent
    init_latents = pil_to_latents(init_img)

    # Figuring initial time step based on strength
    init_timestep = int(steps * strength) 
    timesteps = scheduler.timesteps[-init_timestep]
    timesteps = torch.tensor([timesteps], device="cuda")

    # Adding noise to the latents 
    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)
    init_latents = scheduler.add_noise(init_latents, noise, timesteps)
    latents = init_latents

    # Computing the timestep to start the diffusion loop
    t_start = max(steps - init_timestep, 0)
    timesteps = scheduler.timesteps[t_start:].to("cuda")

    # Iterating through defined steps
    for i,ts in enumerate(tqdm(timesteps)):
        # We need to scale the i/p latents to match the variance
        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)

        # Predicting noise residual using U-Net
        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)

        # Performing Guidance
        pred = u + g*(t-u)

        # Conditioning  the latents
        latents = scheduler.step(pred, ts, latents).prev_sample

        # Saving intermediate images
        if save_int: 
            if not os.path.exists(f'./steps'):
                os.mkdir(f'./steps')
            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')

    # Returning the latent representation to output an image of 3x512x512
    return latents_to_pil(latents)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨éšæœºå™ªå£°ï¼Œè€Œæ˜¯ä½¿ç”¨`strength`å‚æ•°æ¥è®¡ç®—æ·»åŠ å¤šå°‘å™ªå£°ä»¥åŠè¿è¡Œæ‰©æ•£å¾ªç¯çš„æ­¥éª¤æ•°ã€‚é€šè¿‡å°†å¼ºåº¦(é»˜è®¤å€¼= 0.8)ä¹˜ä»¥ç¬¬ 10(50-50 * 0.8)æ­¥çš„æ­¥æ•°(é»˜è®¤å€¼= 50)å¹¶è¿è¡Œå‰©ä½™ 40(50*0.8)æ­¥çš„æ‰©æ•£å¾ªç¯æ¥è®¡ç®—å™ªæ³¢é‡ã€‚è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªåˆå§‹å›¾åƒï¼Œå¹¶é€šè¿‡`prompt_2_img_i2i`å‡½æ•°ä¼ é€’å®ƒã€‚

```
p = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')
image = Image.open(p).convert('RGB').resize((512,512))
prompt = ["Wolf howling at the moon, photorealistic 4K"]
images = prompt_2_img_i2i(prompts = prompt, init_img = image)

## Plotting side by side
fig, axs = plt.subplots(1, 2, figsize=(12, 6))
for c, img in enumerate([image, images[0]]): 
    axs[c].imshow(img)
    if c == 0 : axs[c].set_title(f"Initial image")
    else: axs[c].set_title(f"Image 2 Image output")
```

![](img/41bf4b29ac8c6fff3986b6a8719ad84b.png)

å›¾ 7:å›¾åƒåˆ°å›¾åƒç®¡é“çš„å¯è§†åŒ–ã€‚å·¦è¾¹æ˜¯ img2img ç®¡é“ä¸­ä¼ é€’çš„åˆå§‹å›¾åƒï¼Œå³è¾¹æ˜¯ img2img ç®¡é“çš„è¾“å‡ºã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„`prompt_2_img_i2i`å‡½æ•°ä»æä¾›çš„åˆå§‹è‰å›¾ä¸­åˆ›å»ºäº†ä¸€ä¸ªæ¼‚äº®çš„å²è¯—å›¾åƒã€‚

# 3 ç»“è®º

æˆ‘å¸Œæœ›è¿™èƒ½å¾ˆå¥½åœ°æ¦‚è¿°å¦‚ä½•è°ƒæ•´`prompt_2_img`å‡½æ•°ï¼Œä¸ºä½ çš„ç¨³å®šæ‰©æ•£å¾ªç¯å¢åŠ é¢å¤–çš„èƒ½åŠ›ã€‚å¯¹è¿™ä¸ªä½çº§å‡½æ•°çš„ç†è§£å¯¹äºå°è¯•ä½ è‡ªå·±çš„æƒ³æ³•æ¥æ”¹å–„ç¨³å®šæ‰©æ•£æˆ–å®ç°æˆ‘å¯èƒ½åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æ¶‰åŠçš„æ–°è®ºæ–‡æ˜¯æœ‰ç”¨çš„ã€‚

æˆ‘å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»å®ƒï¼Œå¹¶éšæ—¶ä½¿ç”¨æˆ‘çš„ä»£ç ï¼Œå¹¶å°è¯•ç”Ÿæˆæ‚¨çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå¦‚æœå¯¹ä»£ç æˆ–åšå®¢å¸–å­æœ‰ä»»ä½•åé¦ˆï¼Œè¯·éšæ—¶è”ç³» aayushmnit@gmail.com çš„ LinkedIn æˆ–å‘ç”µå­é‚®ä»¶ç»™æˆ‘ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ç½‘ç«™ä¸Šé˜…è¯»åšå®¢çš„æ—©æœŸå‘å¸ƒ[Aayush agr awal-åšå®¢(aayushmnit.com)](https://aayushmnit.com/blog.html)ã€‚

# 4 å‚è€ƒæ–‡çŒ®

*   [Fast.ai è¯¾ç¨‹â€”â€”ã€Šä»æ·±åº¦å­¦ä¹ åŸºç¡€åˆ°ç¨³å®šæ‰©æ•£ã€‹å‰ä¸¤èŠ‚](https://www.fast.ai/posts/part2-2022-preview.html)
*   [ğŸ§¨æ‰©æ•£å™¨çš„ç¨³å®šæ‰©æ•£](https://huggingface.co/blog/stable_diffusion)
*   [è¿›å…¥ç¨³å®šæ‰©æ•£çš„ä¸–ç•Œ](https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/)
# æœºå™¨å­¦ä¹  102:é€»è¾‘å›å½’

> åŸæ–‡ï¼š<https://towardsdatascience.com/machine-learning-102-logistic-regression-9e6dc2807772>

## å›åˆ°åˆ†ç±»çš„åŸºç¡€

åœ¨ä¹‹å‰çš„ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ¢ç´¢äº†çº¿æ€§å›å½’â€”â€”æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦ä¸­ä½¿ç”¨çš„æ‰€æœ‰å…¶ä»–é«˜çº§æ¨¡å‹çš„åŸºç¡€ã€‚çº¿æ€§å›å½’æ¨¡å‹è¿ç»­å› å˜é‡ï¼Œå¦‚è‚¡ç¥¨ä»·æ ¼ã€‚

ç„¶è€Œï¼Œåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¹Ÿå¿…é¡»å¤„ç†å¸ƒå°”æˆ–åˆ†ç±»ç›¸å…³å˜é‡ã€‚å…¸å‹çš„å¸ƒå°”å› å˜é‡åŒ…æ‹¬åƒå€ºåŠ¡äººçš„è´·æ¬¾è¿çº¦çŠ¶æ€â€”â€”å®Œå…¨å¿è¿˜/è¿çº¦ï¼Œæˆ–åˆ†ç±»çŒ«çš„å›¾ç‰‡â€”â€”æ˜¯çŒ«/ä¸æ˜¯çŒ«ğŸˆï¼

ç”¨äºæ¨¡æ‹Ÿå¸ƒå°”å› å˜é‡çš„æœ€åŸºæœ¬æ¨¡å‹ä¹‹ä¸€æ˜¯é€»è¾‘å›å½’æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†æ·±å…¥æ¢è®¨ä»¥ä¸‹ç»†èŠ‚:

*   é€»è¾‘å›å½’æ¨¡å‹èƒŒåçš„åŸºæœ¬æ•°å­¦ã€‚
*   å¸ƒå°”å› å˜é‡çš„æŸå¤±å‡½æ•°ã€‚
*   éšæœºæ¢¯åº¦ä¸‹é™æ³•ã€‚
*   å¦‚ä½•ä½¿ç”¨ Python åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹ğŸã€‚

å¦‚æœæ‚¨åˆšåˆšå¼€å§‹å­¦ä¹ æ•°æ®ç§‘å­¦ï¼Œæˆ–è€…å°½ç®¡æ‚¨å·²ç»æœ‰äº†å¾ˆé«˜çš„èŒä¸šç”Ÿæ¶¯ï¼Œä½†åªæ˜¯æƒ³ä¿®æ”¹ä¸€äº›åŸºæœ¬æ¦‚å¿µï¼Œè¯·ç»§ç»­é˜…è¯»ï¼Œç›´åˆ°æœ¬æ–‡ç»“æŸï¼

![](img/8a6a446282308865a19217cd887b7f48.png)

[è‹±å›½æ–¯å‡¯å²›](https://unsplash.com/s/photos/skye%2C-united-kingdom)ã€‚ç”±[ç½—ä¼¯ç‰¹Â·å¢å…‹æ›¼](https://unsplash.com/@robertlukeman?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹æ‘„çš„ç…§ç‰‡ã€‚

# å¸ƒå°”å› å˜é‡ã€æ¦‚ç‡å’Œèµ”ç‡

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢é€»è¾‘å›å½’èƒŒåçš„æ•°å­¦ï¼Œä»æœºå™¨å­¦ä¹ ä¸­æœ€åŸºæœ¬çš„æ¨¡å‹å¼€å§‹â€” [çº¿æ€§å›å½’](/machine-learning-101-linear-regression-72ba6c04fbf1)ã€‚

åœ¨çº¿æ€§å›å½’ä¸­ï¼Œå› å˜é‡ *d* è¿ç»­æ— ç•Œï¼Œä¸è§£é‡Šå˜é‡*m*g*â‚ã€ *g* â‚‚ã€â€¦ *gâ‚˜* æˆçº¿æ€§å…³ç³»:*

*d*=*c*â‚*g*â‚+*c*â‚‚*g*â‚‚+â€¦*+câ‚˜gâ‚˜ï¼Œ*

å…¶ä¸­ *c* â‚ã€ *c* â‚‚ã€â€¦ *câ‚˜* ä¸ºä¸è§£é‡Šå˜é‡ç›¸å…³è”çš„ *m* æ¨¡å‹å‚æ•°ã€‚

ç„¶è€Œï¼Œåœ¨äºŒè¿›åˆ¶åˆ†ç±»é—®é¢˜ä¸­ï¼Œå› å˜é‡ *d* æ˜¯å¸ƒå°”å‹çš„: *d* åªæœ‰ 0 æˆ– 1 çš„å€¼ã€‚ç”±äºä¸Šè¿°çº¿æ€§å›å½’æ–¹ç¨‹ä¸º *d* è¿”å›è¿ç»­ä¸”æ— ç•Œçš„å€¼ï¼Œå› æ­¤ä¸èƒ½ç›´æ¥ç”¨äºæ¨¡æ‹Ÿå¸ƒå°”å› å˜é‡ã€‚

æˆ‘ä»¬å¯ä»¥å°è¯•å°† *d* çš„è¿ç»­è¾“å‡ºè§†ä¸ºä¸€ç§æ¦‚ç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸èƒ½æœ‰è´Ÿæ¦‚ç‡æˆ–å¤§äº 1 çš„æ¦‚ç‡ï¼å› æ­¤ï¼Œ *d* ä¸èƒ½ä½œä¸ºæ¦‚ç‡æ¥å¯¹å¾…ã€‚

ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥è¯•ç€ç”¨[å‡ ç‡](https://en.wikipedia.org/wiki/Odds)æ¥å¤„ç†å®ƒï¼Œè€Œä¸æ˜¯æŠŠ *d* å½“ä½œä¸€ç§æ¦‚ç‡ã€‚èµ”ç‡é€šè¿‡ä»¥ä¸‹å…³ç³»ä¸æ¦‚ç‡å¯†åˆ‡ç›¸å…³:

*O* = *P* / (1 - *P* )ï¼Œ

å…¶ä¸­ *O* æ˜¯èµ”ç‡ï¼Œ *P* æ˜¯æ¦‚ç‡ã€‚

å½“æ¦‚ç‡ *P* çš„èŒƒå›´ä¸º[0ï¼Œ1]æ—¶ï¼Œèµ”ç‡ *O* çš„èŒƒå›´ä¸º[0ï¼Œâˆ)ä½¿å¾—å…¶å¯¹äºæ­£å€¼æ˜¯æ— ç•Œçš„ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦è€ƒè™‘è´Ÿå€¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å–å‡ ç‡çš„å¯¹æ•° O å¾—åˆ°å‡ ç‡çš„å¯¹æ•° l :

*l* = log( *O* )ï¼Œ

å…¶ä¸­ *l* æ˜¯è¿ç»­çš„ï¼Œæœ‰å€¼åŸŸ(-âˆï¼Œâˆ)ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•ä¸€èˆ¬æ€§æŸå¤±ï¼Œlog æ˜¯ä»¥ *e* ä¸ºåº•çš„å¯¹æ•°ã€‚

è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥ä¾¿ä½¿ç”¨ä¸Šé¢çš„çº¿æ€§å›å½’æ–¹ç¨‹ï¼Œä½¿ç”¨èµ”ç‡å’Œæ¦‚ç‡å¯¹å¸ƒå°”å› å˜é‡è¿›è¡Œå»ºæ¨¡ï¼æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯å°† *d* è§†ä¸ºå¯¹æ•°æ¯” *l* :

*l*= log(*o*)= log(*p*/(1-*p*)=*c*â‚*g*â‚+*c*â‚‚*g*â‚‚+â€¦*+câ‚˜gâ‚˜.*

æˆ‘ä»¬å¯ä»¥é‡æ–°æ’åˆ—è¿™ä¸ªç­‰å¼ï¼Œç”¨ P ä»£æ›¿:

*p*/(1-*p*)= exp(*c*â‚*g*â‚+*c*â‚‚*g*â‚‚+â€¦*+câ‚˜gâ‚˜*)
*p*= 1/(1+exp(-(*c*â‚

å¯ä»¥æ›´ç®€æ´åœ°å†™æˆ:

*p*= 1/(1+exp(-**m**áµ€**g**)ï¼Œ

å…¶ä¸­ **m** æ˜¯é•¿åº¦ä¸º *m* çš„å‘é‡ï¼ŒåŒ…å«æ¨¡å‹å‚æ•°:**m**=**[*c*â‚ï¼Œ *c* â‚‚ï¼Œâ€¦ *câ‚˜* ]áµ€ï¼Œ **g** ä¹Ÿæ˜¯é•¿åº¦ä¸º *m* çš„å‘é‡ï¼ŒåŒ…å«è§£é‡Šå˜é‡: **g è¯¥æ–¹ç¨‹ä¹Ÿè¢«ç§°ä¸º[é€»è¾‘å‡½æ•°](https://en.wikipedia.org/wiki/Logistic_function)ï¼Œå› æ­¤è¢«ç§°ä¸ºâ€œé€»è¾‘å›å½’â€ï¼****

**çº¿æ€§å›å½’æ¨¡å‹*d*=**m**áµ€**g***å·²ç»è½¬åŒ–ä¸ºé€»è¾‘å›å½’æ¨¡å‹*p*= 1/(1+exp(-**m**áµ€**g**))ï¼Œå°†æ¦‚ç‡ *P* å»ºæ¨¡ä¸º **m** å’Œ **g** çš„éçº¿æ€§å‡½æ•°ï¼è¯·æ³¨æ„ï¼ŒåŸå§‹å¸ƒå°”å› å˜é‡ *d* æ²¡æœ‰å‡ºç°åœ¨é€»è¾‘å›å½’æ¨¡å‹ä¸­â€”â€”æˆ‘ä»¬åªå¤„ç†æ¦‚ç‡ï¼*d*â€™ç¨åå°†å†æ¬¡å‡ºç°åœ¨æŸå¤±å‡½æ•°ä¸­ã€‚***

***ç­‰å¼*p*= 1/(1+exp(-**m**áµ€**g**))å¯èƒ½å¾ˆéš¾ç†è§£ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç”¨å›¾è¡¨æ¥çœ‹çœ‹æœ€ç®€å•çš„å¯èƒ½æƒ…å†µã€‚æœ€ç®€å•çš„æƒ…å†µï¼Œ*P*= 1/(1+exp(-*g*))åªæœ‰ 1 ä¸ªè§£é‡Šå˜é‡ *g* å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å½“ *g* å¢å¤§æ—¶ï¼Œ *P* è¶‹äº 1ï¼Œå½“ *g* å‡å°æ—¶ï¼Œ *P* è¶‹äº 0ã€‚å›¾è¡¨ä¸­è¿˜åŒ…æ‹¬ä¸€äº›åˆæˆå¸ƒå°”æ•°æ®ç‚¹ï¼Œç”¨äº *d* ' = 0 å’Œ *d* ' = 1ã€‚***

***![](img/9d17cfb62cb8bdddc652cf43c3d48d8b.png)***

***è“è‰²è¡¨ç¤º *P* = 1 / (1 + exp(- *g* ))ï¼Œæ©™è‰²ç‚¹è¡¨ç¤º d' = 0ï¼Œç»¿è‰²ç‚¹è¡¨ç¤º d' = 1ã€‚ä½œè€…åˆ›é€ çš„å½¢è±¡ã€‚***

***å½“ *d* ' = 1 æ—¶ï¼Œ *P* ç†æƒ³æƒ…å†µä¸‹åº”å…·æœ‰æ¥è¿‘ 1 çš„å¯¹åº”å€¼ï¼Œè€Œå½“ *d* ' = 0 æ—¶ï¼Œ *P* åº”å…·æœ‰æ¥è¿‘ 0 çš„å¯¹åº”å€¼ã€‚ç”±é€»è¾‘å›å½’æ¨¡å‹å®šä¹‰çš„åˆ†ç±»é—®é¢˜è¢«ç®€åŒ–ä¸ºå¯»æ‰¾ä¸€ç»„æ¨¡å‹å‚æ•°**m**=**[*c*â‚ã€ *c* â‚‚ã€â€¦ *câ‚˜* ]áµ€ ï¼Œè¿™å¯¼è‡´äº†è¿™é‡Œæè¿°çš„è¡Œä¸ºã€‚*****

# *****å¸ƒå°”å› å˜é‡çš„å‘é‡*****

*****åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç®—å‡ºäº†å•å¸ƒå°”å› å˜é‡ *d* çš„é€»è¾‘å›å½’æ–¹ç¨‹ã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬æ›´å¯èƒ½éœ€è¦å¤„ç†ä¸€ä¸ªå¸ƒå°”å› å˜é‡çš„å‘é‡ï¼Œå®ƒå¯¹åº”äºä¸€ç»„ä¸åŒçš„æµ‹é‡å€¼: **d** ' = [ *d'* â‚ï¼Œ *d'* â‚‚ï¼Œâ€¦ *d'â‚™* ]áµ€.*****

*****åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹åº”äº **d** ä¸­ç¬¬ *u* ä¸ªå…ƒç´ çš„æ¦‚ç‡ç”±é€»è¾‘å›å½’æ¨¡å‹ç”¨ä¸€ç»„å¯¹åº”çš„ *m* è§£é‡Šå˜é‡ *Gáµ¤* â‚ *ã€Gáµ¤* â‚‚ã€â€¦ *Gáµ¤â‚˜* æ¥å»ºæ¨¡:*****

******páµ¤*= 1/(1+exp(-(*gáµ¤*â‚*c*â‚*+gáµ¤*â‚‚*c*â‚‚+â€¦+*gáµ¤â‚˜câ‚˜*))ã€‚*****

*****æ³¨æ„ï¼Œ *Páµ¤* åªæ˜¯å‘é‡ä¸­çš„ç¬¬ *u* ä¸ªå…ƒç´ : **P** = [ *P* â‚ï¼Œ *P* â‚‚ï¼Œâ€¦ *Pâ‚™* ]áµ€.å¯¹äº **P** ,è¯¥ç­‰å¼ä¹Ÿå¯ä»¥å‘é‡å½¢å¼å†™æˆ:*****

*******P**= 1/(1+exp(-**Gm**))ï¼Œ*****

*****å…¶ä¸­ **G** æ˜¯ä¸€ä¸ªå¤§å°ä¸º*n*Ã—m çš„çŸ©é˜µï¼Œç»“æ„å¦‚ä¸‹:*****

*******g**=ã€*g*â‚â‚ã€ *G* â‚â‚‚ã€â€¦ *G* â‚ *â‚˜* ã€
ã€‚â€¦â€¦.[ *G* â‚‚â‚ã€ *G* â‚‚â‚‚ã€â€¦ *G* â‚‚ *â‚˜* ã€
ã€‚â€¦â€¦.â€¦
ã€‚â€¦â€¦.[ *Gâ‚™* â‚ï¼Œ *Gâ‚™* â‚‚ï¼Œâ€¦ *Gâ‚™â‚˜* ]]ï¼Œ*****

*****å…¶ä¸­*u*g**g**ç¬¬ä¸€è¡ŒåŒ…å« *m* è§£é‡Šå˜é‡çš„å‘é‡ *Gáµ¤* â‚ *ã€Gáµ¤* â‚‚ã€â€¦ *Gáµ¤â‚˜* ã€å’Œ**m**=**[*c*â‚ã€*******

# ********æœ€å°åŒ–äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°********

********ä¸ºäº†é‡åŒ–æˆ‘ä»¬çš„é€»è¾‘å›å½’æ¨¡å‹çš„è¡¨ç°ï¼Œæˆ‘ä»¬éœ€è¦æœ€å°åŒ–æ¨¡å‹çš„æ¦‚ç‡é¢„æµ‹ *P* å’Œå…³äºæ¨¡å‹å‚æ•° **m** çš„å¸ƒå°”åŸºç¡€çœŸå€¼ *d* ä¹‹é—´çš„è¯¯å·®ã€‚********

******ç”¨äºé‡åŒ–åˆ†ç±»é—®é¢˜ä¸­çš„é¢„æµ‹è¯¯å·®çš„æœ€å¸¸ç”¨æŸå¤±å‡½æ•°ä¹‹ä¸€æ˜¯[äºŒå…ƒäº¤å‰ç†µ](https://en.wikipedia.org/wiki/Cross_entropy)ã€‚å¯¹äºç¬¬ *u* ä¸ªæ•°æ®ç‚¹ï¼ŒäºŒå…ƒäº¤å‰ç†µä¸º:******

*******láµ¤*(**m**)=-*d*'*áµ¤*åŸæœ¨(*páµ¤*(**m**)-(1-*d*'*áµ¤*)åŸæœ¨(1 - *Páµ¤* ( **m** )ã€‚******

*******Láµ¤* ( **m** )ä¸­çš„ç¬¬ä¸€é¡¹ä»…åœ¨ *d'áµ¤* = 1 æ—¶æœ‰æ•ˆï¼Œç¬¬äºŒé¡¹ä»…åœ¨ *d* ' *áµ¤* = 0 æ—¶æœ‰æ•ˆã€‚è¦è·å¾—æ‰€æœ‰ *n* ä¸ªæ•°æ®ç‚¹çš„æŸå¤±ï¼Œæˆ‘ä»¬åªéœ€æ±‚å’Œ:******

*******l*(**m**)=Ïƒ*áµ¤láµ¤*(**m**)ã€‚******

******æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–å…³äºæ¨¡å‹å‚æ•° **m** çš„æŸå¤±å‡½æ•° *L* ( **m** )ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°† *Láµ¤* ( **m** )ç›¸å¯¹äº **m** è¿›è¡ŒåŒºåˆ†:******

******âˆ‚*láµ¤*(**m**)/âˆ‚**m**=âˆ‚/âˆ‚**m**(-*d*'*áµ¤*åŸæœ¨(*páµ¤*(**m**)-(1-*d*'*áµ¤*)åŸæœ¨(1 - *Páµ¤* ( **m** ))******

******è¿™ä¸ªå¯¼æ•°çœ‹èµ·æ¥ä»¤äººç”Ÿç•ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[ä¹˜ç§¯æ³•åˆ™](https://en.wikipedia.org/wiki/Product_rule)å°†âˆ‚*láµ¤*(**m**)/âˆ‚**m**åˆ†è§£æˆæ›´ç®€å•çš„æˆåˆ†:******

******âˆ‚*láµ¤*(**m**)/âˆ‚**m**=âˆ‚*láµ¤*(**m**)/âˆ‚*páµ¤*(**m**âˆ‚*pÇ*(**m**)/âˆ©**m**ã€‚******

******æœ¯è¯­âˆ‚*láµ¤*(**m**)/âˆ‚*páµ¤*(**m**)å¯ä»¥è¯„ä¼°ä¸º:******

******âˆ‚*láµ¤*(**m**)/âˆ‚*páµ¤*(**m**)*=-**d*'*áµ¤*/*páµ¤*+(1-*d*'*áµ¤*)/(1******

******è€Œâˆ‚*páµ¤*(**m**)/âˆ‚**m**å¯ä»¥ç”±*p*= 1/(1+exp(-**m**áµ€**g**))è®¡ç®—ä¸º:******

******âˆ‚*páµ¤*(**m**)/âˆ‚**m**=-1/(1+exp(-**m**áµ€**g**áµ¤)(-**g**t118ã€‘áµ¤**t121ã€‘exp(-**m**áµ€**g**********

****å…¶ä¸­**g**t140ã€‘áµ¤=[*gâ‚™*â‚ã€ *Gâ‚™* â‚‚ã€â€¦ *Gâ‚™â‚˜* ]áµ€æ˜¯åŒ…å«çŸ©é˜µ **G** çš„ç¬¬ *u* è¡Œçš„å‘é‡ã€‚****

****ä½¿ç”¨è¿™äº›ç»“æœï¼Œå¯¼æ•°âˆ‚*láµ¤*(**m**)/âˆ‚**m**ç°åœ¨å¯ä»¥è¢«è¯„ä¼°ä¸º:****

****âˆ‚*láµ¤*(**m**)/âˆ‚**m**=(*páµ¤-**d*'*áµ¤*)/(*páµ¤*(1-*páµ¤*)**g***áµ¤**p*(1-*p*)****

****ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†è®¾ç½®âˆ‚*láµ¤*(**m**)/âˆ‚**m**= 0ï¼Œå¹¶æ±‚è§£æœ€ä½³æ¨¡å‹å‚æ•° **m** ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬æ— æ³•æ±‚è§£å‡º **m** ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å‡ºç°åœ¨âˆ‚*láµ¤*(**m**)/âˆ‚**m**çš„è¡¨è¾¾å¼ä¸­ï¼å› æ­¤ï¼Œçœ‹èµ·æ¥æˆ‘ä»¬å¯èƒ½éœ€è¦æ±‚åŠ©äºæ•°å€¼æ–¹æ³•æ¥æœ€å°åŒ–ç›¸å¯¹äº **m** çš„ *Láµ¤* ( **m** )ã€‚****

# ****éšæœºæ¢¯åº¦ä¸‹é™****

****å¯ä»¥ä½¿ç”¨å„ç§æ•°å€¼æ–¹æ³•ï¼Œä¾‹å¦‚[ç‰›é¡¿æ³•](https://en.wikipedia.org/wiki/Newton's_method)æˆ–[éšæœºæ¢¯åº¦ä¸‹é™æ³•](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)æ¥æœ€å°åŒ–é€»è¾‘å›å½’æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚å¯¹äºæœ¬æ–‡ï¼Œæˆ‘ä»¬å°†æ¢ç´¢éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œå…¶å˜ä½“ä»åœ¨æ›´é«˜çº§çš„æ¨¡å‹ä¸­ç§¯æä½¿ç”¨ï¼Œå¦‚ç¥ç»ç½‘ç»œæˆ–[è”é‚¦å­¦ä¹ æ¡†æ¶](https://arxiv.org/pdf/1602.05629.pdf)ã€‚****

****éšæœºæ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¸€é˜¶è¿­ä»£ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºé€šè¿‡æ²¿æœ€é™¡è´Ÿæ¢¯åº¦æ–¹å‘è¿­ä»£ç§»åŠ¨æ¥æœç´¢å¯å¾®å‡½æ•°çš„å±€éƒ¨æœ€å°å€¼ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»ç»¿ç‚¹å¼€å§‹ï¼Œå¦‚æœæˆ‘ä»¬æ²¿ç€è“è‰²æ›²çº¿æœç€æœ€é™¡è´Ÿæ¢¯åº¦çš„æ–¹å‘ç»§ç»­åœ¨ *x* ä¸­å°æ­¥å‰è¿›ï¼Œæœ€ç»ˆåº”è¯¥ä¼šåˆ°è¾¾çº¢ç‚¹â€”â€”æœ€å°ç‚¹ã€‚****

****![](img/f0b29c5e5040d144ab65150ae92199ad.png)****

****ä»ç»¿ç‚¹åˆ°çº¢ç‚¹å‘æœ€é™¡çš„è´Ÿæ¢¯åº¦æ–¹å‘ç§»åŠ¨ã€‚ä½œè€…åˆ›é€ çš„å½¢è±¡ã€‚****

****è®°ä½ï¼Œæˆ‘ä»¬å¸Œæœ›ç›¸å¯¹äº **m** æœ€å°åŒ– *L* ( **m** )ã€‚åˆ©ç”¨éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæˆ‘ä»¬æ²¿ç€æœ€é™¡è´Ÿæ¢¯åº¦çš„æ–¹å‘åœ¨ **m** ä¸­é‡‡å–å¤šä¸ªå°æ­¥éª¤:-âˆ‚*láµ¤*(**m**)/âˆ‚**m**å‘ *L* ( **m** )çš„æœ€å°å€¼ç‚¹å‰è¿›ã€‚****

****å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨âˆ‚*láµ¤*(**m**)/âˆ‚**m**å¯¹ **d** ' = [ *d'* â‚ã€ *d'* â‚‚ã€â€¦ *d'â‚™* ]áµ€:ä¸­çš„æ‰€æœ‰ *n* å…ƒç´ è¿­ä»£æ›´æ–°æ¨¡å‹å‚æ•° **m******

******m**:=**m**-*r*âˆ‚*láµ¤*(**m**)/âˆ‚**mï¼Œ******

****å…¶ä¸­ *R* æ˜¯æŸä¸ªå­¦ä¹ ç‡ï¼Œå®ƒæ§åˆ¶æˆ‘ä»¬æ‰€èµ°çš„æ­¥æ•°çš„å¤§å°ã€‚****

****å­¦ä¹ ç‡çš„å¤§å°éå¸¸é‡è¦:å¦‚æœ *R* å¤ªå°ï¼Œæ­¥é•¿å‡ ä¹ä¸ä¼šæ”¹å˜ **m** çš„å€¼ï¼Œå¦ä¸€æ–¹é¢ï¼Œå¦‚æœ *R* å¤ªå¤§ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½ä¼šè¶…è¿‡æœ€å°ç‚¹ï¼ä¸º *R* é€‰æ‹©ä¸€ä¸ªå¥½çš„å€¼å–å†³äºæ‰€æ¶‰åŠçš„æ•°æ®ã€‚****

****éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•å¯ä»¥ç”¨ä¼ªä»£ç ç¼–å†™:****

```
****# Initialize the model parameters with some initial value.
m = initialize_m()# Iterate the algorithm until some termination condition is reached.
while termination_condition == False:
    # Shuffle the rows the training data.
    shuffle_training_data() # Iterate through all the n elements in the data.
    for i in range(n):
        # Update the model parameters m using the derivative of the 
        # loss function for that element.
        m = m - R * dLdm[i] # Check if the termination condition is reached.
    check_termination_condition()****
```

# ****æ¦‚è¿°****

****æˆ‘ä»¬å·²ç»è®¨è®ºäº†å¾ˆå¤šç»†èŠ‚ï¼Œæ‰€ä»¥åœ¨ç»§ç»­ç”¨ Python ç¼–ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„å†…å®¹ï¼****

1.  ****æˆ‘ä»¬å¸Œæœ›å¯¹å¸ƒå°”å› å˜é‡ *d* ' âˆˆ {0ï¼Œ1}è¿›è¡Œå»ºæ¨¡ã€‚****
2.  ****æˆ‘ä»¬æ²¡æœ‰ç›´æ¥å¯¹å¸ƒå°”å€¼å»ºæ¨¡ï¼Œè€Œæ˜¯ä½¿ç”¨:*p*= 1/(1+exp(-(*c*â‚*g*â‚+*c*â‚‚*g*â‚‚+â€¦*+câ‚˜gâ‚˜*))å¯¹æ¦‚ç‡å»ºæ¨¡ã€‚****
3.  ****æˆ‘ä»¬ç”¨äºŒå…ƒäº¤å‰ç†µæ¥è¡¡é‡æ¨¡å‹çš„é¢„æµ‹è¯¯å·®:
    *(**m**)=-*d*'*áµ¤*log(*páµ¤*(**m**)-(1-*d*'*áµ¤*)log(1-ã€t74*****
4.  ****ä¸ºäº†æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹å‚æ•° **m** ï¼Œæˆ‘ä»¬ç›¸å¯¹äº **m** æœ€å°åŒ– *Láµ¤* ( **m** )ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ä¸ªæœ€å°åŒ–é—®é¢˜å¿…é¡»ä½¿ç”¨æ•°å€¼æ–¹æ³•æ¥è§£å†³ï¼Œä¾‹å¦‚éšæœºæ¢¯åº¦ä¸‹é™ã€‚****

# ****ä½¿ç”¨ Python è¿›è¡Œé€»è¾‘å›å½’****

****æˆ‘ä»¬ç°åœ¨å·²ç»åšäº†è¶³å¤Ÿçš„æ•°å­¦ï¼åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Python åˆ›å»ºä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹æ±‚è§£å™¨ï¼****

****é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰å®ç°é€»è¾‘å›å½’æ¨¡å‹çš„`logistic_regression`:****

```
**import numpy as npdef logistic_regression(G, m):
    """
    Logistic regression model. Inputs
    G: np.array of shape nxm containing the explanatory variables.
    m: np.array of length m containing the model parameters. Returns
    P: np.array of length n containing the modeled probabilities.
    """
    return 1 / (1 + np.exp(-np.dot(G, m)))**
```

****æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰`binary_cross_entropy`å’Œ`binary_cross_entropy_grad`ï¼Œå®ƒä»¬è®¡ç®—äºŒå…ƒäº¤å‰ç†µåŠå…¶æ¢¯åº¦ã€‚****

```
**def binary_cross_entropy(d, P):
    """
    Calculates the mean binary cross entropy for all n data points. Inputs
    d: np.array of length n containing the boolean dependent 
       variables.
    P: np.array of length n containing the model's probability 
       predictions. Returns
    bce: float containing the mean binary cross entropy.
    """

    # For d = 1:
    d_1 = d[d == 1] * np.log(P[d == 1])
    # For d = 0:
    d_0 = (1 - d[d == 0]) * np.log(1 - P[d == 0])
    return -np.mean(d_1) - np.mean(d_0)def binary_cross_entropy_grad(g, d, p):
    """
    Calculates the gradient of the binary cross entropy loss for a 
    single data point. Inputs:
    g: np.array of length m containing the vector of explanatory 
       variables for 1 data point.
    d: integer containing the boolean dependent variable for 1 data 
       point.
    p: float containing the model's probability predictions for 1 
       data point. Returns:
    bce_grad: np.array of length m containing the gradients of the 
              binary cross entropy.
    """
    return g * (p - d)**
```

****æœ€åï¼Œæˆ‘ä»¬å®šä¹‰`SGD`ï¼Œå®ƒæ‰§è¡Œéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®ç‚¹é€ä¸€è¿­ä»£æ›´æ–°æ¨¡å‹å‚æ•°ã€‚****

```
**def SGD(d, G, m, R = 0.01):
    """
    Stochastic gradient descent. Updates the model parameters m 
    iteratively using all individual data points in d.

    Inputs
    d: np.array of length n containing the boolean dependent 
       variables.
    G: np.array of shape nxm containing the explanatory variables.
    m: np.array of length m containing the initial model parameters.
    R: float containing the SGD learning rate.

    Returns
    m: np.array of length m containing the updated model parameters.
    """
    # Shuffle the data points.
    indices = np.arange(len(d))
    np.random.shuffle(indices)
    d = d[indices]
    G = G[indices] # Iteratively update the model parameters using every single
    # data point one by one.
    for i in range(len(d)):
        P = logistic_regression(G[i], m)
        m = m - R * binary_cross_entropy_grad(G[i], d[i], P) return m**
```

****ç°åœ¨æˆ‘ä»¬åªéœ€è¦ä¸€äº›æ•°æ®æ¥æµ‹è¯•æˆ‘ä»¬çš„é€»è¾‘å›å½’æ±‚è§£å™¨ï¼æˆ‘ä»¬ä½¿ç”¨ä»`sklearn`è·å¾—çš„é¸¢å°¾èŠ±åˆ†ç±»æ•°æ®é›†ã€‚åŸå§‹æ•°æ®é›†æä¾›äº† 3 ä¸ªä¸åŒçš„ç±»:0ã€1 å’Œ 2ã€‚æˆ‘ä»¬å°†æ•°æ®é›†ä»…é™äº 0 å’Œ 1 çš„ç±»ã€‚****

```
**from sklearn.datasets import load_iris# Load the iris flower dataset from sklearn.
data = load_iris()
G = data["data"]   # Explanatory variable matrix.
d = data["target"] # Dependent variable array.# The dataset has 3 classes. For the time being, restrict the data
# to only classes 0 and 1 in order to create boolean dependent
# variables.                  
want = (d == 0) | (d == 1)G = G[want]
d = d[want]**
```

****å› ä¸ºæˆ‘ä»¬çš„é€»è¾‘å›å½’æ¨¡å‹****

*****p*= 1/(1+exp(-(*c*â‚*g*â‚+*c*â‚‚*g*â‚‚+â€¦*+câ‚˜gâ‚˜*))****

****æ²¡æœ‰æ˜ç¡®åŒ…æ‹¬åå·®é¡¹ï¼Œæˆ‘ä»¬éœ€è¦åœ¨çŸ©é˜µ **G** çš„ç¬¬ä¸€åˆ—æ·»åŠ ä¸€åˆ— 1ã€‚è¿™åˆ— 1 å°†æ‰®æ¼”åç½®é¡¹çš„è§’è‰²ã€‚****

```
**# Because we do not explicitly account for a bias term in our
# logistic regression model, we need to add a column of 1s to the
# matrix G. This will play the term of the bias term.
G = np.hstack([np.ones([len(G), 1]), G])# Take a look at the first 3 data points in the dataset.
for i in range(3):
    print(G[i], d[i]) [1\.  5.1 3.5 1.4 0.2] 0
[1\.  4.9 3\.  1.4 0.2] 0
[1\.  4.7 3.2 1.3 0.2] 0**
```

****ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®ç»“æ„ **G** å’Œ **d** ï¼Œæ˜¯æ—¶å€™å¯¹æ¨¡å‹å‚æ•° **m** è¿›è¡Œåˆæ­¥çŒœæµ‹äº†ï¼ç”±äºæ¯ä¸ªæ•°æ®ç‚¹æœ‰ 5 ä¸ªè§£é‡Šå˜é‡ï¼Œå› æ­¤åœ¨ **m** ä¸­å°†æœ‰ 5 ä¸ªæ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬çŒœæµ‹æ¨¡å‹å‚æ•°çš„å€¼ä¸º 1ã€‚****

```
**m = np.ones(5)**
```

****æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿è¡Œéšæœºæ¢¯åº¦ä¸‹é™è§£ç®—å™¨ 20 æ¬¡è¿­ä»£ï¼Œä½¿ç”¨å­¦ä¹ ç‡ *R* = 0.01 æ¥é˜²æ­¢ç®—æ³•è¶…è¿‡æœ€å°ç‚¹ã€‚****

```
**# Run the SGD algorithm 20 times.
for i in range(20):
    # Use a learning rate of 0.01.
    m = SGD(d, G, m, 0.01) # Calculate the mean loss for all data in the dataset.
    P = logistic_regression(G, m)
    bce = binary_cross_entropy(d, P) print("Step {:3d} : m = [ ".format(i+1), end = "")
    for j in range(len(m)):
        print("{:6.3f}".format(m[j]), end = " ")
    print("], loss = {:.3f}.".format(bce)) Step   1 : m = [  0.692 -0.506 -0.080  0.640  0.961 ], loss = 0.548\. Step   2 : m = [  0.674 -0.517 -0.207  0.854  1.050 ], loss = 0.417\. Step   3 : m = [  0.643 -0.609 -0.355  0.985  1.110 ], loss = 0.322\. Step   4 : m = [  0.629 -0.620 -0.440  1.124  1.168 ], loss = 0.258\. Step   5 : m = [  0.616 -0.638 -0.514  1.234  1.214 ], loss = 0.218\. Step   6 : m = [  0.608 -0.635 -0.568  1.337  1.257 ], loss = 0.187\. Step   7 : m = [  0.598 -0.652 -0.626  1.417  1.291 ], loss = 0.165\. Step   8 : m = [  0.592 -0.652 -0.668  1.495  1.324 ], loss = 0.148\. Step   9 : m = [  0.579 -0.691 -0.726  1.545  1.348 ], loss = 0.135\. Step  10 : m = [  0.573 -0.699 -0.765  1.605  1.373 ], loss = 0.123\. Step  11 : m = [  0.568 -0.697 -0.796  1.665  1.398 ], loss = 0.113\. Step  12 : m = [  0.563 -0.702 -0.828  1.716  1.420 ], loss = 0.104\. Step  13 : m = [  0.556 -0.716 -0.862  1.759  1.439 ], loss = 0.097\. Step  14 : m = [  0.551 -0.723 -0.891  1.802  1.457 ], loss = 0.091\. Step  15 : m = [  0.547 -0.729 -0.918  1.843  1.475 ], loss = 0.086\. Step  16 : m = [  0.542 -0.736 -0.944  1.880  1.491 ], loss = 0.081\. Step  17 : m = [  0.538 -0.741 -0.968  1.916  1.506 ], loss = 0.077\. Step  18 : m = [  0.535 -0.741 -0.987  1.953  1.522 ], loss = 0.073\. Step  19 : m = [  0.531 -0.748 -1.009  1.985  1.536 ], loss = 0.069\. Step  20 : m = [  0.528 -0.748 -1.027  2.018  1.549 ], loss = 0.066.**
```

****ä»ä¸Šé¢çš„ç»“æœæ¥çœ‹ï¼Œå¹³å‡äºŒè¿›åˆ¶äº¤å‰ç†µä¼¼ä¹ä¸€ç›´åœ¨ä¸‹é™ï¼Œè¿™æ„å‘³ç€è¯¥æ¨¡å‹åº”è¯¥æœç€ä¸€ç»„ä»¤äººæ»¡æ„çš„å‚æ•°æ”¶æ•›ï¼Œæ‰€ä»¥æ¥ä¸‹æ¥è®©æˆ‘ä»¬æ£€æŸ¥è¯¥æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç”±äºè¿™æ˜¯ä¸€ä¸ªå°æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ç®€å•åœ°ç›´æ¥æ‰“å°æ‰€æœ‰ *n* ä¸ªæ•°æ®ç‚¹çš„ç»“æœï¼Œå¹¶ç›´è§‚åœ°æ£€æŸ¥å®ƒä»¬ã€‚****

```
**# Print the ground truth.
print(d)# Print the modeled probabilities of the logistic regression model, 
# rounded to the nearest integer value.
print(np.round(logistic_regression(G, m)).astype(int)) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1][0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]**
```

****ä¼¼ä¹æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å®Œç¾åœ°åˆ†ç±»äº†æ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ªæ•°æ®ç‚¹ï¼æ¥ä¸‹æ¥è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹æ¨¡å‹çš„å®é™…éèˆå…¥è¾“å‡ºï¼ŒåŒ…æ‹¬ *d* ' = 1 å’Œ *d* ' = 0 ä¸¤ç§æƒ…å†µã€‚****

```
**# Get the probability predictions from the model.
P = logistic_regression(G, m)# d' = 0
print("d' = {}, P = {:.3f}.".format(d[0], P[0]))
# d' = 1
print("d' = {}, P = {:.3f}.".format(d[-1], P[-1])) d' = 0, P = 0.023\. 
d' = 1, P = 0.975.**
```

****æˆ‘ä»¬çœ‹åˆ°ï¼Œå½“ *d* ' = 1 æ—¶ï¼Œ *P* éå¸¸æ¥è¿‘ 1ï¼Œå½“ *d* ' = 0 æ—¶ï¼Œ *P* éå¸¸æ¥è¿‘ 0â€”â€”è¿™æ­£æ˜¯æˆ‘ä»¬é¢„æœŸçš„è¡Œä¸ºï¼****

# ****æé†’ä¸€å¥****

****åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œä½¿ç”¨çš„æ•°æ®æ˜¯éå¸¸å®¹æ˜“åˆ†ç¦»çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ *d* ' = 1 å’Œ *d* ' = 0 ä¹‹é—´çš„è§£é‡Šå˜é‡å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨é¢„æµ‹ä¸­è·å¾—å®Œç¾ç»“æœçš„ã€‚****

****åœ¨å®é™…çš„æ•°æ®ç§‘å­¦é¡¹ç›®ä¸­ï¼Œæ•°æ®é€šå¸¸æ²¡æœ‰è¿™ä¹ˆå¹²å‡€â€”â€”æˆ‘ä»¬éœ€è¦[è®¾è®¡æ•°æ®](https://en.wikipedia.org/wiki/Feature_engineering)ä¸­çš„ç‰¹å¾ï¼Œè€ƒè™‘ **d** ä¸­ 0 å’Œ 1 çš„æ•°é‡ä¹‹é—´çš„æ¯”ä¾‹ï¼Œä»¥åŠ[å¾®è°ƒæ¨¡å‹è®­ç»ƒè¿‡ç¨‹](https://en.wikipedia.org/wiki/Hyperparameter_optimization)ã€‚æ ¹æ®æ•°æ®ï¼Œå…¶ä»–æŸå¤±å‡½æ•°ï¼Œå¦‚[èšç„¦æŸå¤±](https://arxiv.org/pdf/1708.02002.pdf)æˆ–[æ­£åˆ™åŒ–æŸå¤±å‡½æ•°](https://en.wikipedia.org/wiki/Regularization_(mathematics))å¯èƒ½æ¯”äºŒå…ƒäº¤å‰ç†µæ›´å¥½ã€‚****

****åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚éšæœºæ¢¯åº¦ä¸‹é™å¹¶ä¸æ€»æ˜¯é€‚ç”¨äºæ‰€æœ‰æ•°æ®é›†ï¼Œåœ¨å…¶ä»–æ•°æ®é›†ä¸­ï¼Œå…¶ä»–æ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚[æœ‰é™å†…å­˜ BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) æˆ– [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) å¯èƒ½é€‚ç”¨ã€‚****

****æŒ‡æ ‡ä¹Ÿæ˜¯å¦ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„é‡è¦å› ç´ ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç®€å•åœ°æ¯”è¾ƒäº†æ¨¡å‹çš„è¾“å‡ºã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œå¯¹äºåŒ…å«æ•°åƒä¸ªæ•°æ®ç‚¹çš„æ•°æ®é›†ï¼Œè¿™ç§æ–¹æ³•æ˜¾ç„¶æ˜¯è¡Œä¸é€šçš„ï¼ä½¿ç”¨å“ªç§æŒ‡æ ‡å°†å–å†³äºæ‰€æ¶‰åŠçš„æ•°æ®ã€‚åˆ†ç±»ä¸­ä½¿ç”¨çš„å¸¸ç”¨æŒ‡æ ‡åŒ…æ‹¬[å‡†ç¡®åº¦](https://en.wikipedia.org/wiki/Accuracy_and_precision)ã€[æ¥æ”¶å™¨å·¥ä½œç‰¹æ€§](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)æˆ–[ç²¾ç¡®åº¦å’Œå¬å›ç‡](https://en.wikipedia.org/wiki/Precision_and_recall)ã€‚****

****æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å®é™…ä¸Šï¼Œæ•´ä¸ªæ•°æ®é›†åº”è¯¥åˆ†æˆè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼è¯¥æ¨¡å‹åº”åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨éªŒè¯æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•-è¿™æ˜¯ä¸ºäº†ç¡®ä¿è¯¥æ¨¡å‹ä¸åªæ˜¯è®°ä½äº†è®­ç»ƒæ•°æ®é›†ï¼Œè€Œä¸”èƒ½å¤Ÿå°†å…¶é¢„æµ‹æ¨å¹¿åˆ°ä»¥å‰æ²¡æœ‰è§è¿‡çš„æ•°æ®ï¼****

****æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ æ˜¯æå…¶æ·±å…¥å’Œå¹¿é˜”çš„é¢†åŸŸï¼ä¸€æ—¦ä½ å®Œå…¨ç†è§£äº†åŸºæœ¬åŸç†æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œé‚£ä¹ˆå°±å¼€å§‹èŠ±äº›æ—¶é—´æ¢ç´¢æ›´é«˜çº§çš„è¯é¢˜å§ï¼****

# ****æ‘˜è¦****

****åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€»è¾‘å›å½’æ¨¡å‹èƒŒåçš„æ•°å­¦ï¼Œå¹¶ä½¿ç”¨ Python åˆ›å»ºäº†ä¸€ä¸ªåŸºäºéšæœºæ¢¯åº¦ä¸‹é™çš„æ±‚è§£å™¨ï¼ä»Šå¤©æ¢ç´¢çš„æ¦‚å¿µï¼Œå¦‚ä½¿ç”¨æ•°å€¼æ–¹æ³•æœ€å°åŒ–æŸå¤±å‡½æ•°å¯èƒ½æ˜¯åŸºæœ¬çš„ï¼Œä½†é€‚ç”¨äºæ‰€æœ‰å…¶ä»–é«˜çº§æ¨¡å‹ï¼Œå¦‚æ·±åº¦å­¦ä¹ å›¾åƒåˆ†ç±»æˆ–åˆ†å‰²æ¨¡å‹ï¼æˆ‘å¸Œæœ›æ‚¨èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åˆ†ç±»æ¨¡å‹ä»Šå¤©æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ä¸€å¦‚æ—¢å¾€çš„æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼Œä¸‹ç¯‡æ–‡ç« å†è§ï¼****

# ****å‚è€ƒ****

****[1] W. M. Menke (2012)ï¼Œ*åœ°çƒç‰©ç†æ•°æ®åˆ†æ:ç¦»æ•£é€†ç†è®º MATLAB ç‰ˆ*ï¼ŒElsevierã€‚
ã€2ã€‘è‰¾ä¼¦Â·bÂ·å”å°¼(2014)ï¼Œ*Think Stats Python ä¸­çš„æ¢ç´¢æ€§æ•°æ®åˆ†æ*ï¼Œç»¿èŒ¶å‡ºç‰ˆç¤¾ã€‚****
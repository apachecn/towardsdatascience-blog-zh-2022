# ä½¿ç”¨ NLP çš„æ–‡æœ¬æ‘˜è¦:TextRank vs Seq2Seq vs BART

> åŸæ–‡ï¼š<https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09>

![](img/bf7a59d41b19f964ce71b34753c515bf.png)

ä½œè€…å›¾ç‰‡

## ä½¿ç”¨ Pythonã€Gensimã€Tensorflowã€Transformers è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†

## æ‘˜è¦

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ NLP å’Œ Python è§£é‡Š 3 ç§ä¸åŒçš„æ–‡æœ¬æ‘˜è¦ç­–ç•¥:è€å¼çš„ *TextRank* (å¸¦æœ‰ *gensim* )ã€*è‘—åçš„ *Seq2Seq (* å¸¦æœ‰ *tensorflow* )å’Œå°–ç«¯çš„ *BART* (å¸¦æœ‰ *transformers* )ã€‚*

![](img/70c1cbc132ac76ac11ddec89d9c66977.png)

ä½œè€…å›¾ç‰‡

[**ã€NLP(è‡ªç„¶è¯­è¨€å¤„ç†)**](https://en.wikipedia.org/wiki/Natural_language_processing) æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œç ”ç©¶è®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç»™è®¡ç®—æœºç¼–ç¨‹ä»¥å¤„ç†å’Œåˆ†æå¤§é‡è‡ªç„¶è¯­è¨€æ•°æ®ã€‚æœ€å›°éš¾çš„ NLP ä»»åŠ¡æ˜¯è¾“å‡ºä¸æ˜¯å•ä¸ªæ ‡ç­¾æˆ–å€¼(å¦‚åˆ†ç±»å’Œå›å½’)ï¼Œè€Œæ˜¯ä¸€ä¸ªå…¨æ–°çš„æ–‡æœ¬(å¦‚ç¿»è¯‘ã€æ‘˜è¦å’Œå¯¹è¯)ã€‚

**æ–‡æœ¬æ‘˜è¦**æ˜¯åœ¨ä¸æ”¹å˜æ–‡æ¡£å«ä¹‰çš„æƒ…å†µä¸‹ï¼Œå‡å°‘æ–‡æ¡£çš„å¥å­å’Œå•è¯æ•°é‡çš„é—®é¢˜ã€‚ä»åŸå§‹æ–‡æœ¬æ•°æ®ä¸­æå–ä¿¡æ¯å¹¶å°†å…¶ç”¨äºæ‘˜è¦æ¨¡å‹æœ‰ä¸åŒçš„æŠ€æœ¯ï¼Œæ€»ä½“æ¥è¯´ï¼Œå®ƒä»¬å¯ä»¥åˆ†ä¸º**æå–å‹**å’Œ**æŠ½è±¡å‹ã€‚**æå–æ–¹æ³•é€‰æ‹©æ–‡æœ¬ä¸­æœ€é‡è¦çš„å¥å­(ä¸ä¸€å®šç†è§£æ„æ€)ï¼Œå› æ­¤ç»“æœæ‘˜è¦åªæ˜¯å…¨æ–‡çš„å­é›†ã€‚ç›¸åï¼ŒæŠ½è±¡æ¨¡å‹ä½¿ç”¨é«˜çº§ NLP(å³å•è¯åµŒå…¥)æ¥ç†è§£æ–‡æœ¬çš„è¯­ä¹‰ï¼Œå¹¶ç”Ÿæˆæœ‰æ„ä¹‰çš„æ‘˜è¦ã€‚å› æ­¤ï¼ŒæŠ½è±¡æŠ€æœ¯å¾ˆéš¾ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤§é‡çš„å‚æ•°å’Œæ•°æ®ã€‚

æœ¬æ•™ç¨‹æ¯”è¾ƒäº†è€æ´¾çš„æ–¹æ³• *TextRank* (æå–)ã€æµè¡Œçš„ç¼–ç å™¨-è§£ç å™¨ç¥ç»ç½‘ç»œ *Seq2Seq* (æŠ½è±¡)å’Œæœ€å…ˆè¿›çš„åŸºäºæ³¨æ„åŠ›çš„*å˜å½¢é‡‘åˆš*(æŠ½è±¡)ï¼Œå®ƒä»¬å·²ç»å½»åº•æ”¹å˜äº† NLP é¢†åŸŸã€‚

æˆ‘å°†å±•ç¤ºä¸€äº›æœ‰ç”¨çš„ Python ä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„æƒ…å†µ(åªéœ€å¤åˆ¶ã€ç²˜è´´ã€è¿è¡Œ)ï¼Œå¹¶é€šè¿‡æ³¨é‡Šéå†æ¯ä¸€è¡Œä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å¤åˆ¶è¿™ä¸ªç¤ºä¾‹(ä¸‹é¢æ˜¯å®Œæ•´ä»£ç çš„é“¾æ¥)ã€‚

[](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_summarization.ipynb) [## data science _ artificial intelligence _ Utils/example _ text _ summary . ipynb at masterâ€¦

### æ•°æ®ç§‘å­¦é¡¹ç›®å’Œäººå·¥æ™ºèƒ½ç”¨ä¾‹çš„ç¤ºä¾‹â€¦

github.com](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_summarization.ipynb) 

æˆ‘å°†ä½¿ç”¨â€œ**CNN Daily Mail**â€**æ•°æ®é›†ï¼Œå…¶ä¸­ä¸ºæ‚¨æä¾›äº† CNN å’Œã€Šæ¯æ—¥é‚®æŠ¥ã€‹è®°è€…ç”¨è‹±è¯­æ’°å†™çš„æ•°åƒç¯‡æ–°é—»æ–‡ç« ï¼Œä»¥åŠæ¯ç¯‡æ–‡ç« çš„æ‘˜è¦(ä»¥ä¸‹é“¾æ¥)ã€‚**

**[](https://huggingface.co/datasets/cnn_dailymail) [## cnn_dailymail æ‹¥æŠ±è„¸çš„æ•°æ®é›†

### æˆ‘ä»¬æ­£åœ¨é€šè¿‡å¼€æºå’Œå¼€æ”¾ç§‘å­¦æ¥æ¨è¿›å’Œæ°‘ä¸»åŒ–äººå·¥æ™ºèƒ½çš„æ—…ç¨‹ã€‚

huggingface.co](https://huggingface.co/datasets/cnn_dailymail) 

ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†ç»å†:

*   è®¾ç½®:å¯¼å…¥åŒ…ï¼Œè¯»å–æ•°æ®ï¼Œé¢„å¤„ç†ã€‚
*   ç”¨ *gensim* æ‹Ÿåˆ *TextRank* ä»¥æ„å»ºåŸºçº¿ï¼Œå¹¶ä½¿ç”¨ ROUGE æŒ‡æ ‡å’Œæ•°æ®å¯è§†åŒ–è¯„ä¼°ç»“æœã€‚
*   ç”¨ *tensorflow/keras* æ‹Ÿåˆ *Seq2Seq* æ¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
*   é€šè¿‡ HuggingFace ä½¿ç”¨é¢„è®­ç»ƒçš„ *BART* å’Œ*å˜å½¢é‡‘åˆš*åº“ã€‚** 

## **è®¾ç½®**

**é¦–å…ˆï¼Œæˆ‘éœ€è¦å¯¼å…¥ä»¥ä¸‹åº“:**

```
**## for data** import **datasets** #(1.13.3)import **pandas** aspd  #(0.25.1)
import **numpy** #(1.16.4)**## for plotting**
import **matplotlib**.pyplot as plt  #(3.1.2)
import **seaborn** as sns  #(0.9.0)**## for preprocessing**
import **re**
import **nltk** #(3.4.5)
import **contractions** #(0.0.18)**## for textrank**
import **gensim** #(3.8.1)**## for evaluation** import **rouge ** #(1.0.0)
import **difflib****## for seq2seq**
from **tensorflow**.keras import callbacks, models, layers, preprocessing as kprocessing #(2.6.0)**## for bart**
import **transformers** #(3.0.1)
```

**ç„¶åæˆ‘é€šè¿‡ HuggingFace ä½¿ç”¨[ä¸“ç”¨åº“åŠ è½½æ•°æ®é›†:](https://huggingface.co/docs/datasets/)**

```
**## load the full dataset of 300k articles** dataset = **datasets**.load_dataset("cnn_dailymail", '3.0.0')
lst_dics = [dic for dic in dataset["train"]]**## keep the first N articles if you want to keep it lite** 
dtf = **pd**.DataFrame(lst_dics).rename(columns={"article":"text", 
      "highlights":"y"})[["text","y"]].head(20000)
dtf.head()
```

**![](img/b7b27a46c4db9d44454685280277aa18.png)**

**ä½œè€…å›¾ç‰‡**

**è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªéšæœºçš„ä¾‹å­:**

```
i = 1
print("--- Full text ---")
print(dtf["text"][i])
print("--- Summary ---")
print(dtf["y"][i])
```

**![](img/3fc88aa4c89f4e756d2510cbcd254239.png)**

**æ¥è‡ª CNN æ¯æ—¥é‚®ä»¶æ•°æ®é›†çš„æ–‡æœ¬**

**åœ¨è¿™é‡Œï¼Œæˆ‘ç”¨çº¢è‰²æ‰‹åŠ¨æ ‡è®°äº†æ‘˜è¦ä¸­æåˆ°çš„ä¿¡æ¯ã€‚ä½“è‚²æ–‡ç« å¯¹æœºå™¨æ¥è¯´å¾ˆéš¾ï¼Œå› ä¸ºæ²¡æœ‰å¤ªå¤šçš„ç©ºé—´æ¥è§£é‡Šä»€ä¹ˆæ˜¯é‡è¦çš„ï¼Œä»€ä¹ˆæ˜¯ä¸é‡è¦çš„â€¦æ ‡é¢˜å¿…é¡»æŠ¥é“ä¸»è¦ç»“æœã€‚æˆ‘å°†æŠŠè¿™ä¸ªä¾‹å­æ”¾åœ¨æµ‹è¯•é›†ä¸­æ¥æ¯”è¾ƒæ¨¡å‹ã€‚**

```
dtf_train = dtf.iloc[i+1:]
dtf_test = dtf.iloc[:i+1]
```

## **æ–‡æœ¬æ’å**

**[*text rank*](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)(2004)æ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ–‡æœ¬å¤„ç†æ’åæ¨¡å‹ï¼ŒåŸºäº [**Google çš„ *PageRank***](https://en.wikipedia.org/wiki/PageRank) ç®—æ³•ï¼Œåœ¨ä¸€ä¸ªæ–‡æœ¬ä¸­å¯»æ‰¾æœ€ç›¸å…³çš„å¥å­ã€‚ *PageRank* æ˜¯ 1998 å¹´ Google æœç´¢å¼•æ“ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªå¯¹ç½‘é¡µè¿›è¡Œæ’åºçš„ç®—æ³•ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå¦‚æœé¡µé¢ A é“¾æ¥åˆ°é¡µé¢ Bï¼Œé¡µé¢ Cï¼Œé¡µé¢ B é“¾æ¥åˆ°é¡µé¢ Cï¼Œæ’åºå°†æ˜¯é¡µé¢ Cï¼Œé¡µé¢ Bï¼Œé¡µé¢ Aã€‚**

**![](img/c52fe6b7f4a01b79dcb47bfd4723c06e.png)**

**ä½œè€…å›¾ç‰‡**

**TextRank éå¸¸å®¹æ˜“ä½¿ç”¨ï¼Œå› ä¸ºå®ƒæ˜¯æ— äººç›‘ç®¡çš„ã€‚é¦–å…ˆï¼Œæ•´ä¸ªæ–‡æœ¬è¢«åˆ†å‰²æˆå¥å­ï¼Œç„¶åè¯¥ç®—æ³•å»ºç«‹ä¸€ä¸ªå›¾ï¼Œå…¶ä¸­å¥å­æ˜¯èŠ‚ç‚¹ï¼Œé‡å çš„å•è¯æ˜¯é“¾æ¥ã€‚æœ€åï¼Œ *PageRank* ç¡®å®šè¿™ä¸ªå¥å­ç½‘ç»œä¸­æœ€é‡è¦çš„èŠ‚ç‚¹ã€‚**

**ä½¿ç”¨ [*gensim*](https://radimrehurek.com/gensim/) åº“ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°† *TextRank* ç®—æ³•åº”ç”¨äºæ‚¨çš„æ•°æ®:**

```
**'''
Summarizes corpus with TextRank.
:parameter    
    :param corpus: str or list - dtf["text"]    
    :param ratio: length of the summary (ex. 20% of the text)
:return    
    list of summaries
'''**
def **textrank**(corpus, ratio=0.2):    
    if type(corpus) is str:        
       corpus = [corpus]    
    lst_summaries = [**gensim**.summarization.summarize(txt,  
                     ratio=ratio) for txt in corpus]    
return lst_summaries **## Apply the function to corpus**
predicted = **textrank**(corpus=dtf_test["text"], ratio=0.2)
predicted[i]
```

**![](img/b3c4ac51446b69b076dbfc9d76560e61.png)**

**ä½œè€…å›¾ç‰‡**

**å¦‚ä½•æ‰èƒ½**è¯„ä»·**è¿™ä¸ªç»“æœå‘¢ï¼Ÿé€šå¸¸ï¼Œæˆ‘ç”¨ä¸¤ç§æ–¹å¼æ¥åš:**

1.  **[**ROUGE metrics**](https://en.wikipedia.org/wiki/ROUGE_(metric)#:~:text=ROUGE%2C%20or%20Recall%2DOriented%20Understudy,software%20in%20natural%20language%20processing.)**(é¢å‘å›å¿†çš„ Gisting è¯„ä¼°æ›¿è§’):
    é€šè¿‡é‡å  *n-grams* å°†è‡ªåŠ¨ç”Ÿæˆçš„æ‘˜è¦ä¸å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒçš„ä¸€ç»„åº¦é‡ã€‚****

```
****'''
Calculate ROUGE score.
:parameter    
    :param y_test: string or list    
    :param predicted: string or list
'''**
def **evaluate_summary**(y_test, predicted):    
   rouge_score = **rouge**.Rouge()    
   scores = rouge_score.get_scores(y_test, predicted, avg=True)       
   score_1 = round(scores['rouge-1']['f'], 2)    
   score_2 = round(scores['rouge-2']['f'], 2)    
   score_L = round(scores['rouge-l']['f'], 2)    
   print("rouge1:", score_1, "| rouge2:", score_2, "| rougeL:",
         score_2, "--> avg rouge:", round(np.mean(
         [score_1,score_2,score_L]), 2))**## Apply the function to predicted** i = 5
**evaluate_summary**(dtf_test["y"][i], predicted[i])**
```

****![](img/d72e781f769c0e603039f01b9834cc2d.png)****

****ä½œè€…å›¾ç‰‡****

****ç»“æœæ˜¾ç¤ºï¼Œ31%çš„ *unigrams* (ROUGE-1)å’Œ 7%çš„ *bigrams* (ROUGE-2)å‡ºç°åœ¨ä¸¤ä¸ªæ‘˜è¦ä¸­ï¼Œè€Œ*æœ€é•¿å…¬å…±å­åºåˆ—* (ROUGE-L)åŒ¹é…äº† 7%ã€‚æ€»çš„æ¥è¯´ï¼Œå¹³å‡åˆ† 20%ã€‚è¯·æ³¨æ„ï¼Œèƒ­è„‚åˆ†æ•°å¹¶ä¸è¡¡é‡æ€»ç»“çš„æµç•…ç¨‹åº¦ï¼Œå› ä¸ºæˆ‘é€šå¸¸ä½¿ç”¨å–„è‰¯çš„è€äººç±»çš„çœ¼ç›ã€‚****

****2.**å¯è§†åŒ–**:æ˜¾ç¤ºä¸¤ä¸ªæ–‡æœ¬ï¼Œå³æ‘˜è¦å’ŒåŸæ–‡ï¼Œæˆ–é¢„æµ‹æ‘˜è¦å’ŒçœŸå®æ‘˜è¦ï¼Œå¹¶çªå‡ºæ˜¾ç¤ºåŒ¹é…éƒ¨åˆ†ã€‚****

****æˆ‘æƒ³ä½ ä¼šå‘ç°è¿™ä¸ªåŠŸèƒ½éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨ç¬”è®°æœ¬ä¸Šçªå‡ºæ˜¾ç¤ºä¸¤ä¸ªæ–‡æœ¬çš„åŒ¹é…å­å­—ç¬¦ä¸²ã€‚å®ƒå¯ä»¥ç”¨äºå•è¯çº§:****

```
**match = **display_string_matching**(dtf_test["y"][i], predicted[i], both=True, **sentences=False**, titles=["Real Summary", "Predicted Summary"])from **IPython.core.display** import display, HTML
display(HTML(match))**
```

****![](img/bfe422cad015dca790d2d79408197c2e.png)****

****ä½œè€…å›¾ç‰‡****

****æˆ–è€…æ‚¨å¯ä»¥è®¾ç½® *sentences=True* ï¼Œå®ƒå°†åœ¨å¥å­çº§åˆ«è€Œä¸æ˜¯å•è¯çº§åˆ«åŒ¹é…æ–‡æœ¬:****

```
**match = **display_string_matching**(dtf_test["text"][i], predicted[i], both=True, **sentences=True**, titles=["Full Text", "Predicted Summary"])

from **IPython.core.display** import display, HTML
display(HTML(match))**
```

****![](img/1d119d45312c478f30e6b1e27d6d05fd.png)****

****ä½œè€…å›¾ç‰‡****

****è¯¥é¢„æµ‹å…·æœ‰åŸå§‹æ‘˜è¦ä¸­æåˆ°çš„å¤§éƒ¨åˆ†ä¿¡æ¯ã€‚æ­£å¦‚æå–ç®—æ³•æ‰€é¢„æœŸçš„ï¼Œé¢„æµ‹çš„æ‘˜è¦å®Œå…¨åŒ…å«åœ¨æ–‡æœ¬ä¸­:è¯¥æ¨¡å‹è®¤ä¸ºè¿™ 3 ä¸ªå¥å­æ˜¯æœ€é‡è¦çš„ã€‚æˆ‘ä»¬å¯ä»¥å°†æ­¤ä½œä¸ºä¸‹é¢æŠ½è±¡æ–¹æ³•çš„åŸºçº¿ã€‚****

## ****Seq2Seq****

****[åºåˆ—å¯¹åºåˆ—æ¨¡å‹](https://en.wikipedia.org/wiki/Seq2seq) (2014)æ˜¯ä»¥ç‰¹å®šé¢†åŸŸ(å³æ–‡æœ¬è¯æ±‡)çš„åºåˆ—ä¸ºè¾“å…¥ï¼Œè¾“å‡ºå¦ä¸€é¢†åŸŸ(å³æ‘˜è¦è¯æ±‡)çš„æ–°åºåˆ—çš„ç¥ç»ç½‘ç»œã€‚ *Seq2Seq* è½¦å‹é€šå¸¸å…·æœ‰ä»¥ä¸‹å…³é”®ç‰¹å¾:****

*   ******åºåˆ—ä½œä¸ºè¯­æ–™åº“**:å°†æ–‡æœ¬å¡«å……æˆé•¿åº¦ç›¸åŒçš„åºåˆ—ï¼Œå¾—åˆ°ç‰¹å¾çŸ©é˜µã€‚****
*   ******å•è¯åµŒå…¥** **æœºåˆ¶**:ç‰¹å¾å­¦ä¹ æŠ€æœ¯ï¼Œå°†è¯æ±‡è¡¨ä¸­çš„å•è¯æ˜ å°„åˆ°å®æ•°å‘é‡ï¼Œè¿™äº›å‘é‡æ˜¯æ ¹æ®æ¯ä¸ªå•è¯å‡ºç°åœ¨å¦ä¸€ä¸ªå•è¯ä¹‹å‰æˆ–ä¹‹åçš„æ¦‚ç‡åˆ†å¸ƒè®¡ç®—çš„ã€‚****
*   ******ç¼–ç å™¨-è§£ç å™¨ç»“æ„:**ç¼–ç å™¨å¤„ç†è¾“å…¥åºåˆ—ï¼Œå¹¶è¿”å›å…¶è‡ªèº«çš„å†…éƒ¨çŠ¶æ€ï¼Œä½œä¸ºè§£ç å™¨çš„ä¸Šä¸‹æ–‡ï¼Œè§£ç å™¨åœ¨ç»™å®šå‰ä¸€ä¸ªå­—çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹ç›®æ ‡åºåˆ—çš„ä¸‹ä¸€ä¸ªå­—ã€‚****
*   ******ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œç”¨äºé¢„æµ‹çš„æ¨¡å‹**:ç”¨äºè®­ç»ƒçš„æ¨¡å‹ä¸ç›´æ¥ç”¨äºé¢„æµ‹ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å°†ç¼–ç ä¸¤ä¸ªç¥ç»ç½‘ç»œ(éƒ½å…·æœ‰ç¼–ç å™¨-è§£ç å™¨ç»“æ„)ï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒï¼Œå¦ä¸€ä¸ª(ç§°ä¸ºâ€œæ¨ç†æ¨¡å‹â€)é€šè¿‡åˆ©ç”¨æ¥è‡ªè®­ç»ƒæ¨¡å‹çš„ä¸€äº›å±‚æ¥ç”Ÿæˆé¢„æµ‹ã€‚****

****è®©æˆ‘ä»¬ä»ä¸€äº›**æ•°æ®åˆ†æ**å¼€å§‹ï¼Œè¿™æ˜¯ä¸‹ä¸€ä¸ªç‰¹å¾å·¥ç¨‹æ‰€éœ€è¦çš„ã€‚ç”±äºæˆ‘ä»¬è¦å°†æ–‡æœ¬è½¬æ¢æˆå•è¯åºåˆ—ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œåšå‡ºä¸¤ä¸ªå†³å®š:****

1.  ****æ­£ç¡®çš„åºåˆ—å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¯­æ–™åº“æœ‰ä¸åŒçš„é•¿åº¦****
2.  ****æˆ‘ä»¬çš„æ¨¡å‹å¿…é¡»è®°ä½å¤šå°‘å•è¯ï¼Œå› ä¸ºç½•è§çš„å•è¯åº”è¯¥è¢«æ’é™¤åœ¨å¤–****

****æˆ‘å°†æ¸…ç†å’Œåˆ†ææ•°æ®æ¥è§£å†³è¿™ä¸¤ç‚¹ã€‚****

```
****## create stopwords**
lst_stopwords = **nltk**.corpus.stopwords.words("english")
**## add words that are too frequent**
lst_stopwords = lst_stopwords + ["cnn","say","said","new"] **## cleaning function**
def **utils_preprocess_text**(txt, punkt=True, lower=True, slang=True, lst_stopwords=None, stemm=False, lemm=True):
    **### separate sentences with '. '**
    txt = re.sub(r'\.(?=[^ \W\d])', '. ', str(txt))
    **### remove punctuations and characters**
    txt = re.sub(r'[^\w\s]', '', txt) if punkt is True else txt
    **### strip**
    txt = " ".join([word.strip() for word in txt.split()])
    **### lowercase**
    txt = txt.lower() if lower is True else txt
   ** ### slang**
    txt = contractions.fix(txt) if slang is True else txt   
    **### tokenize (convert from string to list)**
    lst_txt = txt.split()
    **### stemming (remove -ing, -ly, ...)**
    if stemm is True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_txt = [ps.stem(word) for word in lst_txt]
    **### lemmatization (convert the word into root word)**
    if lemm is True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_txt = [lem.lemmatize(word) for word in lst_txt]
    **### remove Stopwords**
    if lst_stopwords is not None:
        lst_txt = [word for word in lst_txt if word not in 
                   lst_stopwords]
    **### back to string**
    txt = " ".join(lst_txt)
    return txt **## apply function to both text and summaries** dtf_train["**text_clean**"] = dtf_train["text"].apply(lambda x: **utils_preprocess_text**(x, punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, stemm=False, lemm=True))dtf_train["**y_clean**"] = dtf_train["y"].apply(lambda x: **utils_preprocess_text**(x, punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, stemm=False, lemm=True))**
```

****ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»Ÿè®¡å•è¯æ¥çœ‹çœ‹é•¿åº¦åˆ†å¸ƒ:****

```
****## count**
dtf_train['**word_count**'] = dtf_train[column].apply(lambda x: len(**nltk**.word_tokenize(str(x))) )**## plot**
**sns**.distplot(dtf_train["**word_count**"], hist=True, kde=True, kde_kws={"shade":True})**
```

****![](img/7101d174ef2fa633cee54ab4549c59ad.png)****

****ä½œè€…çš„å›¾ç‰‡(å¯¹ X å’Œ y è¿è¡Œç›¸åŒçš„ä»£ç )****

```
**X_len = 400
y_len = 40**
```

****æˆ‘ä»¬æ¥åˆ†æä¸€ä¸‹è¯é¢‘:****

```
**lst_tokens = **nltk**.tokenize.word_tokenize(dtf_train["**text_clean**"].str.cat(sep=" "))
ngrams = [1]

**## calculate**
dtf_freq = pd.DataFrame()
for n in ngrams:
   dic_words_freq = nltk.FreqDist(**nltk**.ngrams(lst_tokens, n))
   dtf_n = pd.DataFrame(dic_words_freq.most_common(), columns=
                        ["word","freq"])
   dtf_n["ngrams"] = n
   dtf_freq = dtf_freq.append(dtf_n)
   dtf_freq["word"] = dtf_freq["word"].apply(lambda x: " 
                         ".join(string for string in x) )
   dtf_freq_X= dtf_freq.sort_values(["ngrams","freq"], ascending=
                         [True,False])

**## plot**
**sns**.barplot(x="freq", y="word", hue="ngrams", dodge=False,
 data=dtf_freq.groupby('ngrams')["ngrams","freq","word"].head(30))
plt.show()**
```

****![](img/681a61936d2ad1e617131ca57bdb6a75.png)****

****ä½œè€…çš„å›¾ç‰‡(å¯¹ X å’Œ y è¿è¡Œç›¸åŒçš„ä»£ç )****

```
**thres = 5 **#<-- min frequency**
X_top_words = len(dtf_freq_X[dtf_freq_X["freq"]>thres])
y_top_words = len(dtf_freq_y[dtf_freq_y["freq"]>thres])**
```

****ä¹‹åï¼Œæˆ‘ä»¬å°±æœ‰äº†ç»§ç»­è¿›è¡Œ**ç‰¹å¾å·¥ç¨‹**æ‰€éœ€çš„ä¸€åˆ‡ã€‚é€šè¿‡ä½¿ç”¨ *tensorflow/keras* å°†é¢„å¤„ç†çš„è¯­æ–™åº“è½¬æ¢æˆåºåˆ—åˆ—è¡¨æ¥åˆ›å»ºç‰¹å¾çŸ©é˜µ:****

```
**lst_corpus = dtf_train["text_clean"]**## tokenize text**
tokenizer = kprocessing.text.**Tokenizer**(num_words=**X_top_words**, lower=False, split=' ', oov_token=None, 
filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
tokenizer.fit_on_texts(lst_corpus)
dic_vocabulary = {"<PAD>":0}
dic_vocabulary.update(tokenizer.word_index)**## create sequence**
lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)**## padding sequence**
X_train = kprocessing.sequence.**pad_sequences**(lst_text2seq, 
                    maxlen=15, padding="post", truncating="post")**
```

****ç‰¹å¾çŸ©é˜µ *X_train* çš„å½¢çŠ¶ä¸º *N ä¸ªæ–‡æ¡£ X ä¸ªåºåˆ—æœ€å¤§é•¿åº¦*ã€‚è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹:****

```
****sns**.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)
plt.show()**
```

****![](img/3a5c8ce7bc5e9cc80e8db9e6252f0a48.png)****

****ä½œè€…å›¾ç‰‡( *N ä¸ªæ–‡æ¡£ x ä¸ªåºåˆ—æœ€å¤§é•¿åº¦)*****

****åœ¨ç»§ç»­ä¹‹å‰ï¼Œä¸è¦å¿˜è®°ä½¿ç”¨ fitted tokenizer å¯¹æµ‹è¯•é›†è¿›è¡ŒåŒæ ·çš„ç‰¹å¾å·¥ç¨‹:****

```
****## text to sequence with the fitted tokenizer**
lst_text2seq = **tokenizer**.texts_to_sequences(dtf_test["**text_clean**"])**## padding sequence**
X_test = kprocessing.sequence.**pad_sequences**(lst_text2seq, maxlen=15,
             padding="post", truncating="post")**
```

****ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹æ€»ç»“ã€‚åœ¨åº”ç”¨ç›¸åŒçš„ç‰¹å¾å·¥ç¨‹ç­–ç•¥ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ªæ‘˜è¦ä¸­æ·»åŠ ä¸¤ä¸ªç‰¹æ®Šçš„æ ‡è®°æ¥ç¡®å®šæ–‡æœ¬çš„å¼€å§‹å’Œç»“æŸã€‚****

```
****# Add START and END tokens to the summaries (y)**
special_tokens = ("<START>", "<END>")
dtf_train["y_clean"] = dtf_train['y_clean'].apply(lambda x: 
                     special_tokens[0]+' '+x+' '+special_tokens[1])
dtf_test["y_clean"] = dtf_test['y_clean'].apply(lambda x: 
                     special_tokens[0]+' '+x+' '+special_tokens[1])**# check example**
dtf_test["y_clean"][i]**
```

****![](img/61d679640b32f97071b02dbca71f99b4.png)****

****ä½œè€…å›¾ç‰‡****

****ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ©ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ä»£ç æ¥åˆ›å»ºå¸¦æœ‰æ‘˜è¦çš„ç‰¹å¾çŸ©é˜µ(å› æ­¤ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ ‡è®°å™¨ã€å¡«å……å™¨ï¼Œå¹¶ç”¨åˆé€‚çš„æ ‡è®°å™¨è½¬æ¢æµ‹è¯•é›†)ã€‚å¦‚æœä½ æ‰“å°å‡ºè¯æ±‡è¡¨ï¼Œä½ ä¼šåœ¨é¡¶éƒ¨çœ‹åˆ°ç‰¹æ®Šçš„è®°å·ã€‚ç¨åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*å¼€å§‹*æ ‡è®°å¼€å§‹é¢„æµ‹ï¼Œå½“*ç»“æŸ*æ ‡è®°å‡ºç°æ—¶ï¼Œé¢„æµ‹çš„æ–‡æœ¬å°†åœæ­¢ã€‚****

****![](img/e4b68f3f6ab75d5d9814876d276d8f67.png)****

****ä½œè€…å›¾ç‰‡****

****æˆ‘ä»¬å¯ä»¥ç»§ç»­è¿›è¡Œå•è¯åµŒå…¥ã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªé€‰æ‹©:ä»å¤´å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„å•è¯åµŒå…¥æ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚å¦‚æœä½ èµ°çš„æ˜¯åè€…ï¼Œé‚£å°±æŒ‰ç…§è¿™éƒ¨åˆ†èµ°ï¼Œå¦åˆ™å¯ä»¥è·³è¿‡ï¼Œç›´æ¥è·³åˆ°æ¨¡å‹è®¾è®¡ã€‚åœ¨ Python ä¸­ï¼Œä½ å¯ä»¥åƒè¿™æ ·ä»[*genism-data*](https://github.com/RaRe-Technologies/gensim-data)*åŠ è½½ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„å•è¯åµŒå…¥æ¨¡å‹:*****

```
***import **gensim_api**nlp = gensim_api.load(**"glove-wiki-gigaword-300"**)***
```

*****æˆ‘æ¨èæ–¯å¦ç¦å¤§å­¦çš„ [*GloVe*](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ç»´åŸºç™¾ç§‘ã€Gigaword å’Œ Twitter è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚æ‚¨å¯ä»¥é€šè¿‡å°†ä»»ä½•å•è¯è½¬æ¢ä¸ºå‘é‡æ¥æµ‹è¯•å®ƒ:*****

```
***word = "home"
nlp[word].shape**>>> (300,)*****
```

*****è¿™äº›å•è¯å‘é‡å¯ä»¥åœ¨ç¥ç»ç½‘ç»œä¸­ç”¨ä½œæƒé‡ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼Œä½¿å¾— id ä¸º *N* çš„å•è¯çš„å‘é‡ä½äºç¬¬*N*è¡Œã€‚*****

```
*****## start the matrix (length of vocabulary x vector size) with all 0s**
X_embeddings = np.zeros((len(X_dic_vocabulary)+1, 300))for word,idx in X_dic_vocabulary.items():
    **## update the row with vector**
    try:
        X_embeddings[idx] =  nlp[word]
    **## if word not in model then skip and the row stays all 0s**
    except:
        pass***
```

*****è¯¥ä»£ç ç”Ÿæˆä»è¯­æ–™åº“ x å‘é‡å¤§å°ä¸­æå–çš„è¯æ±‡çš„å½¢çŠ¶*é•¿åº¦çš„çŸ©é˜µ(300)ã€‚è¯­æ–™åº“çŸ©é˜µå°†ç”¨äºç¼–ç å™¨åµŒå…¥å±‚ï¼Œè€Œæ‘˜è¦çŸ©é˜µå°†ç”¨äºè§£ç å™¨åµŒå…¥å±‚ã€‚è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ª id å°†è¢«ç”¨ä½œè®¿é—®åµŒå…¥çŸ©é˜µçš„ç´¢å¼•ã€‚è¿™ä¸ªåµŒå…¥å±‚çš„è¾“å‡ºå°†æ˜¯ä¸€ä¸ª 2D çŸ©é˜µï¼Œå¯¹äºè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå•è¯ id æœ‰ä¸€ä¸ªå•è¯å‘é‡(åºåˆ—é•¿åº¦Ã—å‘é‡å¤§å°)ã€‚è®©æˆ‘ä»¬ä»¥å¥å­â€œæˆ‘å–œæ¬¢è¿™ç¯‡æ–‡ç« â€ä¸ºä¾‹:******

*****![](img/ad8b0d8547d5a69128b1550de808b145.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****ç»ˆäºåˆ°äº†æ„å»º**ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„æ—¶å€™äº†ã€‚**é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ¸…æ¥šä»€ä¹ˆæ˜¯æ­£ç¡®çš„è¾“å…¥å’Œè¾“å‡º:*****

*   *****è¾“å…¥æ˜¯ *X* (æ–‡æœ¬åºåˆ—)åŠ ä¸Š *y* (æ‘˜è¦åºåˆ—)ï¼Œæˆ‘ä»¬å°†éšè—æ‘˜è¦çš„æœ€åä¸€ä¸ªå•è¯*****
*   *****ç›®æ ‡åº”è¯¥æ˜¯æ²¡æœ‰*å¼€å§‹*æ ‡è®°çš„ *y* (æ¦‚è¦åºåˆ—)ã€‚*****

*****åŸºæœ¬ä¸Šï¼Œæ‚¨å°†è¾“å…¥æ–‡æœ¬äº¤ç»™ç¼–ç å™¨ä»¥ç†è§£ä¸Šä¸‹æ–‡ï¼Œç„¶åæ‚¨å‘è§£ç å™¨å±•ç¤ºæ‘˜è¦å¦‚ä½•å¼€å§‹ï¼Œæ¨¡å‹å­¦ä¹ é¢„æµ‹å®ƒå¦‚ä½•ç»“æŸã€‚è¿™æ˜¯ä¸€ç§ç§°ä¸ºâ€œæ•™å¸ˆå¼ºåˆ¶â€çš„è®­ç»ƒç­–ç•¥ï¼Œå®ƒä½¿ç”¨ç›®æ ‡è€Œä¸æ˜¯ç½‘ç»œç”Ÿæˆçš„è¾“å‡ºï¼Œä»¥ä¾¿å®ƒå¯ä»¥å­¦ä¹ é¢„æµ‹ *start* token ä¹‹åçš„å•è¯ï¼Œç„¶åæ˜¯ä¸‹ä¸€ä¸ªå•è¯ï¼Œä¾æ­¤ç±»æ¨(ä¸ºæ­¤ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨æ—¶é—´åˆ†å¸ƒå¯†é›†å±‚)ã€‚*****

*****![](img/39b470316a2f3ff8b5ba58bb81bf4255.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****æˆ‘å°†æå‡ºä¸¤ä¸ªä¸åŒç‰ˆæœ¬çš„ Seq2Seqã€‚ä¸‹é¢æ˜¯ä½ èƒ½å¾—åˆ°çš„æœ€ç®€å•çš„ç®—æ³•:*****

*   *****ä¸€ä¸ªåµŒå…¥å±‚ï¼Œå®ƒå°†ä»å¤´å¼€å§‹åˆ›å»ºä¸€ä¸ªå•è¯åµŒå…¥ï¼Œå°±åƒå‰é¢æè¿°çš„é‚£æ ·ã€‚*****
*   *****ä¸€ä¸ªå•å‘ *LSTM* å±‚ï¼Œè¿”å›ä¸€ä¸ªåºåˆ—ä»¥åŠå•å…ƒæ ¼çŠ¶æ€å’Œéšè—çŠ¶æ€ã€‚*****
*   *****æœ€ç»ˆçš„æ—¶é—´åˆ†å¸ƒå¯†é›†å±‚ï¼Œå®ƒå°†ç›¸åŒçš„å¯†é›†å±‚(ç›¸åŒçš„æƒé‡)åº”ç”¨äº *LSTM* è¾“å‡ºï¼Œæ¯æ¬¡ä¸€ä¸ªæ—¶é—´æ­¥é•¿ï¼Œè¿™æ ·è¾“å‡ºå±‚åªéœ€è¦ä¸€ä¸ªè¿æ¥åˆ°æ¯ä¸ª *LSTM* å•å…ƒã€‚*****

```
***lstm_units = 250
embeddings_size = 300 **##------------ ENCODER (embedding + lstm) ------------------------##** x_in = layers.Input(name="x_in", shape=(X_train.shape[1],))**### embedding**
layer_x_emb = layers.Embedding(name="x_emb", 
                               input_dim=len(X_dic_vocabulary),
                               output_dim=embeddings_size, 
                               trainable=True)
x_emb = layer_x_emb(x_in)**### lstm** 
layer_x_lstm = layers.LSTM(name="x_lstm", units=lstm_units, 
                           dropout=0.4, return_sequences=True, 
                           return_state=True)
x_out, state_h, state_c = layer_x_lstm(x_emb) **##------------ DECODER (embedding + lstm + dense) ----------------##** y_in = layers.Input(name="y_in", shape=(None,))**### embedding**
layer_y_emb = layers.Embedding(name="y_emb", 
                               input_dim=len(y_dic_vocabulary), 
                               output_dim=embeddings_size, 
                               trainable=True)
y_emb = layer_y_emb(y_in)**### lstm** 
layer_y_lstm = layers.LSTM(name="y_lstm", units=lstm_units, 
                           dropout=0.4, return_sequences=True, 
                           return_state=True)
y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])**### final dense layers**
layer_dense = layers.TimeDistributed(name="dense",          layer=layers.Dense(units=len(y_dic_vocabulary), activation='softmax'))
y_out = layer_dense(y_out) **##---------------------------- COMPILE ---------------------------##**
model = models.Model(inputs=[x_in, y_in], outputs=y_out, 
                     name="Seq2Seq")
model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])
model.summary()***
```

*****![](img/1ac086099bf92ec2dae28a1370d916c1.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****å¦‚æœè¿™å¯¹ä½ æ¥è¯´è¿˜ä¸å¤Ÿï¼Œä¸‹é¢æ˜¯ä¹‹å‰çš„ *Seq2Seq* ç®—æ³•çš„ä¸€ä¸ªé«˜çº§(å¹¶ä¸”éå¸¸é‡)ç‰ˆæœ¬:*****

*   *****åµŒå…¥å±‚ï¼Œåˆ©ç”¨æ¥è‡ª*æ‰‹å¥—*çš„é¢„è®­ç»ƒæƒé‡ã€‚*****
*   *****3 ä¸ªåŒå‘ *LSTM* å±‚ï¼Œåœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šå¤„ç†åºåˆ—ã€‚*****
*   *****æœ€ç»ˆæ—¶é—´åˆ†å¸ƒå¯†é›†å±‚(åŒå‰)ã€‚*****

```
***lstm_units = 250 **##-------- ENCODER (pre-trained embeddings + 3 bi-lstm) ----------##**
x_in = layers.Input(name="x_in", shape=(X_train.shape[1],))**### embedding**
layer_x_emb = layers.Embedding(name="x_emb",       
          input_dim=X_embeddings.shape[0], 
          output_dim=X_embeddings.shape[1], 
          weights=[X_embeddings], trainable=False)
x_emb = layer_x_emb(x_in)**### bi-lstm 1**
layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, 
                 dropout=0.2, return_sequences=True, 
                 return_state=True), name="x_lstm_1")
x_out, _, _, _, _ = layer_x_bilstm(x_emb)**### bi-lstm 2**
layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, 
                 dropout=0.2, return_sequences=True, 
                 return_state=True), name="x_lstm_2")
x_out, _, _, _, _ = layer_x_bilstm(x_out)**### bi-lstm 3 (here final states are collected)**
layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, 
                 dropout=0.2, return_sequences=True, 
                 return_state=True), name="x_lstm_3")
x_out, forward_h, forward_c, backward_h, backward_c = layer_x_bilstm(x_out)
state_h = layers.Concatenate()([forward_h, backward_h])
state_c = layers.Concatenate()([forward_c, backward_c]) **##------ DECODER (pre-trained embeddings + lstm + dense) ---------##**
y_in = layers.Input(name="y_in", shape=(None,))**### embedding**
layer_y_emb = layers.Embedding(name="y_emb", 
               input_dim=y_embeddings.shape[0], 
               output_dim=y_embeddings.shape[1], 
               weights=[y_embeddings], trainable=False)
y_emb = layer_y_emb(y_in)**### lstm**
layer_y_lstm = layers.LSTM(name="y_lstm", units=lstm_units*2, dropout=0.2, return_sequences=True, return_state=True)
y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])**### final dense layers**
layer_dense = layers.TimeDistributed(name="dense", 
              layer=layers.Dense(units=len(y_dic_vocabulary), 
               activation='softmax'))
y_out = layer_dense(y_out) **##---------------------- COMPILE ---------------------------------##** model = models.Model(inputs=[x_in, y_in], outputs=y_out, 
                     name="Seq2Seq")
model.compile(optimizer='rmsprop',   
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()***
```

*****![](img/704873ac66147122ac1a2db883c5bd36.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****åœ¨å®é™…æµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•ä¹‹å‰ï¼Œæˆ‘å°†ä¿ç•™ä¸€å°éƒ¨åˆ†è®­ç»ƒé›†è¿›è¡ŒéªŒè¯ã€‚*****

```
*****## train**
training = model.fit(x=[X_train, y_train[:,:-1]], 
                     y=y_train.reshape(y_train.shape[0], 
                                       y_train.shape[1], 
                                       1)[:,1:],
                     batch_size=128, 
                     epochs=100, 
                     shuffle=True, 
                     verbose=1, 
                     validation_split=0.3,
                     callbacks=[callbacks.**EarlyStopping**(
                                monitor='val_loss', 
                                mode='min', verbose=1, patience=2)]
                      )**## plot loss and accuracy**
metrics = [k for k in training.history.keys() if ("loss" not in k) and ("val" not in k)]
fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)ax[0].set(title="Training")
ax11 = ax[0].twinx()
ax[0].plot(training.history['loss'], color='black')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Loss', color='black')
for metric in metrics:
    ax11.plot(training.history[metric], label=metric)
ax11.set_ylabel("Score", color='steelblue')
ax11.legend()ax[1].set(title="Validation")
ax22 = ax[1].twinx()
ax[1].plot(training.history['val_loss'], color='black')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss', color='black')
for metric in metrics:
     ax22.plot(training.history['val_'+metric], label=metric)
ax22.set_ylabel("Score", color="steelblue")
plt.show()***
```

*****![](img/89b00f61340fe970ec25f58ef33e8016.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****è¯·æ³¨æ„ï¼Œæˆ‘åœ¨å›è°ƒä¸­ä½¿ç”¨äº† [*æå‰åœæ­¢*](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) å·¥å…·ï¼Œå½“å—ç›‘æ§çš„æŒ‡æ ‡(å³éªŒè¯æŸå¤±)åœæ­¢æ”¹å–„æ—¶ï¼Œè¯¥å·¥å…·åº”åœæ­¢è®­ç»ƒã€‚è¿™å¯¹èŠ‚çœè‡ªå·±çš„æ—¶é—´ç‰¹åˆ«æœ‰ç”¨ï¼Œå°¤å…¶æ˜¯åƒè¿™æ ·æ¼«é•¿è€Œç—›è‹¦çš„è®­ç»ƒã€‚æˆ‘æƒ³è¡¥å……çš„æ˜¯ï¼Œåœ¨ä¸åˆ©ç”¨ GPU çš„æƒ…å†µä¸‹è¿è¡Œ *Seq2Seq* ç®—æ³•æ˜¯éå¸¸å›°éš¾çš„ï¼Œå› ä¸ºä½ åŒæ—¶åœ¨è®­ç»ƒ 2 ä¸ªæ¨¡å‹(ç¼–ç å™¨-è§£ç å™¨)ã€‚ç”¨æœ‰[NVIDIA GPU](/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781)æˆ–è€… [Google Colab](https://colab.research.google.com/notebooks/gpu.ipynb) çš„ç”µè„‘ä¼šæ›´å¥½ã€‚*****

*****å³ä½¿è®­ç»ƒå®Œæˆäº†ï¼Œä¹Ÿè¿˜æ²¡ç»“æŸï¼ä¸ºäº†æµ‹è¯• *Seq2Seq* æ¨¡å‹ï¼Œä½œä¸ºæœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦æ„å»º**æ¨ç†æ¨¡å‹æ¥ç”Ÿæˆé¢„æµ‹ã€‚**é¢„æµ‹ç¼–ç å™¨å°†æ–°åºåˆ—( *X_test* )ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æœ€åä¸€ä¸ª LSTM å±‚çš„è¾“å‡ºåŠå…¶çŠ¶æ€ã€‚*****

```
*****# Prediction Encoder**
encoder_model = models.Model(inputs=x_in, outputs=[x_out, state_h, state_c], name="Prediction_Encoder")encoder_model.summary()***
```

*****![](img/bd9e1f4a98139c5643d078b53905528f.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****å¦ä¸€æ–¹é¢ï¼Œé¢„æµ‹è§£ç å™¨å°†*å¼€å§‹*æ ‡è®°ã€ç¼–ç å™¨çš„è¾“å‡ºåŠå…¶çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ–°çš„çŠ¶æ€ä»¥åŠè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ(æ¦‚ç‡æœ€é«˜çš„å•è¯å°†æ˜¯é¢„æµ‹)ã€‚*****

```
*****# Prediction Decoder****## double the lstm units if you used bidirectional lstm** lstm_units = lstm_units*2 if any("Bidirectional" in str(layer) for layer in model.layers) else lstm_units**## states of the previous time step** encoder_out = layers.Input(shape=(X_train.shape[1], lstm_units))
state_h, state_c = layers.Input(shape=(lstm_units,)), layers.Input(shape=(lstm_units,))**## decoder embeddings**
y_emb2 = layer_y_emb(y_in)**## lstm to predict the next word**
y_out2, state_h2, state_c2 = layer_y_lstm(y_emb2, initial_state=[state_h, state_c])**## softmax to generate probability distribution over the vocabulary**
probs = layer_dense(y_out2)**## compile**
decoder_model = models.Model(inputs=[y_in, encoder_out, state_h, state_c], outputs=[probs, state_h2, state_c2], name="Prediction_Decoder")decoder_model.summary()***
```

*****![](img/8b30e43df473e479e00dc94d917eb990.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****åœ¨åˆ©ç”¨*å¼€å§‹*ä»¤ç‰Œå’Œç¼–ç å™¨çŠ¶æ€è¿›è¡Œç¬¬ä¸€æ¬¡é¢„æµ‹ä¹‹åï¼Œè§£ç å™¨ä½¿ç”¨ç”Ÿæˆçš„å­—å’Œæ–°çŠ¶æ€æ¥é¢„æµ‹æ–°å­—å’Œæ–°çŠ¶æ€ã€‚è¯¥è¿­ä»£å°†ç»§ç»­è¿›è¡Œï¼Œç›´åˆ°æ¨¡å‹æœ€ç»ˆé¢„æµ‹åˆ°*ç»“æŸ*æ ‡è®°æˆ–è€…é¢„æµ‹çš„æ‘˜è¦è¾¾åˆ°å…¶æœ€å¤§é•¿åº¦ã€‚*****

*****![](img/9fbcf5f123019267a309376b1a5d031d.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****è®©æˆ‘ä»¬å¯¹ä¸Šè¿°å¾ªç¯è¿›è¡Œç¼–ç ï¼Œä»¥ç”Ÿæˆé¢„æµ‹å¹¶æµ‹è¯• *Seq2Seq* æ¨¡å‹:*****

```
*****# Predict** max_seq_lenght = X_test.shape[1]
predicted = []
for x in X_test:
   x = x.reshape(1,-1) **## encode X**
   encoder_out, state_h, state_c = **encoder_model**.predict(x) **## prepare loop**
   y_in = np.array([fitted_tokenizer.word_index[special_tokens[0]]])
   predicted_text = ""
   stop = False
   while not stop: **## predict dictionary probability distribution**
        probs, new_state_h, new_state_c = **decoder_model**.predict(
                          [y_in, encoder_out, state_h, state_c])

        **## get predicted word**
        voc_idx = np.argmax(probs[0,-1,:])
        pred_word = fitted_tokenizer.index_word[voc_idx]

        **## check stop**
        if (pred_word != special_tokens[1]) and 
           (len(predicted_text.split()) < max_seq_lenght):
            predicted_text = predicted_text +" "+ pred_word
        else:
            stop = True

       ** ## next**
        y_in = np.array([voc_idx])
        state_h, state_c = new_state_h, new_state_c predicted_text = predicted_text.replace(
                    special_tokens[0],"").strip()
   predicted.append(predicted_text)***
```

*****![](img/cd4c68034819a7e08ed70dd8cca9c2b1.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****![](img/58357028f6c52c2c83d325c05915077a.png)*****

*****ä½œè€…å›¾ç‰‡*****

*****è¯¥æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡å’Œå…³é”®ä¿¡æ¯ï¼Œä½†å®ƒå¯¹è¯æ±‡çš„é¢„æµ‹å¾ˆå·®ã€‚è¿™æ˜¯å› ä¸ºæˆ‘åœ¨è¿™ä¸ªå®éªŒçš„å®Œæ•´æ•°æ®é›†çš„ä¸€ä¸ªå°å­é›†ä¸Šè¿è¡Œäº† *Seq2Seq "* lite"ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªå¼ºå¤§çš„æœºå™¨ï¼Œä½ å¯ä»¥æ·»åŠ æ›´å¤šçš„æ•°æ®å’Œæé«˜æ€§èƒ½ã€‚*****

## *****å˜å½¢é‡‘åˆš(ç”µå½±å)*****

*****å˜å½¢é‡‘åˆšæ˜¯è°·æ­Œçš„è®ºæ–‡ [*æå‡ºçš„ä¸€ç§æ–°çš„å»ºæ¨¡æŠ€æœ¯*](https://arxiv.org/abs/1706.03762)*ã€2017ã€‘*åœ¨è¯¥è®ºæ–‡ä¸­ï¼Œå±•ç¤ºäº†é¡ºåºæ¨¡å‹(å¦‚ *LSTM* )å¯ä»¥å®Œå…¨è¢«æ³¨æ„åŠ›æœºåˆ¶å–ä»£ï¼Œç”šè‡³è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚è¿™äº›è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡åŒæ—¶å¤„ç†åºåˆ—å’Œæ˜ å°„å•è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»æ¥æ‰§è¡Œä»»ä½• NLP ä»»åŠ¡ï¼Œè€Œä¸ç®¡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­ç›¸è·å¤šè¿œã€‚å› æ­¤ï¼Œåœ¨ä»–ä»¬çš„å•è¯ä¸­åµŒå…¥åŒä¸€ä¸ªå•è¯å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡æœ‰ä¸åŒçš„å‘é‡ã€‚æœ€è‘—åçš„è¯­è¨€æ¨¡å‹æœ‰ Google çš„*[*BERT*](https://en.wikipedia.org/wiki/BERT_(language_model))å’Œ OpenAI çš„*[*GPT*](https://en.wikipedia.org/wiki/GPT-3)ï¼Œç”¨æ•°åäº¿ä¸ªå‚æ•°è¿›è¡Œè®­ç»ƒã€‚*********

******è„¸ä¹¦çš„ [***å·´ç‰¹***](https://huggingface.co/transformers/model_doc/bart.html) (åŒå‘è‡ªå›å½’å˜æ¢å™¨)ä½¿ç”¨æ ‡å‡†çš„ *Seq2Seq* åŒå‘ç¼–ç å™¨(åƒ*ä¼¯ç‰¹*)å’Œå·¦å³è‡ªå›å½’è§£ç å™¨(åƒ *GPT* )ã€‚åŸºæœ¬ä¸Šï¼Œ*å·´ç‰¹* = *ä¼¯ç‰¹* + *GPT* ã€‚******

******å˜å½¢é‡‘åˆšæ¨¡å‹çš„ä¸»åº“æ˜¯ [*å˜å½¢é‡‘åˆš*](https://huggingface.co/transformers/) é€šè¿‡[æ‹¥æŠ±é¢](https://huggingface.co/):******

```
******'''
Summarizes corpus with Bart.
:parameter    
   :param corpus: list - dtf["text"]    
   :param max_len: length of the summary
:return    
    list of summaries
'''**
def **bart**(corpus, max_len):    
    nlp = **transformers**.pipeline("summarization")    
    lst_summaries = [nlp(txt,               
                         max_length=max_len
                         )[0]["summary_text"].replace(" .", ".")                    
                     for txt in corpus]    
    return lst_summaries **## Apply the function to corpus** predicted = **bart**(corpus=dtf_test["text"], max_len=y_len)****
```

******![](img/e0c5525e2ec9fc2dcf18322337f0f65d.png)******

******ä½œè€…å›¾ç‰‡******

******![](img/a3d80217838fa73fe8df03707e880260.png)******

******ä½œè€…å›¾ç‰‡******

******é¢„æµ‹ç®€çŸ­ä½†æœ‰æ•ˆã€‚å¯¹äºå¤§å¤šæ•° NLP ä»»åŠ¡æ¥è¯´ï¼ŒTransformer æ¨¡å‹ä¼¼ä¹æ˜¯è¡¨ç°æœ€å¥½çš„ã€‚******

## ******ç»“è®º******

******è¿™ç¯‡æ–‡ç« æ˜¯æ¼”ç¤º**å¦‚ä½•å°†ä¸åŒçš„ NLP æ¨¡å‹åº”ç”¨åˆ°æ–‡æœ¬æ‘˜è¦ç”¨ä¾‹**çš„æ•™ç¨‹ã€‚æˆ‘æ¯”è¾ƒäº† 3 ç§æµè¡Œçš„æ–¹æ³•:æ— ç›‘ç£çš„ *TextRank* ï¼ŒåŸºäºå•è¯åµŒå…¥çš„ä¸¤ç§ä¸åŒç‰ˆæœ¬çš„æœ‰ç›‘ç£çš„ *Seq2Seq* ï¼Œä»¥åŠé¢„è®­ç»ƒçš„ *BART* ã€‚æˆ‘ç»å†äº†ç‰¹å¾å·¥ç¨‹ï¼Œæ¨¡å‹è®¾è®¡ï¼Œè¯„ä¼°å’Œå¯è§†åŒ–ã€‚******

******æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒï¼å¦‚æœ‰é—®é¢˜å’Œåé¦ˆï¼Œæˆ–è€…åªæ˜¯åˆ†äº«æ‚¨æ„Ÿå…´è¶£çš„é¡¹ç›®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚******

> ******ğŸ‘‰[æˆ‘ä»¬æ¥è¿çº¿](https://linktr.ee/maurodp)ğŸ‘ˆ******

> ******æœ¬æ–‡æ˜¯ç³»åˆ—æ–‡ç«  **NLP ä¸ Python** çš„ä¸€éƒ¨åˆ†ï¼Œå‚è§:******

******[](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) [## åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†ç±»:Tf-Idf vs Word2Vec vs BERT

### é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡ã€è¯„ä¼°ã€è¯è¢‹çš„å¯è§£é‡Šæ€§ã€è¯åµŒå…¥ã€è¯­è¨€æ¨¡å‹

towardsdatascience.com](/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) [](/text-analysis-feature-engineering-with-nlp-502d6ea9225d) [## ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡æœ¬åˆ†æå’Œç‰¹å¾å·¥ç¨‹

### è¯­è¨€æ£€æµ‹ï¼Œæ–‡æœ¬æ¸…ç†ï¼Œé•¿åº¦ï¼Œæƒ…æ„Ÿï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ŒN-grams é¢‘ç‡ï¼Œè¯å‘é‡ï¼Œä¸»é¢˜â€¦

towardsdatascience.com](/text-analysis-feature-engineering-with-nlp-502d6ea9225d) [](/text-classification-with-no-model-training-935fe0e42180) [## ç”¨äºæ— æ¨¡å‹è®­ç»ƒçš„æ–‡æœ¬åˆ†ç±»çš„ BERT

### å¦‚æœæ²¡æœ‰å¸¦æ ‡ç­¾çš„è®­ç»ƒé›†ï¼Œè¯·ä½¿ç”¨ BERTã€å•è¯åµŒå…¥å’Œå‘é‡ç›¸ä¼¼åº¦

towardsdatascience.com](/text-classification-with-no-model-training-935fe0e42180) [](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9) [## å¸¦ NLP çš„ AI èŠå¤©æœºå™¨äºº:è¯­éŸ³è¯†åˆ«+å˜å½¢é‡‘åˆš

### ç”¨ Python æ„å»ºä¸€ä¸ªä¼šè¯´è¯çš„èŠå¤©æœºå™¨äººï¼Œä¸ä½ çš„äººå·¥æ™ºèƒ½è¿›è¡Œå¯¹è¯

towardsdatascience.com](/ai-chatbot-with-nlp-speech-recognition-transformers-583716a299e9)******
# è®¿é—® 2022 å¹´çº½çº¦å¸‚çš„å‡ºç§Ÿè½¦æ•°æ®

> åŸæ–‡ï¼š<https://towardsdatascience.com/accessing-the-nyc-taxi-data-in-2022-8363adfda76a>

## å…³äºæœ€è¿‘çš„å˜åŒ–ä½ éœ€è¦çŸ¥é“çš„ä¸€åˆ‡

![](img/f0b61774eec19ae69ff890d8c7a536e9.png)

å›¾ç‰‡ç”± Benjamin Gremler é€šè¿‡ Unsplash æä¾›

æˆªè‡³ 2022 å¹´ 5 æœˆ 13 æ—¥ï¼Œå¯¹çº½çº¦å¸‚å‡ºç§Ÿè½¦æ•°æ®çš„è®¿é—®å·²å‘ç”Ÿå˜åŒ–ã€‚æ‹¼èŠ±ç°åœ¨å·²ç»æˆä¸ºæ–°çš„é»˜è®¤æ–‡ä»¶æ ¼å¼ï¼Œè€Œä¸æ˜¯ CSVã€‚å®é™…ä¸Šï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦æ›´æ”¹ä»£ç ä¸­çš„ä¸¤ä»¶äº‹:

1.  å°†è·¯å¾„æ›´æ”¹ä¸º S3 æ¡¶
2.  ä½¿ç”¨`dd.read_parquet()`æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä½ å¸¸ç”¨çš„`dd.read_csv()`æˆ–`pd.read_csv()`

è¿™ç¯‡æ–‡ç« æä¾›äº†å˜åŒ–çš„èƒŒæ™¯ï¼Œè§£é‡Šäº†æ‹¼èŠ±æ–‡ä»¶æ ¼å¼çš„å¥½å¤„ï¼Œå¹¶å±•ç¤ºäº†å¯¹çº½çº¦å¸‚ 11 å¹´å‡ºç§Ÿè½¦æ•°æ®çš„è¿è¡Œè®¡ç®—ã€‚

![](img/fdd09ab75535bad21e234415463ac6d1.png)

å›¾ç‰‡æ¥è‡ª[www1.nyc.gov](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

# ä½ å¤šå¹´æ¥ä¸€ç›´åœ¨åšçš„äº‹æƒ…

[NYC TLC æ•°æ®é›†](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)æ˜¯æœ€çŸ¥åçš„å…¬å…±æ•°æ®é›†ä¹‹ä¸€ã€‚è¿™æ˜¯ä¸ºæ•°ä¸å¤šçš„æ—¢å¤§(> 100GBs) *åˆç›¸å¯¹å¹²å‡€çš„å…¬å…±æ•°æ®é›†ä¹‹ä¸€ã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œè®¸å¤šå…¬å¸å°†å…¶ç”¨äºæ¼”ç¤ºå’Œå†…éƒ¨æµ‹è¯•ã€‚åå¤šå¹´æ¥ï¼Œæ•°æ®é›†ä¸€ç›´æ˜¯å¤§æ•°æ®é¢†åŸŸçš„ä¸€ä¸ªå¯é ç‰¹å¾ã€‚*

åˆ°äº†å‘¨æœ«ï¼Œè¿™ç§æƒ…å†µçªç„¶å‘ç”Ÿäº†å˜åŒ–ã€‚å¦‚æœä½ ç°åœ¨å°è¯•æ‹¨æ‰“ä½ ç†Ÿæ‚‰çš„`read_csv`ç”µè¯ï¼Œä½ ä¼šé‡åˆ°ä¸€ä¸ª`IndexError: list index out of range`:

```
# read in 2012 CSV data 
ddf = dd.read_csv("s3://nyc-tlc/trip data/yellow_tripdata_2012-*.csv")
```

è¿™å°†äº§ç”Ÿå¦‚ä¸‹æ¶ˆæ¯:

```
--------------------------------------------------------------------------- IndexError Traceback (most recent call last)
 Input In [21], in <cell line: 2>() ** 1** # read in 2012 CSV data ----> 2 ddf = dd.read_csv( **3** "s3://nyc-tlc/trip data/yellow_tripdata_2012-*.csv", **4** ) File ~/mambaforge/envs/dask-dataframes/lib/python3.9/site-packages/dask/dataframe/io/csv.py:741, in make_reader.<locals>.read(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs) **728** **def** read( **729** urlpath, **730** blocksize="default", (...) **739** **kwargs, **740** ): --> 741**return** read_pandas( **742** reader, **743** urlpath, **744** blocksize=blocksize, **745** lineterminator=lineterminator, **746** compression=compression, **747** sample=sample, **748** sample_rows=sample_rows, **749** enforce=enforce, **750** assume_missing=assume_missing, **751** storage_options=storage_options, **752** include_path_column=include_path_column, **753** **kwargs, **754** ) File ~/mambaforge/envs/dask-dataframes/lib/python3.9/site-packages/dask/dataframe/io/csv.py:520, in read_pandas(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs) **515** paths = get_fs_token_paths(urlpath, mode="rb", storage_options=storage_options)[ **516** 2 **517** ] **519** # Infer compression from first path --> 520 compression = infer_compression(paths[0]) **522** **if** blocksize == "default": **523** blocksize = AUTO_BLOCKSIZEIndexError: list index out of range
```

# æ¢æˆæ‹¼èŠ±åœ°æ¿

æ‹¼èŠ±åœ°æ¿å·²ç»æˆä¸ºçº½çº¦å¸‚ TLC æ•°æ®çš„æ–°é»˜è®¤å€¼ã€‚è¦è®¿é—®æ•°æ®ï¼Œæ‚¨éœ€è¦:

1.  å°† S3 å­˜å‚¨æ¡¶çš„è·¯å¾„æ”¹ä¸ºä½¿ç”¨æ–‡ä»¶æ‰©å±•å`.parquet`è€Œä¸æ˜¯`.csv`
2.  ä½¿ç”¨`dd.read_parquet()`æ–¹æ³•ä»£æ›¿`dd.read_csv()`

```
# read in 2012 Parquet data 
ddf = dd.read_parquet("s3://nyc-tlc/trip data/yellow_tripdata_2012-*.parquet") ddf.head()
```

![](img/71f8d05227ed96c0f12d1513c08091cf.png)

ä½œè€…æä¾›çš„å›¾ç‰‡(ä¸ºä¾¿äºé˜…è¯»ï¼Œå·²æˆªæ–­)

[**Dask**](https://dask.org/) **æ˜¯æŒ‰æ¯”ä¾‹è¯»å–çº½çº¦å¸‚æ–°å‡ºç§Ÿè½¦æ•°æ®çš„æœ€ä½³æ–¹å¼ã€‚Dask ä½¿æ‚¨èƒ½å¤Ÿæœ€å¤§é™åº¦åœ°æé«˜æ‹¼èŠ±æ–‡ä»¶æ ¼å¼çš„å¹¶è¡Œè¯»/å†™èƒ½åŠ›ã€‚**

ä½ ä¹Ÿå¯ä»¥é€šè¿‡`pd.read_parquet()`ä½¿ç”¨ pandasï¼Œä½†è¿™æ„å‘³ç€ä½ åªèƒ½ä½¿ç”¨ä¸€ä¸ª CPU å†…æ ¸æ¥å¤„ç†æ•°æ®ã€‚è¿™å°†ä½¿ä½ çš„å·¥ä½œæµç¨‹å˜å¾—æ›´æ…¢ï¼Œå¯æ‰©å±•æ€§æ›´å·®ã€‚

é˜…è¯»è¿™ç¯‡æ–‡ç« äº†è§£æ›´å¤šå…³äº Dask å¦‚ä½•å¸®åŠ©ä½ åŠ é€Ÿæ•°æ®åˆ†æçš„ä¿¡æ¯ã€‚

# æ—§ä¹ éš¾æ”¹

å¦‚æœä½ çœŸçš„ï¼ŒçœŸçš„ï¼ŒçœŸçš„æƒ³ä½¿ç”¨ä¸€ä¸ªè¾ƒæ…¢çš„å¹¶è¡Œ I/Oï¼Œè¾ƒå°‘çš„å‹ç¼©é€‰é¡¹ï¼Œæ²¡æœ‰åˆ—ä¿®å‰ªæˆ–è°“è¯ä¸‹æ¨çš„åŠ£è´¨æ–‡ä»¶æ ¼å¼ğŸ˜‰æ‚¨ä»ç„¶å¯ä»¥è®¿é—®`csv_backup`ç›®å½•ä¸­çš„ CSV æ•°æ®:

```
# read in 2012 CSV data 
ddf = dd.read_csv("s3://nyc-tlc/csv_backup/yellow_tripdata_2012-*.csv")
```

è¯·æ³¨æ„ï¼Œä¸æ–°çš„ Parquet æ–‡ä»¶ç›¸æ¯”ï¼Œè¿™äº› CSV æ–‡ä»¶çš„å¹¶è¡Œ I/O é€Ÿåº¦è¾ƒæ…¢ï¼Œå‹ç¼©é€‰é¡¹è¾ƒå°‘ï¼Œå¹¶ä¸”æ²¡æœ‰åˆ—ä¿®å‰ªæˆ–è°“è¯ä¸‹æ¨ã€‚å¦‚æœæ‚¨æ­£åœ¨å¤§è§„æ¨¡å·¥ä½œï¼Œé™¤éæ‚¨æœ‰éå¸¸å……åˆ†çš„ç†ç”±ä½¿ç”¨ CSVï¼Œå¦åˆ™æ‚¨é€šå¸¸åº”è¯¥ä½¿ç”¨æ‹¼èŠ±åœ°æ¿è€Œä¸æ˜¯ CSVã€‚[é˜…è¯»è¿™ä¸ªåšå®¢](https://coiled.io/blog/writing-parquet-files-with-dask-using-to_parquet/)ï¼Œäº†è§£æ›´å¤šå…³äºå¦‚ä½•ç”¨ Dask ç¼–å†™æ‹¼èŠ±æ–‡ä»¶çš„ä¿¡æ¯ã€‚

# è¿˜æœ‰æ›´å¥½çš„

NYC TLC æ‹¼èŠ±æ–‡ä»¶çš„å”¯ä¸€ç¼ºç‚¹æ˜¯ä¸‹è½½è¿™äº›æ‹¼èŠ±æ–‡ä»¶éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œå› ä¸ºæ¯å¹´æœ‰ 12 ä¸ªéå¸¸å¤§çš„åˆ†åŒºã€‚ä¸ºäº†å¹¶è¡Œ IO å’Œæ›´å¿«çš„è®¡ç®—ï¼Œæœ€å¥½å°†æ•°æ®é›†é‡æ–°åˆ†åŒºåˆ°æ›´ä¼˜åŒ–çš„å¤§å°ã€‚

```
ddf = ddf.repartition(partition_size="100MB")
```

ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¯¹ 2011 å¹´åˆ° 2021 å¹´çš„ NYC TLC æ•°æ®æ‰§è¡Œ groupby è®¡ç®—ã€‚è¿™æ˜¯ç£ç›˜ä¸Šè¶…è¿‡ 200GB çš„æœªå‹ç¼©æ•°æ®ã€‚æ‚¨çš„æœ¬åœ°æœºå™¨ä¸å¤ªå¯èƒ½æœ‰è¿è¡Œè¯¥åˆ†æçš„å†…å­˜ã€‚æˆ‘ä»¬å°†åœ¨ä¸€ä¸ªæœ‰ 50 ä¸ªå·¥äººå’Œ 16GB å†…å­˜çš„å·çŠ¶é›†ç¾¤ä¸Šè¿è¡Œæˆ‘ä»¬çš„è®¡ç®—ã€‚[é˜…è¯»æ–‡æ¡£](https://docs.coiled.io/user_guide/getting_started.html)å¼€å§‹ä½¿ç”¨ Coiledã€‚

```
from coiled import Cluster 
from distributed import Client 
import dask.dataframe as dd # launch Coiled cluster 
cluster = Cluster( 
    name="dataframes", 
    n_workers=50, 
    worker_memory="16GiB", 
    software="coiled-examples/dask-dataframes",
) # connect Dask to Coiled 
client = Client(cluster)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥åŠ è½½æˆ‘ä»¬çš„æ•°æ®é›†:

```
# read in all data for 2011-2021 
ddf = dd.read_parquet("s3://nyc-tlc/trip data/yellow_tripdata_2011-*.parquet") for i in range(2012,2022): 
   ddf_temp = dd.read_parquet(f"s3://nyc-tlc/trip data/yellow_tripdata_{i}-*.parquet") 
    ddf = ddf.append(ddf_temp) # repartition dataset 
ddf = ddf.repartition(partition_size="100MB").persist()
```

ç°åœ¨é€šè¿‡è®¡ç®—æ¥è¿è¡Œæˆ‘ä»¬çš„ç»„:

```
%%time 
# perform groupby aggregation ddf.groupby('passenger_count').trip_distance.mean().compute()CPU times: user 526 ms, sys: 55.1 ms, total: 582 ms 
Wall time: 10.3 s passenger_count
49.0      0.000000
208.0     0.241961
10.0      0.386429
19.0      0.690000
211.0     0.970000
192.0     1.010000
254.0     1.020000
223.0     1.160000
96.0      1.195000
177.0     1.340000
33.0      1.615000
249.0     1.690000
193.0     1.740000
112.0     1.800000
97.0      1.870000
129.0     2.050000
37.0      2.160000
0.0       2.522421
47.0      2.560000
15.0      2.590000
255.0     2.953333
6.0       2.975480
5.0       3.001735
70.0      3.060000
7.0       3.288784
247.0     3.310000
58.0      3.460000
225.0     4.830000
8.0       4.950078
250.0     5.010000
4.0       5.028690
9.0       5.675410
2.0       5.869093
3.0       5.931338
1.0       6.567514
61.0      8.780000
65.0     18.520000
36.0     20.160000
Name: trip_distance, dtype: float64
```

# æ‹¼èŠ±åœ°æ¿æ˜¯ä½ çš„æ–°æœ‹å‹

æ˜¯çš„ï¼Œè¿™ç§è®¿é—®æ›´æ”¹æ˜¯ç—›è‹¦çš„ï¼Œå¹¶ä¸”æ„å‘³ç€æ‚¨å¯èƒ½å¿…é¡»æ›´æ–°ä¸€äº›é—ç•™ä»£ç ã€‚ä½†è¿™ç§æ”¹å˜æ˜¯æœ‰å……åˆ†ç†ç”±çš„:Parquet æ˜¯ä¸€ç§æ›´æœ‰æ•ˆçš„æ–‡ä»¶æ ¼å¼ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¿™ç§è§„æ¨¡çš„æ•°æ®é›†æ—¶ï¼Œæœ€å¥½ä»¥å¹¶è¡Œæ–¹å¼è¯»å–ã€‚Parquet ä½¿æ‚¨èƒ½å¤Ÿæ‰§è¡ŒèŠ‚çœæ—¶é—´çš„æ“ä½œï¼Œæ¯”å¦‚å¹¶è¡Œ IOã€åˆ—ä¿®å‰ªå’Œè°“è¯ä¸‹æ¨ã€‚

![](img/f8235beb87a8ca791c929e5a8049760b.png)

[é˜…è¯»æˆ‘ä»¬å…³äºä½¿ç”¨ Parquet ä¼˜äº CSV æˆ– JSON çš„æ‰€æœ‰ä¼˜åŠ¿çš„å¸–å­](https://coiled.io/blog/parquet-file-column-pruning-predicate-pushdown/),äº†è§£æ›´å¤šä¿¡æ¯å¹¶æ›´è¯¦ç»†åœ°æ£€æŸ¥ä¸Šè¿°åŸºå‡†ã€‚

*åŸè½½äº 2022 å¹´ 5 æœˆ 17 æ—¥*[*https://coiled . io*](https://coiled.io/blog/nyc-taxi-parquet-csv-index-error/)*ã€‚*
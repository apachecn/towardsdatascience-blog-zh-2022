# ä½¿ç”¨ Python è¿›è¡Œä¸»é¢˜å»ºæ¨¡å®è·µ

> åŸæ–‡ï¼š<https://towardsdatascience.com/hands-on-topic-modeling-with-python-1e3466d406d7>

## ä½¿ç”¨æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…(LDA)å’Œ pyLDAvis å¯è§†åŒ–çš„ä¸»é¢˜å»ºæ¨¡æ•™ç¨‹

![](img/aacbc6a747e3a17116e9308986030676.png)

[å¸ƒæ‹‰å¾·åˆ©Â·è¾›æ ¼å°”é¡¿](https://unsplash.com/@bradleysingleton?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) æ‹æ‘„çš„ç…§ç‰‡

ä¸»é¢˜å»ºæ¨¡æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ–‡æœ¬æŒ–æ˜ä¸­æå–ç»™å®šæ–‡æœ¬ä¸»é¢˜çš„ä¸€ç§æµè¡ŒæŠ€æœ¯ã€‚åˆ©ç”¨ä¸»é¢˜å»ºæ¨¡ï¼Œæˆ‘ä»¬å¯ä»¥æ‰«æå¤§é‡çš„éç»“æ„åŒ–æ–‡æœ¬ï¼Œä»¥æ£€æµ‹å…³é”®å­—ã€ä¸»é¢˜å’Œä¸»é¢˜ã€‚

ä¸»é¢˜å»ºæ¨¡æ˜¯ä¸€ç§æ— ç›‘ç£çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä¸éœ€è¦æ ‡è®°æ•°æ®æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚å®ƒä¸åº”è¯¥ä¸*ä¸»é¢˜åˆ†ç±»*æ··æ·†ï¼Œåè€…æ˜¯ä¸€ç§å—ç›‘ç£çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œéœ€è¦æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒä»¥é€‚åº”å’Œå­¦ä¹ ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸»é¢˜å»ºæ¨¡å¯ä»¥ä¸ä¸»é¢˜åˆ†ç±»ä¸€èµ·ä½¿ç”¨ï¼Œå…¶ä¸­æˆ‘ä»¬é¦–å…ˆæ‰§è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œä»¥æ£€æµ‹ç»™å®šæ–‡æœ¬ä¸­çš„ä¸»é¢˜ï¼Œå¹¶ç”¨ç›¸åº”çš„ä¸»é¢˜æ ‡è®°æ¯ä¸ªè®°å½•ã€‚ç„¶åï¼Œè¿™ä¸ªæ ‡è®°çš„æ•°æ®ç”¨äºè®­ç»ƒåˆ†ç±»å™¨ï¼Œå¹¶å¯¹çœ‹ä¸è§çš„æ•°æ®æ‰§è¡Œä¸»é¢˜åˆ†ç±»ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ä¸»é¢˜å»ºæ¨¡ï¼Œå¹¶ä»‹ç»å¦‚ä½•é€šè¿‡æ–‡æœ¬é¢„å¤„ç†å‡†å¤‡æ•°æ®ï¼Œé€šè¿‡ coherence score åˆ†é…æœ€ä½³æ•°é‡çš„ä¸»é¢˜ï¼Œä½¿ç”¨æ½œåœ¨ Dirichlet åˆ†é…(LDA)æå–ä¸»é¢˜ï¼Œä»¥åŠä½¿ç”¨ pyLDAvis å¯è§†åŒ–ä¸»é¢˜ã€‚

åœ¨é˜…è¯»æœ¬æ–‡çš„åŒæ—¶ï¼Œæˆ‘é¼“åŠ±æ‚¨æŸ¥çœ‹æˆ‘çš„ GitHub ä¸Šçš„ [Jupyter ç¬”è®°æœ¬](https://github.com/Idilismiguzel/NLP-with-Python/blob/master/Topic%20Modeling/Disneyland_Reviews_Topic_Modeling_LDA.ipynb)ä»¥è·å¾—å®Œæ•´çš„åˆ†æå’Œä»£ç ã€‚

æˆ‘ä»¬æœ‰å¾ˆå¤šäº‹æƒ…è¦è°ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼ğŸ¤“

# 1.æ•°æ®

æˆ‘ä»¬å°†ä½¿ç”¨å¯ä»¥ä» Kaggle ä¸‹è½½çš„[è¿ªå£«å°¼ä¹å›­è¯„è®ºæ•°æ®é›†](https://www.kaggle.com/datasets/arushchillar/disneyland-reviews)ã€‚å®ƒå¯¹å·´é»ã€åŠ å·å’Œé¦™æ¸¯çš„è¿ªå£«å°¼ä¹å›­åˆ†åº—æœ‰ 42ï¼Œ000 æ¡è¯„è®ºå’Œè¯„çº§ã€‚è¯„çº§æ åŒ…æ‹¬è¯„çº§åˆ†æ•°ï¼Œå¯ç”¨äºä¸»é¢˜åˆ†ç±»ï¼Œå°†æœªæŸ¥çœ‹çš„è¯„è®ºåˆ†ä¸ºæ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§ã€‚è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ï¼Œä½†æ˜¯å¦‚æœä½ å¯¹ä¸»é¢˜åˆ†ç±»æ„Ÿå…´è¶£ï¼Œä½ å¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„æ–‡ç« ã€‚

<https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640>  

æˆ‘ä»¬æ¥è¯»ä¸€ä¸‹æ•°æ®ï¼Œçœ‹çœ‹å‰å‡ è¡Œã€‚

```
# Read the data
reviews = pd.read_csv('/content/DisneylandReviews.csv', encoding='latin-1')

# Remove missing values
reviews = reviews.dropna()
```

![](img/cec9cb8e0d0f1bf757b632da2e04c4dd.png)

æ•°æ®é›†çš„å‰ 5 è¡Œ

è®©æˆ‘ä»¬åªè¿‡æ»¤â€œè¯„è®ºâ€å’Œâ€œè¯„çº§â€åˆ—ã€‚

```
# Filter only related columns and drop duplicated reviews
reviews = reviews[["Review_Text", "Rating"]]
reviews = reviews.drop_duplicates(subset='Review_Text')
```

è®©æˆ‘ä»¬ä½¿ç”¨ seaborn çš„`countplot`æ‰“å°ä¸€ä¸ªä»·å€¼è®¡æ•°æŸ±çŠ¶å›¾ï¼Œä»¥äº†è§£è¯„è®ºçš„æ€»ä½“æƒ…ç»ªã€‚

```
# Create a bar plot with value counts
sns.countplot(x='Rating', data=reviews)
```

![](img/f80dc0c34c27338e7143ba0afa758b05.png)

å¤§å¤šæ•°æ˜¯æ­£é¢çš„ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›è´Ÿé¢çš„è¯„ä»·

# 2.æ•°æ®æ¸…ç†å’Œé¢„å¤„ç†

åœ¨å¼€å§‹ä¸»é¢˜å»ºæ¨¡ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡æ–‡æœ¬å¹¶è¿›è¡Œæ¸…æ´—å’Œé¢„å¤„ç†ã€‚è¿™æ˜¯æ‰€æœ‰æ–‡æœ¬æŒ–æ˜ç®¡é“ä¸­è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼Œæœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºæ­¤ã€‚å¯¹äºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†éµå¾ªä»¥ä¸‹æ­¥éª¤:

1.  **å°å†™**æ¯ä¸ªå•è¯
2.  **å°†ç¼©å†™**æ›¿æ¢ä¸ºæ›´é•¿çš„å½¢å¼
3.  **åˆ é™¤ç‰¹æ®Šå­—ç¬¦å’Œä¸éœ€è¦çš„å•è¯**
4.  ä½¿ç”¨`nltk.WordPunctTokenizer()`å¯¹æ¯ä¸ªå•è¯è¿›è¡Œæ ‡è®°åŒ–æˆ‘ä»¬å°†ä»å•è¯æˆ–å¥å­çš„å­—ç¬¦ä¸²ä¸­æå–æ ‡è®°ã€‚
5.  **ä½¿ç”¨`nltk.stem.WordNetLemmatizer()`å¯¹æ¯ä¸ªå•è¯è¿›è¡Œè¯æ¡æ•´ç†ï¼Œæˆ‘ä»¬å°†æŠŠå•è¯æ¢å¤åˆ°å®ƒä»¬çš„å­—å…¸å½¢å¼ï¼Œè¿™æ ·æ‰€æœ‰å…·æœ‰ç›¸ä¼¼æ„æ€çš„å•è¯å°†è¢«é“¾æ¥æˆä¸€ä¸ªå•è¯ã€‚**

ä¸ºäº†åº”ç”¨æ‰€æœ‰åˆ—å‡ºçš„æ­¥éª¤ï¼Œæˆ‘å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ã€‚ç„¶è€Œï¼Œä¸ºäº†å¢åŠ æ¨¡å—åŒ–å’Œæ˜“äºè°ƒè¯•ï¼Œæ‚¨å¯ä»¥åœ¨å•ç‹¬çš„å‡½æ•°ä¸­å®šä¹‰æ¯ä¸ªä»»åŠ¡ã€‚

```
def text_preprocessing(text):

    # Convert words to lower case
    text = text.lower()

    # Expand contractions
    if True:
        text = text.split()
        new_text = []
        for word in text:
            if word in contractions:
                new_text.append(contractions[word])
            else:
                new_text.append(word)
        text = " ".join(new_text)

    # Format words and remove unwanted characters
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\<a href', ' ', text)
    text = re.sub(r'&amp;', '', text) 
    text = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', text)
    text = re.sub(r'<br />', ' ', text)
    text = re.sub(r'\'', ' ', text) 

    # Tokenize each word
    text = nltk.WordPunctTokenizer().tokenize(text)

    # Lemmatize each word
    text = [nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v') for token in text if len(token)>1]

return text
```

```
def to_string(text):
    # Convert list to string
    text = ' '.join(map(str, text))

    return text

# Create a list of review by applying text_preprocessing function
reviews['Review_Clean_List'] = list(map(text_preprocessing, reviews.Review_Text))

# Return to string with to_string function
reviews['Review_Clean'] = list(map(to_string, reviews['Review_Clean_List']))
```

è®©æˆ‘ä»¬é€šè¿‡æ‰“å°ä¸€ä¸ªéšæœºçš„è¡Œæ¥çœ‹çœ‹æ–°çš„åˆ—ã€‚

![](img/0ec1e318966bc692cfec7bc76bac5281.png)

æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œåœ¨è¿›å…¥ä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åˆ é™¤åœç”¨è¯ã€‚åœç”¨è¯æ˜¯ç‰¹å®šäºè¯­è¨€çš„å¸¸ç”¨è¯(å³è‹±è¯­ä¸­çš„â€œtheâ€ã€â€œaâ€å’Œâ€œanâ€)ï¼Œæ—¢ä¸ä¼šå¢åŠ ä»·å€¼ï¼Œä¹Ÿä¸ä¼šæ”¹å–„å¯¹ç»¼è¿°çš„è§£é‡Šï¼Œå¹¶ä¸”å¾€å¾€ä¼šåœ¨å»ºæ¨¡ä¸­å¼•å…¥åå·®ã€‚æˆ‘ä»¬å°†ä»`nltk`åº“ä¸­åŠ è½½è‹±è¯­åœç”¨è¯è¡¨ï¼Œå¹¶ä»æˆ‘ä»¬çš„è¯­æ–™åº“ä¸­åˆ é™¤è¿™äº›å•è¯ã€‚

å› ä¸ºæˆ‘ä»¬è¦åˆ é™¤åœç”¨è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯èƒ½è¦æ£€æŸ¥è¯­æ–™åº“ä¸­æœ€å¸¸ç”¨çš„è¯ï¼Œå¹¶è¯„ä¼°æˆ‘ä»¬æ˜¯å¦ä¹Ÿè¦åˆ é™¤å…¶ä¸­çš„ä¸€äº›è¯ã€‚è¿™äº›å•è¯ä¸­çš„ä¸€äº›å¯èƒ½åªæ˜¯ç»å¸¸é‡å¤ï¼Œå¹¶æ²¡æœ‰ç»™æ„æ€å¢åŠ ä»»ä½•ä»·å€¼ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨è—ä¹¦åº“ä¸­çš„`Counter`æ¥ç»Ÿè®¡å­—æ•°ã€‚

```
# Import Counter 
from collections import Counter

# Join all word corpus
review_words = ','.join(list(reviews['Review_Clean'].values))

# Count and find the 30 most frequent
Counter = Counter(review_words.split())
most_frequent = Counter.most_common(30)

# Bar plot of frequent words
fig = plt.figure(1, figsize = (20,10))
_ = pd.DataFrame(most_frequent, columns=("words","count"))
sns.barplot(x = 'words', y = 'count', data = _, palette = 'winter')
plt.xticks(rotation=45);
```

![](img/fd9b6d363b22426e7ff59ca3d61bb213.png)

30 ä¸ªæœ€å¸¸ç”¨çš„å•è¯(åˆ é™¤åœç”¨è¯ä¹‹å‰)

ä¸å‡ºæ‰€æ–™ï¼Œå‰ 30 åä¸­ç»å¸¸å‡ºç°ä¸è¿ªå£«å°¼å’Œå…¬å›­å†…å®¹ç›¸å…³çš„è¯ï¼Œå¦‚â€œå…¬å›­â€ã€â€œè¿ªå£«å°¼â€å’Œâ€œè¿ªå£«å°¼ä¹å›­â€ã€‚æˆ‘ä»¬å°†é€šè¿‡å°†è¿™äº›è¯æ·»åŠ åˆ°åœç”¨è¯åˆ—è¡¨ä¸­æ¥åˆ é™¤å®ƒä»¬ã€‚æ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„åˆ—è¡¨ã€‚

```
# Load the list of stopwords
nltk.download('stopwords')

stopwords_list = stopwords.words('english')
stopwords_list.extend(['park', 'disney', 'disneyland'])

reviews['Review_Clean_List'] = [[word for word in line if word not in stopwords_list] for line in reviews['Review_Clean_List']]
reviews['Review_Clean'] = list(map(text_as_string, reviews['Review_Clean_List']))

# Join all word corpus
review_words = ','.join(list(reviews['Review_Clean'].values))

# Count and find the 30 most frequent
Counter = Counter(review_words.split())
most_frequent = Counter.most_common(30)

# Bar plot of frequent words
fig = plt.figure(1, figsize = (20,10))
_ = pd.DataFrame(most_frequent, columns=("words","count"))
sns.barplot(x = 'words', y = 'count', data = _, palette = 'winter')
plt.xticks(rotation=45);
```

![](img/cd92ad59ff13323e8148b0d57af5123d.png)

30 ä¸ªæœ€å¸¸ç”¨çš„å•è¯(åˆ é™¤åœç”¨è¯å’Œä¸€äº›å¸¸ç”¨è¯å)

# å¥–é‡‘

è®©æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„`review_words`åˆ›å»ºé¢„å¤„ç†æ–‡æœ¬è¯­æ–™åº“çš„è¯äº‘ã€‚â˜ï¸ï¸ ï¸ï¸â˜ï¸ â˜ï¸

```
# Generate the word cloud
wordcloud = WordCloud(background_color="white",
                      max_words= 200,
                      contour_width = 8,
                      contour_color = "steelblue",
                      collocations=False).generate(review_words)

# Visualize the word cloud
fig = plt.figure(1, figsize = (10, 10))
plt.axis('off')
plt.imshow(wordcloud)
plt.show()
```

![](img/aef13e6293b7e508683d2cd838cceb6d.png)

æ–‡æœ¬é¢„å¤„ç†åçš„ Wordcloud

# 3.è¯æ±‡è¢‹

ä¸ºäº†ä½¿ç”¨æ–‡æœ¬ä½œä¸ºæœºå™¨å­¦ä¹ ç®—æ³•çš„è¾“å…¥ï¼Œæˆ‘ä»¬éœ€è¦ä»¥æ•°å­—æ ¼å¼å‘ˆç°å®ƒã€‚å•è¯è¢‹æ˜¯ä¸€ä¸ª[å‘é‡ç©ºé—´æ¨¡å‹](https://en.wikipedia.org/wiki/Vector_space_model)ï¼Œè¡¨ç¤ºå•è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°ã€‚æ¢å¥è¯è¯´ï¼Œå•è¯è¢‹å°†æ¯ä¸ªè¯„è®ºè½¬æ¢æˆå•è¯è®¡æ•°çš„é›†åˆï¼Œè€Œä¸è€ƒè™‘é¡ºåºæˆ–æ„ä¹‰çš„é‡è¦æ€§ã€‚

æˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ Gensim çš„`corpora.Dictionary`åˆ›å»ºå­—å…¸ï¼Œç„¶åä½¿ç”¨`dictionary.doc2bow`åˆ›å»ºå•è¯åŒ…ã€‚

```
# Create Dictionary
id2word = gensim.corpora.Dictionary(reviews['Review_Clean_List'])

# Create Corpus: Term Document Frequency
corpus = [id2word.doc2bow(text) for text in reviews['Review_Clean_List']]
```

é€šè¿‡åˆ›å»ºå­—å…¸ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªæ•´æ•° id(åˆå id2word)æ˜ å°„æ¯ä¸ªå•è¯ï¼Œç„¶åæˆ‘ä»¬åœ¨æ¯ä¸ªå­—å…¸ä¸Šè°ƒç”¨ doc2bow å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ª(idï¼Œfrequency)å…ƒç»„åˆ—è¡¨ã€‚

# 4.ç¡®å®šä¸»é¢˜çš„æ•°é‡

å†³å®šä¸»é¢˜å»ºæ¨¡çš„ä¸»é¢˜æ•°é‡å¯èƒ½å¾ˆå›°éš¾ã€‚ç”±äºæˆ‘ä»¬å·²ç»åˆæ­¥äº†è§£äº†ä¸Šä¸‹æ–‡ï¼Œç¡®å®šå»ºæ¨¡ä¸»é¢˜çš„æ•°é‡ä¸ä¼šå¤ªç¦»è°±ã€‚ä½†æ˜¯ï¼Œå¦‚æœè¿™ä¸ªæ•°å­—å¤ªå¤§ï¼Œé‚£ä¹ˆæ¨¡å‹å¯èƒ½æ— æ³•æ£€æµ‹åˆ°å®é™…ä¸Šæ›´å¹¿æ³›çš„ä¸»é¢˜ï¼Œå¦‚æœè¿™ä¸ªæ•°å­—å¤ªå°ï¼Œé‚£ä¹ˆä¸»é¢˜å¯èƒ½æœ‰å¤§é‡é‡å çš„å•è¯ã€‚å› ä¸ºè¿™äº›åŸå› ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨è¯é¢˜è¿è´¯æ€§è¯„åˆ†ã€‚

```
from gensim.models import CoherenceModel

# Compute coherence score
number_of_topics = []
coherence_score = []
for i in range(1,10):
  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           iterations=50,
                                           num_topics=i)
  coherence_model_lda = CoherenceModel(model=lda_model, 
                                       texts=reviews['Review_Clean_List'], 
                                       dictionary=id2word, 
                                       coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  number_of_topics.append(i)
  coherence_score.append(coherence_lda)

# Create a dataframe of coherence score by number of topics 
topic_coherence = pd.DataFrame({'number_of_topics':number_of_topics,
                                'coherence_score':coherence_score})

# Print a line plot
sns.lineplot(data=topic_coherence, x='number_of_topics', y='coherence_score')
```

![](img/8d7de2d43e487baf67d2b41df670db89.png)![](img/3f0cbbe4f0232375ab2166d623cf213e.png)

ä¸»é¢˜æ•°é‡çš„è¿è´¯æ€§å¾—åˆ†

ç”±äºä½¿ç”¨å››ä¸ªä¸»é¢˜è·å¾—äº†éå¸¸é«˜çš„ä¸€è‡´æ€§åˆ†æ•°(0.3429)ï¼Œå¹¶ä¸”ä»å››ä¸ªä¸»é¢˜åˆ°äº”ä¸ªä¸»é¢˜æ²¡æœ‰å¤§çš„è·³è·ƒï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨å››ä¸ªä¸»é¢˜æ„å»ºæˆ‘ä»¬çš„ LDA æ¨¡å‹ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å°†ç›¸å¹²è¶…å‚æ•°å®šä¹‰ä¸º`coherence='c_v'`ï¼Œä½†ä¹Ÿæœ‰å…¶ä»–é€‰é¡¹ï¼Œå¦‚*â€˜u _ massâ€™ï¼Œâ€˜c _ UCIâ€™ï¼Œâ€˜c _ npmiâ€™*ï¼ŒéªŒè¯å®ƒä»¬å°†æ˜¯æœ€ä½³å®è·µã€‚(è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ Gensim çš„[æ–‡æ¡£](https://radimrehurek.com/gensim/models/coherencemodel.html)ã€‚)

# 5.åŸºäº LDA çš„ä¸»é¢˜å»ºæ¨¡

[æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)æ˜¯ä¸€ç§æµè¡Œçš„ç”¨äºä¸»é¢˜å»ºæ¨¡çš„ç»Ÿè®¡æ— ç›‘ç£æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒå‡è®¾æ¯ä¸ªä¸»é¢˜ç”±å•è¯ç»„æˆï¼Œæ¯ä¸ªæ–‡æ¡£(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯æ¯ä¸ªç»¼è¿°)ç”±è¿™äº›å•è¯çš„é›†åˆç»„æˆã€‚å› æ­¤ï¼ŒLDA è¯•å›¾æ‰¾åˆ°æœ€èƒ½æè¿°æ¯ä¸ªä¸»é¢˜çš„è¯ï¼Œå¹¶åŒ¹é…è¿™äº›è¯æ‰€ä»£è¡¨çš„è¯„è®ºã€‚

LDA ä½¿ç”¨[ç‹„åˆ©å…‹é›·åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Dirichlet_distribution)ï¼Œè¿™æ˜¯è´å¡”åˆ†å¸ƒçš„ä¸€ç§æ¨å¹¿ï¼Œä¸ºä¸¤ä¸ªæˆ–æ›´å¤šç»“æœ(K)çš„æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡ã€‚ä¾‹å¦‚ï¼ŒK = 2 æ˜¯è´å¡”åˆ†å¸ƒçš„ç‹„åˆ©å…‹é›·åˆ†å¸ƒçš„ç‰¹ä¾‹ã€‚

ç‹„åˆ©å…‹é›·åˆ†å¸ƒç”¨ Dir( *Î±)* è¡¨ç¤ºï¼Œå…¶ä¸­ *Î±* < 1(å¯¹ç§°)è¡¨ç¤ºç¨€ç–æ€§ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦å¦‚ä½•å‘ˆç°ä¸»é¢˜å’Œè¯ä»¥è¿›è¡Œä¸»é¢˜å»ºæ¨¡ã€‚æ­£å¦‚ä½ åœ¨ä¸‹é¢çœ‹åˆ°çš„ï¼Œä½¿ç”¨ *Î±* < 1ï¼Œæˆ‘ä»¬åœ¨è¾¹/è§’ä¸Šæœ‰å½¼æ­¤åˆ†å¼€çš„åœ†(æ¢å¥è¯è¯´æ˜¯ç¨€ç–çš„)ï¼Œè€Œä½¿ç”¨ *Î± >* 1ï¼Œæˆ‘ä»¬åœ¨ä¸­å¿ƒæœ‰å½¼æ­¤éå¸¸æ¥è¿‘å¹¶ä¸”éš¾ä»¥åŒºåˆ†çš„åœ†ã€‚ä½ å¯ä»¥æŠŠè¿™äº›åœˆæƒ³è±¡æˆè¯é¢˜ã€‚

![](img/ba45e2f77c611b1cd17ac551af70d534.png)

LDA ä½¿ç”¨ä¸¤ç§ç‹„åˆ©å…‹é›·åˆ†å¸ƒï¼Œå…¶ä¸­

*   k æ˜¯ä¸»é¢˜çš„æ•°é‡
*   *M* è¡¨ç¤ºæ–‡ä»¶çš„æ•°é‡
*   *N* è¡¨ç¤ºç»™å®šæ–‡æ¡£ä¸­çš„å­—æ•°
*   Dir(alpha)æ˜¯æ¯æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒçš„ Dirichlet åˆ†å¸ƒ
*   Dir(beta)æ˜¯æ¯ä¸»é¢˜å•è¯åˆ†å¸ƒçš„ç‹„åˆ©å…‹é›·åˆ†å¸ƒ

ç„¶åï¼Œå®ƒå¯¹æ¯ä¸ªå•è¯ä½ç½®ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒ

*   ä¸ºæ–‡æ¡£ I ä¸­çš„ç¬¬ *j* ä¸ªå•è¯é€‰æ‹©ä¸€ä¸ªé¢˜ç›®ï¼›z_{iï¼Œj}
*   ä¸ºç‰¹å®šå•è¯é€‰æ‹©ä¸€ä¸ªå•è¯ï¼›w_{iï¼Œj}

![](img/e91e37f9ce9ccfd79104c0c9cd234b69.png)![](img/af60eed121c109419759dafa5cbb65f7.png)

æ–‡çŒ®ä¸­ LDA çš„å¹³æ¿æ ‡è®°

å¦‚æœæˆ‘ä»¬æŠŠæ‰€æœ‰çš„ç‰‡æ®µæ”¾åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸‹é¢çš„å…¬å¼ï¼Œå®ƒæè¿°äº†å…·æœ‰ä¸¤ä¸ªç‹„åˆ©å…‹é›·åˆ†å¸ƒå’Œå¤šé¡¹å¼åˆ†å¸ƒçš„æ–‡æ¡£çš„æ¦‚ç‡ã€‚

![](img/3c6ece09608b490de049c6c99258a90e.png)

æ–‡ä»¶çš„æ¦‚ç‡

ç†è®ºå¤Ÿäº†ï¼ğŸ¤“è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Gensim ä¸­çš„`ldaModel`åœ¨ Python ä¸­æ‰§è¡Œ LDA æ¨¡å‹ã€‚

```
 # Define the number of topics 
n_topics = 4

# Run the LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=n_topics, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=10,
                                           passes=10,
                                           alpha='symmetric',
                                           iterations=100,
                                           per_word_topics=True)
```

è®©æˆ‘ä»¬æ¥æ¢ç©¶æ¯ä¸ªä¸»é¢˜ä¸­å‡ºç°çš„å•è¯åŠå…¶ç›¸å¯¹æƒé‡ã€‚

```
for idx, topic in lda_model.print_topics(-1):
    print("Topic: {} Word: {}".format(idx, topic))
```

![](img/b7199f5f311068c651e34df5a77b97c3.png)

æ¯ä¸ªä¸»é¢˜ä¸­å‡ºç°çš„å•è¯åŠå…¶ç›¸å¯¹æƒé‡

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸€ä¸ªè¯é¢˜ä¸æ’é˜Ÿç­‰å¾…æœ‰å…³ï¼›ä¸‹ä¸€ä¸ªå’Œå‚è§‚ã€ä½å®¿ã€ç¾é£Ÿæœ‰å…³ï¼›è¿˜æœ‰ä¸€ä¸ªè·Ÿé…’åº—ã€é—¨ç¥¨ã€æ‘åº„æœ‰å…³ï¼›æœ€åä¸€ä¸ªä¸é­”æœ¯ã€çˆ±æƒ…å’Œçªå‡ºå·´é»å’Œä½›ç½—é‡Œè¾¾çš„èŠ‚ç›®æœ‰å…³ã€‚

# 6.ç”¨ pyLDAvis å¯è§†åŒ–

pyLDAvis æ˜¯ä¸€ä¸ªåŸºäº web çš„äº¤äº’å¼å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºå¯è§†åŒ–ä¸»é¢˜æ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨`pip install pyldavis`è½»æ¾åœ°åœ¨ python ä¸­å®‰è£…ï¼Œå¹¶ä½¿ç”¨`enable_notebook().`åœ¨ Python ç¬”è®°æœ¬ä¸Šè¿è¡Œå¯è§†åŒ–

```
# Import and enable notebook to run visualization
import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()

vis = pyLDAvis.gensim_models.prepare(lda_model, 
                                     corpus, 
                                     dictionary=lda_model.id2word)
vis
```

![](img/0770ef46ad3d46baf282759023f5ab7d.png)

ä¸»é¢˜ 1 çš„ pyLDAvis è¡¨ç¤º(Î» = 1)

åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸ªä¸»é¢˜åœ¨ä¸»é¢˜é—´è·ç¦»å›¾ä¸Šè¡¨ç¤ºä¸ºä¸€ä¸ªæ°”æ³¡(å¤šç»´æ ‡åº¦åˆ° x å’Œ y è½´ä¸Š),å¦‚æœæˆ‘ä»¬å•å‡»ä¸€ä¸ªä¸»é¢˜ï¼Œå¯è§†åŒ–ä¼šè‡ªåŠ¨è°ƒæ•´åˆ°è¯¥ç‰¹å®šä¸»é¢˜ã€‚æ°”æ³¡ä¹‹é—´çš„è·ç¦»ä»£è¡¨ä¸»é¢˜ä¹‹é—´çš„è¯­ä¹‰è·ç¦»ï¼Œå¦‚æœæ°”æ³¡é‡å ï¼Œè¿™æ„å‘³ç€æœ‰å¾ˆå¤šå¸¸ç”¨è¯ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä¸»é¢˜è¢«å¾ˆå¥½åœ°åˆ†å¼€ï¼Œæ²¡æœ‰é‡å ã€‚æ­¤å¤–ï¼Œä¸»é¢˜æ°”æ³¡çš„é¢ç§¯è¡¨ç¤ºæ¯ä¸ªä¸»é¢˜çš„è¦†ç›–èŒƒå›´ï¼Œä¸»é¢˜ 1 è¦†ç›–äº†å¤§çº¦ 50%çš„è¯„è®ºï¼Œè€Œå…¶ä»–ä¸»é¢˜å…±äº«å‡ ä¹ç›¸ç­‰çš„æ•°é‡ã€‚

å³ä¾§çš„å¯è§†åŒ–æ˜¾ç¤ºäº†æ¯ä¸ªä¸»é¢˜çš„å‰ 30 ä¸ªæœ€ç›¸å…³çš„è¯ã€‚è“è‰²é˜´å½±æ¡è¡¨ç¤ºè¯¥è¯åœ¨æ‰€æœ‰è¯„è®ºä¸­çš„å‡ºç°ï¼Œçº¢è‰²æ¡è¡¨ç¤ºè¯¥è¯åœ¨æ‰€é€‰ä¸»é¢˜ä¸­çš„å‡ºç°ã€‚åœ¨å®ƒçš„é¡¶éƒ¨ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸€å¼ è°ƒæ•´ç›¸å…³æ€§åº¦é‡Î»(å…¶ä¸­ 0 â‰¤ Î» â‰¤ 1)çš„å¹»ç¯ç‰‡ï¼ŒÎ» = 1 è°ƒæ•´æ¯ä¸ªä¸»é¢˜ä¸­æœ€æœ‰å¯èƒ½å‡ºç°çš„å•è¯çš„å¯è§†åŒ–ï¼ŒÎ» = 0 è°ƒæ•´ä»…ç‰¹å®šäºæ‰€é€‰ä¸»é¢˜çš„å•è¯ã€‚

è®©æˆ‘ä»¬æ£€æŸ¥è¯é¢˜ 2ğŸ‘€

![](img/32ef2377b349a890073e75a97e6d33cd.png)

ä¸»é¢˜ 2 çš„ pyLDAvis è¡¨ç¤º(Î» = 1)

ä¸»é¢˜ 3ğŸ‘€

![](img/6b09a97fe162c92e9cb5a2df596875b5.png)

æœ€åæ˜¯ä¸»é¢˜ 4ğŸ‘€

![](img/97df64ddfb8ab1530df692eaa878e277.png)

# ç»“è®º

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•ä»æ–‡æœ¬æ•°æ®ä¸­æ£€æµ‹ä¸»é¢˜å’Œå…³é”®å­—ï¼Œä»¥ä¾¿åœ¨ä¸éœ€è¦æ‰«ææ•´ä¸ªæ–‡æœ¬çš„æƒ…å†µä¸‹ç†è§£å†…å®¹ã€‚æˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•åº”ç”¨é¢„å¤„ç†ï¼ŒåŒ…æ‹¬æ¸…ç†æ–‡æœ¬ã€è¯æ±‡åŒ–å’Œåˆ é™¤åœç”¨è¯&ä¸ºæœºå™¨å­¦ä¹ å‡†å¤‡æ•°æ®çš„æœ€å¸¸ç”¨è¯ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªå•è¯äº‘ï¼Œå¸®åŠ©æˆ‘ä»¬å¯è§†åŒ–æ•´ä½“å†…å®¹ã€‚ä¸ºäº†æ‰¾åˆ°è¿ªå£«å°¼ä¹å›­è¯„è®ºæ•°æ®é›†çš„ä¸»é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…(LDA)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸»é¢˜å»ºæ¨¡çš„æ¦‚ç‡æ–¹æ³•ï¼Œå‡è®¾*ä¸»é¢˜*å¯ä»¥è¡¨ç¤ºä¸ºæ–‡æœ¬è¯­æ–™åº“ä¸­å•è¯çš„åˆ†å¸ƒã€‚æ¯ä¸ªæ–‡æ¡£(åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹å›é¡¾ä¸­)å¯ä»¥å±•ç¤ºä¸æ­¢ä¸€ä¸ªä¸åŒæ¯”ä¾‹çš„ä¸»é¢˜ã€‚æ¯”ä¾‹æœ€é«˜çš„ä¸»é¢˜è¢«é€‰ä¸ºè¯¥æ–‡æ¡£çš„*ä¸»é¢˜*ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ coherence score æ¥å®šä¹‰ä¸»é¢˜çš„æ•°é‡ï¼Œæœ€åä½¿ç”¨ pyLDAvis æ¥å¯è§†åŒ–æˆ‘ä»¬çš„ä¸»é¢˜å’Œå…³é”®è¯ã€‚

LDA æ˜¯ä¸€ç§ç›¸å¯¹ç®€å•çš„ä¸»é¢˜å»ºæ¨¡æŠ€æœ¯ï¼Œå¤šäºäº† pyLDAvisï¼Œæ‚¨å¯ä»¥å‘ä¸ç†Ÿæ‚‰è¯¥æŠ€æœ¯èŒƒå›´çš„å…¶ä»–äººå±•ç¤ºç»“æœã€‚å¯è§†åŒ–ä¹Ÿæœ‰åŠ©äºæè¿°åŠŸèƒ½åŸç†ï¼Œå¹¶ä½¿ä¸»é¢˜æ¨¡å‹æ›´æ˜“äºè§£é‡Šå’Œè¯´æ˜ã€‚

è™½ç„¶æˆ‘ä»¬åªè®¨è®ºäº† LDA æŠ€æœ¯ï¼Œä½†æ˜¯è¿˜æœ‰è®¸å¤šå…¶ä»–æŠ€æœ¯å¯ä»¥ç”¨äºä¸»é¢˜å»ºæ¨¡ã€‚ä¸¾å‡ ä¸ªä¾‹å­ï¼Œ[(LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis)ï¼Œ[éè´ŸçŸ©é˜µåˆ†è§£](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)ï¼Œ [Word2vec](https://en.wikipedia.org/wiki/Word2vec) ã€‚å¦‚æœä½ å¯¹è¿™ä¸ªè¯é¢˜æ„Ÿå…´è¶£ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ ä¹Ÿæ¢ç´¢ä¸€ä¸‹è¿™äº›æ–¹æ³•ï¼Œå®ƒä»¬éƒ½æœ‰ä¸åŒçš„ä¼˜ç‚¹&ç¼ºç‚¹ï¼Œè¿™å–å†³äºç”¨ä¾‹ã€‚

æˆ‘å¸Œæœ›æ‚¨å–œæ¬¢é˜…è¯»å’Œå­¦ä¹ ä¸»é¢˜å»ºæ¨¡ï¼Œå¹¶å‘ç°è¿™ç¯‡æ–‡ç« å¾ˆæœ‰ç”¨ï¼âœ¨

**å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿ** [**æˆä¸ºä¼šå‘˜æ±‚æ›´ï¼**](https://idilismiguzel.medium.com/membership)

*ä½ å¯ä»¥åœ¨è¿™é‡Œ* [***é˜…è¯»æˆ‘çš„å…¶ä»–æ–‡ç« ***](https://medium.com/@idilismiguzel)**å’Œ* [***å…³æ³¨æˆ‘ä¸Šåª’***](http://medium.com/@idilismiguzel/follow)*å¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚âœ¨**

**å‚è€ƒ**

1.  **è¿ªå£«å°¼ä¹å›­æŸ¥çœ‹æ¥è‡ª Kaggle çš„æ•°æ®é›†ã€‚è®¸å¯è¯: [CC0:å…¬å…±é¢†åŸŸ](https://creativecommons.org/publicdomain/zero/1.0/)**
2.  **æ ‡é¢˜ç…§ç‰‡ç”±[å¸ƒæ‹‰å¾·åˆ©Â·è¾›æ ¼å°”é¡¿](https://unsplash.com/@bradleysingleton?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„**
3.  **æ‰€æœ‰å…¶ä»–å›¾ç‰‡å‡ç”±ä½œè€…æä¾›**
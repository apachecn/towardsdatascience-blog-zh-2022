# ç”¨ Python å®ç°å¸æ³•åˆ¤å†³çš„è‡ªç„¶è¯­è¨€å¤„ç†

> åŸæ–‡ï¼š<https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-9462c15c2a64>

![](img/a42e9b4e9510f434241ac07a2aabc2d6.png)

[https://pixabay.com/](https://pixabay.com/)

## ç¬¬ 10 éƒ¨åˆ†:é¢„æµ‹

æˆ‘ä»¬åˆ°äº†è¿™ä¸€ç³»åˆ—æ–‡ç« çš„æœ€åä¸€éƒ¨åˆ†ï¼Œåœ¨è¿™é‡Œæˆ‘å°†ä½¿ç”¨ ML æ¨¡å‹å¯¹å¸æ³•åˆ¤å†³è¿›è¡Œåˆ†ç±»ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†ä½¿ç”¨å·²ç»æ ‡è®°çš„è®°å½•ä½œä¸ºè®­ç»ƒé›†æ¥æ‰§è¡Œç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä»¥ä¾¿å°†æ¨¡å‹åº”ç”¨äºé‚£äº›æ²¡æœ‰æ ‡è®°çš„è®°å½•ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥æå¤§åœ°å¸®åŠ©åœ¨è®°å½•åˆ°è¾¾å®˜å‘˜æ‰‹ä¸­æ—¶è‡ªåŠ¨å¯¹è®°å½•è¿›è¡Œåˆ†ç±»ã€‚

æˆ‘å°†æ·±å…¥æ¢ç©¶æŠ€æœ¯:

*   æœ€é¢‘ç¹åŸºçº¿
*   é€»è¾‘å›å½’
*   æ”¯æŒå‘é‡åˆ†ç±»å™¨
*   å¸¦æœ‰ Keras çš„æ·±åº¦ç¥ç»ç½‘ç»œ

è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„åº“:

```
# Libraries we will use in this section
from sklearn.metrics import precision_score, classification_report, f1_score
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest, chi2

from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

from scipy.sparse import random

import sklearn as skl
skl.warnings.filterwarnings("ignore")
```

ä¸ºäº†æœ‰ä¸€ä¸ªåŸºå‡†æ¥æ¯”è¾ƒæˆ‘ä»¬çš„æ¨¡å‹ç»“æœï¼Œæˆ‘å°†é¦–å…ˆä½¿ç”¨æœ€é¢‘ç¹çš„åŸºçº¿ä½œä¸ºæ‰€æœ‰æ–‡æ¡£çš„é¢„æµ‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰äº†ä¸ä¹‹ç›¸å…³çš„æ€§èƒ½æŒ‡æ ‡ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹çš„æ€§èƒ½æ¯”æœ€é¢‘ç¹çš„åŸºçº¿å·®ï¼Œå®ƒå°±ä¸å€¼å¾—ã€‚

## æœ€é¢‘ç¹æ ‡ç­¾åŸºçº¿

åœ¨å°† *df_factor* åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹å‰ï¼Œæˆ‘å°†åˆ é™¤æ²¡æœ‰ä¸»é¢˜çš„è¡Œã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†åˆ›å»ºä¸€ä¸ªé®ç½©å¹¶å°†å…¶åº”ç”¨äº dfã€‚

```
#I will split first the dataset into a test set (20%) and a temporary set (80%). 
#Then, I will split the latter into train set (80% of temporary set) 
#and development set (20% of temporary set)

from sklearn.model_selection import train_test_split

tmp, test = train_test_split(df_factor_label, test_size=0.2, random_state=123) #for replicability
train, dev = train_test_split(tmp, test_size=0.2, random_state=123) #for replicability

vectorizer_logit = TfidfVectorizer(ngram_range = (2,6), min_df = 0.001, max_df = 0.75, stop_words = 'english')

X_train = vectorizer_logit.fit_transform(train.Lemmas)

#we cannot refit the vectorizer
X_dev = vectorizer_logit.transform(dev.Lemmas)
X_test = vectorizer_logit.transform(test.Lemmas)

y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
y_dev = dev.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
```

ä»æè¿°æ€§ç»Ÿè®¡éƒ¨åˆ†ï¼Œæˆ‘ä»¬çŸ¥é“æœ€å¸¸è§çš„ç±»åˆ«æ˜¯â€œç¨â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åªé¢„æµ‹æ‰€æœ‰è§‚æµ‹å€¼çš„ç¨æ”¶ä½œä¸ºåŸºçº¿æ¨¡å‹ã€‚è®©æˆ‘ä»¬æ£€ç´¢è¯¥ç±»åˆ«çš„ç´¢å¼•ã€‚ä½œä¸ºä¸€ä¸ªå‚è€ƒæŒ‡æ ‡ï¼Œæˆ‘å°†ä¸»è¦ä¾é å¾®è§‚å¹³å‡ f1 åˆ†æ•°ï¼Œå…¶ä¸­å¾®è§‚åœç•™åœ¨â€œè®¡ç®—æ€»çš„çœŸé˜³æ€§ã€å‡é˜´æ€§å’Œå‡é˜³æ€§ã€‚â€äº‹å®ä¸Šï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘å®è§‚å¹³å‡æŒ‡æ ‡ï¼Œæˆ‘ä»¬ä¸ä¼šè€ƒè™‘æ•°æ®ä¸å¹³è¡¡çš„äº‹å®ã€‚

```
ind = y_train.columns.get_loc("Tax")
#print(ind)

most_frequent = np.zeros(42)
most_frequent[ind] = 1
#print(most_frequent)

most_frequent_prediction = [most_frequent for i in range(y_dev.shape[0])]
most_frequent_prediction = pd.DataFrame(most_frequent_prediction, columns = y_dev.columns)

print(classification_report(y_dev, most_frequent_prediction, target_names =  y_dev.columns))
```

## é€»è¾‘å›å½’åŸºçº¿

```
logit = OneVsRestClassifier(LogisticRegression())
logit.fit(X_train, y_train)
```

è®©æˆ‘ä»¬å°†äº¤å‰éªŒè¯åº”ç”¨åˆ°åŸ¹è®­ä¸­(å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºäº¤å‰éªŒè¯çš„çŸ¥è¯†ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘ä»¥å‰çš„æ–‡ç« ):

```
# estimating the (test) F1 score via cross validation
for k in [2, 5, 10]:
    cv = cross_val_score(OneVsRestClassifier(LogisticRegression()), X_train, y = y_train, cv = k, n_jobs = -1, scoring = "f1_micro")
    fold_size = X_train.shape[0]/k

    print("F1 with {} folds for bag-of-words is {}".format(k, cv.mean()))
    print("Training on {} instances/fold, testing on {}".format(round(fold_size*(k-1)), round(fold_size)))
    print()
```

![](img/af67d31ac69ff9fe6b9fdcb34101729d.png)

```
y_pred_logit_baseline = logit.predict(X_dev)
print(classification_report(y_dev, y_pred_logit_baseline, target_names = y_dev.columns))
```

å°½ç®¡â€œå¹¼ç¨šâ€,è¿™ä¸ªæ¨¡å‹åœ¨æœ€é¢‘ç¹åŸºçº¿çš„å¼€å‘é›†ä¸­è¡¨ç°å¾—æ›´å¥½ã€‚æˆ‘ä»¬çš„å¾®è§‚ F1 å¾—åˆ†ä¸º 54%,ä½äº 16%ã€‚

ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬å†³å®šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚å®ƒåœ¨æµ‹è¯•æ•°æ®ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿ

```
y_pred_logit_baseline_test = logit.predict(X_test)
print(classification_report(y_test, y_pred_logit_baseline_test, target_names = y_test.columns))
```

![](img/acf5e0d4f00888f9e08a560d7044e253.png)

***æ­£è§„åŒ–å¼ºåº¦***

ç°åœ¨æˆ‘å°†æ”¹è¿›æˆ‘çš„é€»è¾‘æ¨¡å‹ã€‚

æ¯å½“æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éƒ½å¿…é¡»è€ƒè™‘æ–¹å·®åå·®çš„æƒè¡¡å’Œè¿‡åº¦æ‹Ÿåˆçš„é£é™©:äº‹å®ä¸Šï¼Œå¢åŠ å‚æ•°çš„æ•°é‡æ€»æ˜¯ä¼šå¯¼è‡´è®­ç»ƒè¯¯å·®çš„å‡å°‘ï¼Œä½†ä¸ä¼šå¯¼è‡´æµ‹è¯•è¯¯å·®çš„å‡å°‘ï¼Œå› ä¸º(å¹³æ–¹)åå·®ä¼šå‡å°‘ï¼Œä½†æ–¹å·®ä¼šå¢åŠ ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å½“æˆ‘ä»¬å¢åŠ å‚æ•°æ•°é‡æ—¶å¢åŠ æŸå¤±å‡½æ•°çš„æƒ©ç½šé¡¹ã€‚åœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚æ•° cã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œé™ä½ c å°†åŠ å¼ºğœ†è°ƒèŠ‚å™¨ã€‚çš„ç¡®ï¼Œc å¯¹ 1/ğœ†.æ¥è¯´æ˜¯å¥‡æ€ªçš„å› æ­¤ï¼Œğœ†è¶Šé«˜ï¼Œæƒ©ç½šé¡¹è¶Šé«˜ï¼Œæ¨¡å‹çš„å‚æ•°å°±è¶Šå°‘ã€‚

æ³¨:æƒ³äº†è§£æ›´å¤šå…³äºæ­£è§„åŒ–çš„å†…å®¹ï¼Œå¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘ä¹‹å‰çš„æ–‡ç« [ã€‚](https://medium.com/dataseries/preventing-overfitting-regularization-5eda7d5753bc)

```
from sklearn.metrics import f1_score
best_c = None
best_performance = 0.0

for c in [20, 10, 5, 2, 0.5, 0.1, 0.05, 0.01]:
    print(c)
    classifier_c = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', 
                                             class_weight='balanced',
                                             C=c
                                     ))
    classifier_c.fit(X_train, y_train)
    predictions_c = classifier_c.predict(X_dev)
    score = f1_score(y_dev, predictions_c, average='micro')
    if score > best_performance:
        best_performance = score
        best_c = c
        print("New best performance: {}".format(score))

    #print(classification_report(y_dev, predictions_c, target_names = y_dev.columns)) 
```

![](img/1c44ed91cdf60b911f10d4746a1e35f2.png)

ä»ä¸Šé¢çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œæœ€å¥½çš„æ¨¡å‹è¾“å‡ºç­‰äº 81%çš„å¾® f1 åˆ†æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°:

![](img/88ae29b2f2a99a78bfcdd129dc65522b.png)

***åŠŸèƒ½é€‰æ‹©***

è®©æˆ‘ä»¬æŠŠç‰¹å¾çš„æ•°é‡å‡å°‘åˆ° 4500 ä¸ªã€‚æˆ‘ä¼šæŠŠè¿™ä¸ªæ¨¡å‹å’Œä¹‹å‰ c çš„æœ€ä½³å€¼çš„ç»“æœç»“åˆèµ·æ¥ã€‚

```
from sklearn.feature_selection import SelectKBest, chi2

selector = SelectKBest(chi2, k=4500).fit(X_train, y_train)

X_train_sel = selector.transform(X_train)
X_dev_sel = selector.transform(X_dev)
X_test_sel = selector.transform(X_test)

classifier_sel = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', 
                                    class_weight='balanced', C = best_c))
classifier_sel.fit(X_train_sel, y_train)

predictions_sel = classifier_sel.predict(X_dev_sel)
print(classification_report(y_dev, predictions_sel, target_names = y_dev.columns))
```

![](img/4965c03d1e4715eb0d09d9a470eef668.png)

çœ‹æ¥ç‰¹å¾é€‰æ‹©å¹¶æ²¡æœ‰å¸¦æ¥æå‡(ç°åœ¨å¾® f1 è¯„åˆ†æ›´ä½ï¼Œ72%)ã€‚

```
#let's also evaluate the model in the test set
predictions_sel_test = classifier_sel.predict(X_test_sel)
print(classification_report(y_test, predictions_sel_test, target_names = y_test.columns))
```

![](img/affcff6b6828ded145515b3ddc8c1adf.png)

***é™ç»´***

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸åœ¨ç‰¹å¾ä¸­è¿›è¡Œé€‰æ‹©ï¼Œè€Œæ˜¯åˆ›å»ºæ–°çš„ä½ç»´ç‰¹å¾ï¼Œä½œä¸ºåŸå§‹ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚

```
from sklearn.decomposition import TruncatedSVD

best_performance=0.0
best_k = None

for k in [300, 500, 1000, 1500,2000, 2500]:
    print(k)
    svd = TruncatedSVD(n_components=k)

    X_train_dim = svd.fit_transform(X_train_sel)
    X_dev_dim = svd.transform(X_dev_sel)
    X_test_dim = svd.transform(X_test_sel)

    classifier_dim = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', 
                                        class_weight='balanced', C = best_c)) #still including the parameter c
    classifier_dim.fit(X_train_dim, y_train)
    predictions_dim = classifier_dim.predict(X_dev_dim)
    score = f1_score(y_dev, predictions_dim, average='micro')
    if score > best_performance:
        best_performance = score
        best_k = k
        print("New best performance: {}".format(score))

    #print(classification_report(y_dev, predictions_dim, target_names = y_dev.columns))
    print()
```

å®ƒå¯¼è‡´æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ç­‰äº 72%ã€‚

## æ”¯æŒå‘é‡åˆ†ç±»å™¨

å¯¹äºæˆ‘çš„ç¬¬äºŒä¸ªåˆ†ç±»æ¨¡å‹ï¼Œæˆ‘å†³å®šä½¿ç”¨ä¸€ä¸ªæ”¯æŒå‘é‡åˆ†ç±»å™¨(ä¸€å¯¹ä¸€å¯¹å…¨éƒ¨)ï¼Œåƒä»¥å‰ä¸€æ ·ä½¿ç”¨è°ƒä¼˜å‚æ•° Cã€‚

```
svc_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df = 0.25, stop_words = 'english')

tmp, test = train_test_split(df_factor_label, test_size=0.2, random_state=123) #for replicability
train, dev = train_test_split(tmp, test_size=0.2, random_state=123) #for replicability

X_train_svc = svc_vectorizer.fit_transform(train.text)
X_dev_svc = svc_vectorizer.transform(dev.text)
X_test_svc = svc_vectorizer.transform(test.text)

y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
y_dev = dev.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)

best_c = None #same ratio as above
best_f1_score = 0.0
for c in [5, 1.0, 0.5]:
    print(c)
    svc_clf = OneVsRestClassifier(LinearSVC(C = c, class_weight = 'balanced')).fit(X_train_svc, y_train)
    cv_reg = cross_val_score(svc_clf, X_train_svc, y = y_train, cv = 5, n_jobs = -1, scoring = "f1_micro")

    new_predictions_regularized = svc_clf.predict(X_dev_svc)
    f1 = f1_score(y_dev, new_predictions_regularized, average='micro')
    print("5-CV on train at C={}: {}".format(c, cv_reg.mean()))
    print(classification_report(y_dev, new_predictions_regularized, target_names = y_dev.columns))
    print()    
    if f1 > best_f1_score:
        best_f1_score = f1
        best_c = c
        #print("New best performance: {}".format(f1))
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€ä½³æ€§èƒ½æ˜¯ C=1 çš„é‚£ä¸ªï¼Œå¾® f1=0.84ã€‚

å› æ­¤ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æ‰€æœ‰ä¸Šè¿°æ¨¡å‹çš„æŒ‡æ ‡(åœ¨å¼€å‘é›†ä¸­):

![](img/af4984cf13c9c57166dbec4348ac2146.png)

å› æ­¤ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œæˆ‘å‡è®¾æœ€å¥½çš„æ˜¯ SVC:è¿™æ˜¯æˆ‘è¦ä¸ä¸¤ä¸ªåŸºçº¿è¿›è¡Œæ¯”è¾ƒçš„æ¨¡å‹ã€‚ä½†æ˜¯åœ¨è¿›å…¥å¼•å¯¼éƒ¨åˆ†ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚

```
lsvc = OneVsRestClassifier(LinearSVC(C = best_c, class_weight = "balanced"))
lsvc.fit(X_train_svc, y_train)

y_pred_svc = lsvc.predict(X_test_svc)
print(classification_report(y_test, y_pred_svc, target_names = y_test.columns))

#finally, let's store the dev predictions to be used in the next section.
best_preds = lsvc.predict(X_dev_svc)
```

![](img/e1ee306f2b5916ae1d1af183f678b7b5.png)

## Keras ç¥ç»ç½‘ç»œ

Keras æ˜¯ä¸€ä¸ªå¼€æºè½¯ä»¶åº“ï¼Œä¸º ann(äººå·¥ç¥ç»ç½‘ç»œ)æä¾›äº† Python æ¥å£ã€‚å®ƒå¯ä»¥è¿è¡Œåœ¨ TensorFlowã€å¾®è½¯è®¤çŸ¥å·¥å…·åŒ…ã€Theano æˆ– PlaidML ä¹‹ä¸Šã€‚

Keras çš„å¼€å‘æ˜¯ä¸ºäº†è®©ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æ›´å®¹æ˜“åŸå‹åŒ–ã€æ„å»ºå’Œå®éªŒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®ƒå…·æœ‰ç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œå…è®¸æ‚¨è½»æ¾åˆ›å»ºå’Œè®­ç»ƒå„ç§ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€é€’å½’ç¥ç»ç½‘ç»œ(RNNs)å’Œé•¿çŸ­æœŸè®°å¿†(LSTM)ç½‘ç»œã€‚

Keras çš„è®¾è®¡æ˜¯çµæ´»å’Œæ¨¡å—åŒ–çš„ï¼Œå› æ­¤æ‚¨å¯ä»¥è½»æ¾åœ°é…ç½®ã€ç»„åˆå’Œå¾®è°ƒæ‚¨åˆ›å»ºçš„æ¨¡å‹ã€‚å®ƒè¿˜å…·æœ‰å„ç§é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºä¸€ç³»åˆ—ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚

ä¸ºæ­¤ï¼Œæˆ‘å°†ä½¿ç”¨ Keras æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¥é¢„æµ‹ä¸å¸æ³•åˆ¤å†³ç›¸å…³çš„æ ‡ç­¾ã€‚

ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†åªè€ƒè™‘é‚£äº›å…·æœ‰å•ä¸ªæ ‡ç­¾çš„è®°å½•ï¼Œè¿™æ ·ä»»åŠ¡å°†å½’ç»“ä¸ºä¸€ä¸ªå¤šç±»ä»»åŠ¡ï¼Œè€Œä¸æ˜¯å¤šç±»ã€å¤šæ ‡ç­¾ä»»åŠ¡ã€‚

```
import tensorflow as tf
from keras.models import Sequential
from keras.layers import InputLayer, Input
from keras.layers import Reshape, MaxPooling2D
from keras.layers import Conv2D, Dense, Flatten, Activation, Dropout
from keras.optimizers import SGD, RMSprop, Adagrad #want to try different optimizers

#downloading df
df_factor = pd.read_pickle('data/df_factor.pkl')

#creating a df containing only records with one label
m=[len(df_factor.category[i])==1 for i in range(len(df_factor))]
df_factor_single_label=df_factor[m]

import pandas as pd
from sklearn.model_selection import train_test_split

train, test = train_test_split(df_factor_single_label, test_size=0.2, random_state = 123) #here, we don't need a dev set
                                                                                        #since it can be specified direclty
                                                                                        #during training
#for this purpose, I will use the TfIdf vecotrizer.

vectorizer_nn = TfidfVectorizer(ngram_range = (1, 2), min_df = 0.001, max_df = 0.25, stop_words = 'english')

#let's also store the full dataset into a X_nn variable, so that we will be able to plot the training history.

X_nn = vectorizer_nn.fit_transform(df_factor_single_label.text)
X_train_nn = vectorizer_nn.fit_transform(train.text)
X_test_nn = vectorizer_nn.transform(test.text)

y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
y = df_factor_single_label.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)
```

ç°åœ¨è®©æˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹:

```
model = Sequential()
model.add(Dense(3000, activation='relu', input_dim = X_train_nn.shape[1]))
model.add(Dropout(0.1))
model.add(Dense(600, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(200, activation='relu'))
model.add(Dense(y_train.shape[1], activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
#rms = RMSprop(learning_rate=0.001, rho=0.9)
#adam = Adagrad(learning_rate=0.01)
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(X_train_nn, y_train, epochs = 5, batch_size = 100, verbose = 1, validation_split=0.2)

#score = model.evaluate(X_test_nn, y_test, batch_size = 100)
#score
```

![](img/353e20bcfb4d79b235ed698622985376.png)

æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªéªŒè¯/å¼€å‘å‡†ç¡®ç‡ä¸º 91.23%çš„æ¨¡å‹ï¼Œè¿˜ä¸é”™ï¼

```
from keras.models import load_model

model.save('Keras Models/NN_labels.h5')  # creates a HDF5 file 'NN_labels.h5'

%matplotlib inline
import pandas as pd
import seaborn

df = pd.DataFrame(history.history)
df[['val_accuracy', 'accuracy']].plot.line()
df[['val_loss', 'loss']].plot.line()
```

![](img/56f0d5971583d872fd678fd8ff0ae6b8.png)![](img/c53c1141454977c9c9299882bfdf6b58.png)

```
#downloading model
from keras.models import load_model

model = load_model('Keras Models/NN_labels.h5')

#using it to predict on new, never-seen-before data.

loss, accuracy = model.evaluate(X_test_nn, y_test, batch_size = 100)

print("test loss: ", loss)
print("test accuracy: ", accuracy)
```

![](img/47769b24b32f83087e8a220bdf180210.png)

æµ‹è¯•ç²¾åº¦ç›¸å½“ä»¤äººæ»¡æ„(91.3%)ï¼Œå› æ­¤æˆ‘ç›¸ä¿¡è¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åœ°æ ‡è®°æ–°æ–‡ç« ã€‚

æœ‰äº†è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘æœ‰ä¿¡å¿ƒåœ¨æ–°æ–‡ç« è¢«æ’å…¥åˆ°æ•°æ®åº“ä¸­æ—¶è‡ªåŠ¨æ ‡è®°å®ƒä»¬ã€‚è¿™å¯èƒ½æ˜¯å®ç°èƒ½å¤Ÿè§£å†³åŸå§‹ä»»åŠ¡(å³å¤šç±»ã€å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜)çš„è§£å†³æ–¹æ¡ˆçš„èµ·ç‚¹ã€‚è¿™ä¸ªæ¨¡å‹çš„ä¸€ä¸ªå¯èƒ½çš„æ”¹è¿›æ˜¯:è®­ç»ƒä¸€ç»„ä¸åŒçš„åˆ†ç±»å™¨(å¯ä»¥æ˜¯å…·æœ‰ä¸åŒç»“æ„çš„ç¥ç»ç½‘ç»œï¼Œæˆ–è€…ä¸åŒçš„æ¨¡å‹ï¼Œå¦‚é€»è¾‘å›å½’ã€SVC ç­‰ç­‰)ã€‚).ç„¶åä½¿ç”¨å®ƒä»¬æ¥é¢„æµ‹æ–‡ç« çš„æµ‹è¯•åºåˆ—ï¼Œå¹¶ä¸”æ¯å½“æ¨¡å‹è¾“å‡ºä¸åŒçš„æ ‡ç­¾æ—¶ï¼Œå°†å®ƒä»¬éƒ½å½’å› äºè¯¥è§‚å¯Ÿ/æ–‡æœ¬ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯ä»¥ä½¿ç”¨é€»è¾‘å›å½’ã€SVC å’Œ NN æ¥é¢„æµ‹æ–°æ–‡ç« æ ‡ç­¾ï¼Œå¦‚æœå®ƒä»¬éƒ½è¿”å›ä¸åŒçš„ç»“æœï¼Œåˆ™å°†æ‰€æœ‰ä¸‰ä¸ªæ ‡ç­¾å½’å±äºè¯¥æ–‡ç« ã€‚

## ç»“è®º

æˆ‘ä»¬åˆ°äº†å…³äºå¸æ³•åˆ¤å†³çš„ NLP ç³»åˆ—æ–‡ç« çš„æœ€åä¸€éƒ¨åˆ†ã€‚å¦‚æœå°½ç®¡æˆ‘å†™çš„ä¸œè¥¿å¾ˆæ— èŠï¼Œä½ è¿˜æ˜¯è®¾æ³•æ¥äº†ğŸ˜ƒè°¢è°¢å¤§å®¶ï¼è¿™å¯¹æˆ‘å¤ªé‡è¦äº†ã€‚

æˆ‘æ€»æ˜¯æ„Ÿè°¢ä»»ä½•å»ºè®¾æ€§çš„åé¦ˆï¼Œæ‰€ä»¥è¯·éšæ—¶é€šè¿‡ Medium æˆ– Linkedin è”ç³»æˆ‘ã€‚

ä¸‹ä¸€ç¯‡æ–‡ç« å†è§ï¼

## å‚è€ƒ

*   [NLTK::è‡ªç„¶è¯­è¨€å·¥å…·åŒ…](https://www.nltk.org/)
*   [Python ä¸­çš„ spaCy å·¥ä¸šçº§è‡ªç„¶è¯­è¨€å¤„ç†](https://spacy.io/)
*   [å¸æ³•æ–°é—»| DOJ |å¸æ³•éƒ¨](https://www.justice.gov/news)
*   [å¸æ³•éƒ¨ 2009â€“2018 å¹´æ–°é—»å‘å¸ƒ| Kaggle](https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases)
*   [https://spacy.io/usage/linguistic-features#named-entities](https://spacy.io/usage/linguistic-features#named-entities)
*   [https://medium.com/p/d81bdfa14d97/edit](https://medium.com/p/d81bdfa14d97/edit)
*   https://www.nltk.org/api/nltk.sentiment.vader.html
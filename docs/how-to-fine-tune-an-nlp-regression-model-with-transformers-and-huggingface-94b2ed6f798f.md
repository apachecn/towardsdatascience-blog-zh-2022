# å¦‚ä½•ä½¿ç”¨è½¬æ¢å™¨å¾®è°ƒ NLP å›å½’æ¨¡å‹

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f>

## ä»æ•°æ®é¢„å¤„ç†åˆ°ä½¿ç”¨çš„å®Œæ•´æŒ‡å—

![](img/8947c6c9f8d0217916b7cb4156a44c7c.png)

ç…§ç‰‡ç”± [DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) åœ¨ [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹æ‘„

åƒ [HuggingFace](https://huggingface.co/) è¿™æ ·çš„åœ¨çº¿å›¾ä¹¦é¦†ä¸ºæˆ‘ä»¬æä¾›äº†æœ€å…ˆè¿›çš„é¢„è®­ç»ƒäººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºæ•°æ®ç§‘å­¦çš„è®¸å¤šä¸åŒåº”ç”¨ã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ¥è§£å†³å›å½’é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯ DistilBERTï¼Œå®ƒæ˜¯è‘—åçš„ BERT çš„æ›´è½»ã€æ›´å¿«çš„ç‰ˆæœ¬ï¼Œå…¶æ€§èƒ½ä¸º 95%ã€‚

å‡è®¾æˆ‘ä»¬æœ‰æ¥è‡ªåœ¨çº¿å¹¿å‘Šçš„æ–‡æœ¬ï¼Œå¹¶ä¸”å®ƒçš„å“åº”ç‡è¢«å¹¿å‘Šé›†æ ‡å‡†åŒ–äº†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå¯ä»¥é¢„æµ‹å¹¿å‘Šæ•ˆæœçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

è®©æˆ‘ä»¬é€šè¿‡å¯¼å…¥å¿…è¦çš„åº“å¹¶å¯¼å…¥æˆ‘ä»¬çš„æ•°æ®æ¥å¼€å§‹ç¼–ç :

```
import numpy as np
import pandas as pdimport transformers
from datasets import Dataset,load_dataset, load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassificationX=pd.read_csv('ad_data.csv')
X.head(3)
```

![](img/c66f5a0233b55803783548c278827e82.png)

**æ–‡æœ¬**ä»£è¡¨å¹¿å‘Šæ–‡æœ¬ï¼Œè€Œ**æ ‡ç­¾**æ˜¯æ ‡å‡†åŒ–å›å¤ç‡ã€‚

# ç†ŠçŒ«åˆ°æ•°æ®é›†

ä¸ºäº†ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦å°† **Pandas æ•°æ®å¸§**è½¬æ¢ä¸ºâ€œ**æ•°æ®é›†**æ ¼å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹ã€‚è¿™äº›å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤è½»æ¾å®Œæˆ:

```
dataset = Dataset.from_pandas(X,preserve_index=False) 
dataset = dataset.train_test_split(test_size=0.3) dataset
```

![](img/c7914f7238dd8f3f58c35004b833dbbe.png)

å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†å¯¹è±¡åŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ‚¨ä»ç„¶å¯ä»¥è®¿é—®å¦‚ä¸‹æ‰€ç¤ºçš„æ•°æ®:

```
dataset['train']['text'][:5]
```

![](img/3dfe25588df5a42775b966b73c4322e2.png)

# ä»¤ç‰ŒåŒ–&å¦‚ä½•æ·»åŠ æ–°ä»¤ç‰Œ

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å¯¼å…¥å®ƒçš„æ ‡è®°å™¨å¹¶æ ‡è®°æˆ‘ä»¬çš„æ•°æ®ã€‚

```
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

è®©æˆ‘ä»¬æ¥æ ‡è®°ä¸€ä¸ªå¥å­ï¼Œçœ‹çœ‹æˆ‘ä»¬å¾—åˆ°äº†ä»€ä¹ˆ:

```
tokenizer('ğŸš¨ JUNE DROP LIVE ğŸš¨')['input_ids']
```

![](img/38d1e16658d44f97947e182ec6f68395.png)

æˆ‘ä»¬å¯ä»¥è§£ç è¿™äº› id å¹¶çœ‹åˆ°å®é™…çš„ä»¤ç‰Œ:

```
[tokenizer.decode(i) for i in tokenizer('ğŸš¨ JUNE DROP LIVE ğŸš¨')['input_ids']]
```

![](img/58a3dee3bad617478798a790f15f3db8.png)

**ã€CLSã€‘**å’Œ**ã€SEPã€‘**æ˜¯ç‰¹æ®Šçš„æ ‡è®°ï¼Œæ€»æ˜¯å‡ºç°åœ¨å¥å­çš„å¼€å¤´å’Œç»“å°¾ã€‚å¦‚ä½ æ‰€è§ï¼Œä»£æ›¿è¡¨æƒ…ç¬¦å·çš„æ˜¯ğŸš¨æ˜¯**ã€UNKã€‘**ä»¤ç‰Œï¼Œè¡¨ç¤ºä»¤ç‰ŒæœªçŸ¥ã€‚è¿™æ˜¯å› ä¸ºé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹**è’¸é¦å™¨**çš„å•è¯è¢‹é‡Œæ²¡æœ‰è¡¨æƒ…ç¬¦å·ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å‘æ ‡è®°å™¨æ·»åŠ æ›´å¤šçš„æ ‡è®°ï¼Œä»¥ä¾¿åœ¨æˆ‘ä»¬æ ¹æ®æ•°æ®è°ƒæ•´æ¨¡å‹æ—¶å¯ä»¥å¯¹å®ƒä»¬è¿›è¡Œè®­ç»ƒã€‚è®©æˆ‘ä»¬ç»™æˆ‘ä»¬çš„ç¬¦å·åŒ–å™¨æ·»åŠ ä¸€äº›è¡¨æƒ…ç¬¦å·ã€‚

```
for i in ['ğŸš¨', 'ğŸ™‚', 'ğŸ˜', 'âœŒï¸' , 'ğŸ¤© ']:
    tokenizer.add_tokens(i)
```

ç°åœ¨ï¼Œå¦‚æœä½ å°†å¥å­ç¬¦å·åŒ–ï¼Œä½ ä¼šçœ‹åˆ°è¡¨æƒ…ç¬¦å·ä»ç„¶æ˜¯è¡¨æƒ…ç¬¦å·ï¼Œè€Œä¸æ˜¯[UNK]ç¬¦å·ã€‚

```
[tokenizer.decode(i) for i in tokenizer('ğŸš¨ JUNE DROP LIVE ğŸš¨')['input_ids']]
```

![](img/8df6d0529877ce10fd9d7bf7e97580e3.png)

ä¸‹ä¸€æ­¥æ˜¯å¯¹æ•°æ®è¿›è¡Œæ ‡è®°ã€‚

```
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

# å¾®è°ƒæ¨¡å‹

æ˜¯æ—¶å€™å¯¼å…¥é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹äº†ã€‚

```
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=1)
```

æ ¹æ®æ–‡æ¡£ï¼Œå¯¹äºå›å½’é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡ **num_labels=1** ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ä»¤ç‰ŒåµŒå…¥çš„å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬å‘ä»¤ç‰ŒåŒ–å™¨æ·»åŠ äº†æ›´å¤šçš„ä»¤ç‰Œã€‚

```
model.resize_token_embeddings(len(tokenizer))
```

# åº¦é‡å‡½æ•°

åœ¨å›å½’é—®é¢˜ä¸­ï¼Œæ‚¨è¯•å›¾é¢„æµ‹ä¸€ä¸ªè¿ç»­å€¼ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦åº¦é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´è·ç¦»çš„æŒ‡æ ‡ã€‚æœ€å¸¸è§çš„æŒ‡æ ‡æ˜¯ MSE(å‡æ–¹è¯¯å·®)å’Œ RMSE(å‡æ–¹æ ¹è¯¯å·®)ã€‚å¯¹äºè¿™ä¸ªåº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ RMSEï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°åœ¨è®­ç»ƒæ•°æ®æ—¶ä½¿ç”¨å®ƒã€‚

```
from datasets import load_metric def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    rmse = mean_squared_error(labels, predictions, squared=False)
    return {"rmse": rmse}
```

# è®­ç»ƒæ¨¡å‹

```
from transformers import TrainingArguments, Trainertraining_args = TrainingArguments(output_dir="test_trainer",
                                  logging_strategy="epoch",
                                  evaluation_strategy="epoch",
                                  per_device_train_batch_size=16,
                                  per_device_eval_batch_size=16,
                                  num_train_epochs=3,
                                  save_total_limit = 2,
                                  save_strategy = 'no',
                                  load_best_model_at_end=False
                                  ) trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    compute_metrics=compute_metrics
)
trainer.train()
```

# ä¿å­˜å¹¶åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å’Œæ ‡è®°å™¨

è¦ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:

```
# save the model/tokenizermodel.save_pretrained("model")
tokenizer.save_pretrained("tokenizer")# load the model/tokenizerfrom transformers import AutoModelForTokenClassification
model = AutoModelForSequenceClassification.from_pretrained("model")
tokenizer = AutoTokenizer.from_pretrained("tokenizer")
```

# å¦‚ä½•ä½¿ç”¨æ¨¡å‹

ä¸€æ—¦æˆ‘ä»¬åŠ è½½äº†è®°å·èµ‹äºˆå™¨å’Œæ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ Transformer çš„**è®­ç»ƒå™¨**ä»æ–‡æœ¬è¾“å…¥ä¸­è·å¾—é¢„æµ‹ã€‚æˆ‘åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†æ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶è¿”å›é¢„æµ‹ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ­¥éª¤å¦‚ä¸‹:

1.  å°†æ•°æ®å¸§ä¸­çš„æ–‡æœ¬æ·»åŠ åˆ°åä¸º text çš„åˆ—ä¸­ã€‚
2.  å°†æ•°æ®å¸§è½¬æ¢ä¸ºæ•°æ®é›†ã€‚
3.  å°†æ•°æ®é›†æ ‡è®°åŒ–ã€‚
4.  ä½¿ç”¨åŸ¹è®­å¸ˆè¿›è¡Œé¢„æµ‹ã€‚

å½“ç„¶ï¼Œä½ å¯ä»¥ä¸ç”¨ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†å¤šä¸ªè¾“å…¥ã€‚è¿™æ ·ï¼Œå®ƒä¼šæ›´å¿«ï¼Œå› ä¸ºå®ƒä½¿ç”¨æ‰¹æ¬¡åšé¢„æµ‹ã€‚

```
from transformers import Trainer
trainer = Trainer(model=model)def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True) def pipeline_prediction(text):
    df=pd.DataFrame({'text':[text]})
    dataset = Dataset.from_pandas(df,preserve_index=False) 
    tokenized_datasets = dataset.map(tokenize_function)
    raw_pred, _, _ = trainer.predict(tokenized_datasets) 
    return(raw_pred[0][0])pipeline_prediction("ğŸš¨ Get 50% now!")-0.019468416
```

# æ€»ç»“ä¸€ä¸‹

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ¥è§£å†³å›å½’é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨ Huggingface çš„ transformers åº“æ¥åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ DistilBERTï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„æ•°æ®å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚æˆ‘è®¤ä¸º transformer æ¨¡å‹éå¸¸å¼ºå¤§ï¼Œå¦‚æœä½¿ç”¨å¾—å½“ï¼Œå¯ä»¥æ¯” word2vec å’Œ TF-IDF ç­‰æ›´ç»å…¸çš„å•è¯åµŒå…¥æ–¹æ³•äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚

**æƒ³ä»æˆ‘è¿™é‡Œå¾—åˆ°æ›´å¤šï¼Ÿ:** [åœ¨åª’ä½“](https://medium.com/@billybonaros) ä¸Šå…³æ³¨æˆ‘åœ¨
ä¸­é“¾æ¥çš„[ä¸Šæ·»åŠ æˆ‘é€šè¿‡ä½¿ç”¨](https://www.linkedin.com/in/billybonaros/) [**æˆ‘çš„æ¨èé“¾æ¥**](https://billybonaros.medium.com/membership) æ³¨å†Œåª’ä½“æ¥æ”¯æŒæˆ‘ã€‚
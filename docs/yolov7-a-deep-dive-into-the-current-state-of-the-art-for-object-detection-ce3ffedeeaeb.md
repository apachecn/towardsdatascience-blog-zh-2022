# YOLOv7:æ·±å…¥äº†è§£å½“å‰ç‰©ä½“æ£€æµ‹çš„æœ€æ–°æŠ€æœ¯

> åŸæ–‡ï¼š<https://towardsdatascience.com/yolov7-a-deep-dive-into-the-current-state-of-the-art-for-object-detection-ce3ffedeeaeb>

## *åœ¨å®šåˆ¶åŸ¹è®­è„šæœ¬ä¸­ä½¿ç”¨ YOLOv7 éœ€è¦çŸ¥é“çš„ä¸€åˆ‡*

*æœ¬æ–‡ç”±* [*å…‹é‡Œæ–¯Â·ä¼‘æ–¯*](https://medium.com/@chris.p.hughes10) *&* [*ä¼¯çº³ç‰¹æ™®ä¼Šæ ¼é˜µè¥*](https://medium.com/@bepuca) åˆè‘—

åœ¨å…¶å‘å¸ƒåä¸ä¹…ï¼ŒYOLOv7 æ˜¯ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æœ€å¿«å’Œæœ€å‡†ç¡®çš„å®æ—¶å¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/abs/2207.02696)åœ¨ MS COCO æ•°æ®é›†ä¸Šå±•ç¤ºäº†è¿™ç§æ”¹è¿›çš„æ¶æ„å¦‚ä½•åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢è¶…è¶Šæ‰€æœ‰ä»¥å‰çš„ YOLO ç‰ˆæœ¬ä»¥åŠæ‰€æœ‰å…¶ä»–å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼›åœ¨ä¸ä½¿ç”¨ä»»ä½•é¢„è®­ç»ƒç ç çš„æƒ…å†µä¸‹å®ç°è¿™ä¸€æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨å›´ç»•ä»¥å‰çš„ YOLO æ¨¡å‹çš„å‘½åæƒ¯ä¾‹çš„æ‰€æœ‰[äº‰è®®ä¹‹å](https://blog.roboflow.com/yolov4-versus-yolov5/)ï¼Œç”±äº YOLOv7 æ˜¯ç”±å¼€å‘ [Scaled-YOLOv4](https://arxiv.org/abs/2011.08036) çš„åŒä¸€ä½œè€…å‘å¸ƒçš„ï¼Œæœºå™¨å­¦ä¹ ç¤¾åŒºä¼¼ä¹å¾ˆä¹æ„æ¥å—è¿™æ˜¯â€œå®˜æ–¹â€YOLO å®¶æ—çš„ä¸‹ä¸€ä¸ªè¿­ä»£ï¼

åœ¨ YOLOv7 å‘å¸ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬â€”â€”ä½œä¸ºå¾®è½¯æ•°æ®å’Œäººå·¥æ™ºèƒ½æœåŠ¡çº¿çš„ä¸€éƒ¨åˆ†â€”â€”æ­£åœ¨è¿›è¡Œä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºäºå¯¹è±¡æ£€æµ‹çš„å®¢æˆ·é¡¹ç›®ï¼Œè¿™ä¸ªé¢†åŸŸä¸ COCO å®Œå…¨ä¸åŒã€‚ä¸ç”¨è¯´ï¼Œæˆ‘ä»¬å’Œå®¢æˆ·éƒ½å¯¹å°† YOLOv7 åº”ç”¨äºæˆ‘ä»¬çš„é—®é¢˜çš„å‰æ™¯æ„Ÿåˆ°éå¸¸å…´å¥‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå½“ä½¿ç”¨å¼€ç®±å³ç”¨çš„è®¾ç½®æ—¶ï¼Œç»“æœæ˜¯â€¦è¿™ä¹ˆè¯´å§ï¼Œä¸å¤ªå¥½ã€‚

åœ¨é˜…è¯»äº†å®˜æ–¹è®ºæ–‡åï¼Œæˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶å®ƒå¯¹æ¶æ„çš„å˜åŒ–è¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ï¼Œä½†å®ƒå¿½ç•¥äº†è®¸å¤šå…³äºæ¨¡å‹å¦‚ä½•è¢«è®­ç»ƒçš„ç»†èŠ‚ï¼›ä¾‹å¦‚ï¼Œåº”ç”¨äº†å“ªäº›æ•°æ®æ‰©å……æŠ€æœ¯ï¼Œä»¥åŠæŸå¤±å‡½æ•°å¦‚ä½•è¡¡é‡æ¨¡å‹åšå¾—å¥½ä¸å¥½ï¼ä¸ºäº†ç†è§£è¿™äº›æŠ€æœ¯ç»†èŠ‚ï¼Œæˆ‘ä»¬å†³å®šç›´æ¥è°ƒè¯•ä»£ç ã€‚ç„¶è€Œï¼Œç”±äº YOLOv7 åº“æ˜¯ YOLOR codebase çš„ä¸€ä¸ªæ´¾ç”Ÿç‰ˆæœ¬ï¼Œè€Œ YOLOR code base æœ¬èº«æ˜¯ YOLOv5 çš„ä¸€ä¸ªæ´¾ç”Ÿç‰ˆæœ¬ï¼Œæˆ‘ä»¬å‘ç°å®ƒåŒ…å«äº†å¾ˆå¤šå¤æ‚çš„åŠŸèƒ½ï¼Œå…¶ä¸­å¾ˆå¤šåœ¨è®­ç»ƒæ¨¡å‹æ—¶å¹¶ä¸éœ€è¦ï¼›ä¾‹å¦‚ï¼Œèƒ½å¤Ÿä»¥ Yaml æ ¼å¼æŒ‡å®šå®šåˆ¶æ¶æ„ï¼Œå¹¶å°†å…¶è½¬æ¢æˆ PyTorch æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä»£ç åº“åŒ…å«è®¸å¤šå·²ç»ä»å¤´å®ç°çš„è‡ªå®šä¹‰ç»„ä»¶ï¼Œå¦‚å¤š GPU è®­ç»ƒå¾ªç¯ã€è‹¥å¹²æ•°æ®æ‰©å……ã€ä¿å­˜æ•°æ®åŠ è½½å™¨å·¥ä½œå™¨çš„é‡‡æ ·å™¨å’Œå¤šå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå…¶ä¸­è®¸å¤šç°åœ¨å¯ä»¥åœ¨ PyTorch æˆ–å…¶ä»–åº“ä¸­è·å¾—ã€‚ç»“æœï¼Œæœ‰å¾ˆå¤šä»£ç éœ€è¦åˆ†æï¼›æˆ‘ä»¬èŠ±äº†å¾ˆé•¿æ—¶é—´æ¥ç†è§£ä¸€åˆ‡æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠè®­ç»ƒå¾ªç¯çš„å¤æ‚æ€§ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹çš„å‡ºè‰²æ€§èƒ½ï¼æœ€ç»ˆï¼Œæœ‰äº†è¿™ç§ç†è§£ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿå»ºç«‹æˆ‘ä»¬çš„è®­ç»ƒé£Ÿè°±ï¼Œåœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸­è·å¾—æŒç»­çš„å¥½ç»“æœã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ‰“ç®—é‡‡ç”¨ä¸€ç§å®ç”¨çš„æ–¹æ³•æ¥æ¼”ç¤ºå¦‚ä½•åœ¨å®šåˆ¶çš„è®­ç»ƒè„šæœ¬ä¸­è®­ç»ƒ YOLOv7 æ¨¡å‹ï¼Œä»¥åŠæ¢ç´¢è¯¸å¦‚æ•°æ®æ‰©å……æŠ€æœ¯ã€å¦‚ä½•é€‰æ‹©å’Œä¿®æ”¹é”šç›’ä»¥åŠæ­ç¤ºæŸå¤±å‡½æ•°å¦‚ä½•å·¥ä½œç­‰é¢†åŸŸï¼›(å¸Œæœ›å¦‚æ­¤ï¼)ä½¿ä½ èƒ½å¤Ÿå»ºç«‹ä¸€ç§ç›´è§‰ï¼ŒçŸ¥é“ä»€ä¹ˆå¯èƒ½å¯¹ä½ è‡ªå·±çš„é—®é¢˜æœ‰æ•ˆã€‚ç”±äº YOLOv7 æ¶æ„åœ¨å®˜æ–¹æ–‡ä»¶ä»¥åŠè®¸å¤šå…¶ä»–æ¥æºä¸­éƒ½æœ‰è¯¦ç»†çš„æè¿°ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œä¸æ‰“ç®—è®¨è®ºå®ƒã€‚ç›¸åï¼Œæˆ‘ä»¬æ‰“ç®—å…³æ³¨æ‰€æœ‰å…¶ä»–ç»†èŠ‚ï¼Œè¿™äº›è™½ç„¶å¯¹ YOLOv7 çš„æ€§èƒ½æœ‰æ‰€è´¡çŒ®ï¼Œä½†**æ²¡æœ‰**åœ¨è®ºæ–‡ä¸­æ¶‰åŠã€‚è¿™å¾€å¾€æ˜¯é€šè¿‡å¤šä¸ªç‰ˆæœ¬çš„ YOLO æ¨¡å‹ç§¯ç´¯èµ·æ¥çš„çŸ¥è¯†ï¼Œä½†å¯¹äºåˆšè¿›å…¥è¯¥é¢†åŸŸçš„äººæ¥è¯´ï¼Œè¦æ‰¾åˆ°è¿™äº›çŸ¥è¯†å¯èƒ½éå¸¸å›°éš¾ã€‚

ä¸ºäº†è¯´æ˜è¿™äº›æ¦‚å¿µï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„ YOLOv7 å®ç°ï¼Œå®ƒåˆ©ç”¨äº†å®˜æ–¹çš„é¢„è®­ç»ƒæƒé‡ï¼Œä½†åœ¨ç¼–å†™æ—¶è€ƒè™‘äº†æ¨¡å—åŒ–å’Œå¯è¯»æ€§ã€‚è¿™ä¸ªé¡¹ç›®æœ€åˆæ˜¯ä¸ºäº†è®©æˆ‘ä»¬æ›´å¥½åœ°äº†è§£ YOLOv7 çš„å·¥ä½œåŸç†ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å¦‚ä½•åº”ç”¨å®ƒï¼Œä½†åœ¨æˆåŠŸåœ°å°†å®ƒç”¨äºå‡ ä¸ªä¸åŒçš„ä»»åŠ¡åï¼Œæˆ‘ä»¬å†³å®šå°†å…¶å…¬å¼€ã€‚è™½ç„¶æˆ‘ä»¬å»ºè®®ä½¿ç”¨[å®˜æ–¹å®ç°](https://github.com/WongKinYiu/yolov7)ï¼Œå¦‚æœä½ æƒ³å‡†ç¡®åœ°å¤åˆ¶ COCO ä¸Šçš„å…¬å¸ƒç»“æœï¼Œæˆ‘ä»¬å‘ç°è¿™ç§å®ç°æ›´çµæ´»åœ°åº”ç”¨å’Œæ‰©å±•åˆ°è‡ªå®šä¹‰åŸŸã€‚å¸Œæœ›è¿™ä¸ªå®ç°èƒ½å¤Ÿä¸ºä»»ä½•å¸Œæœ›åœ¨è‡ªå·±çš„å®šåˆ¶åŸ¹è®­è„šæœ¬ä¸­ä½¿ç”¨ YOLOv7 çš„äººæä¾›ä¸€ä¸ªæ¸…æ™°çš„èµ·ç‚¹ï¼ŒåŒæ—¶ä¸ºæœ€åˆå®ç°ä¸­åŸ¹è®­æ—¶ä½¿ç”¨çš„æŠ€æœ¯æä¾›æ›´å¤šçš„é€æ˜åº¦ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®º:

*   [ä»¥åˆé€‚çš„æ ¼å¼åŠ è½½æ•°æ®](#0c9c)
*   [ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†](#e427)
*   [äº†è§£ YOLOv7 æŸå¤±](#7c9e)
*   [å¾®è°ƒæ–°æ•°æ®é›†](#4f2d)
*   [ä»é›¶å¼€å§‹è®­ç»ƒ](#e086)

ä¸€è·¯æ¢ç´¢æ‰€æœ‰ç»†èŠ‚ï¼Œä¾‹å¦‚:

*   [é”šç®±](#2d59)
*   [ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ(FPN)](#2440)
*   [ä¸­å¿ƒå…ˆéªŒ](#c87a)
*   [æœ€ä¼˜è¿è¾“åˆ†é…](#b103)
*   [é©¬èµ›å…‹å¢å¼º](#3aa9)
*   [æ··éŸ³å¢å¼º](#408a)
*   [å°†æƒé‡è¡°å‡åº”ç”¨äºå‚æ•°ç»„](#515b)
*   [ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦](#6764)
*   [æ¢¯åº¦ç´¯ç§¯ï¼Œä»¥åŠè¿™å¦‚ä½•å½±å“é‡é‡è¡°å‡](#162b)
*   [å‹å· EMA](#0f91)
*   [ä¸ºæ‚¨çš„é—®é¢˜é€‰æ‹©åˆé€‚çš„é”šç®±å°ºå¯¸](#1f98)

***Tlï¼›åšå£«:*** *å¦‚æœä½ åªæƒ³çœ‹åˆ°ä¸€äº›å¯ä»¥ç›´æ¥ä½¿ç”¨çš„å·¥ä½œä»£ç ï¼Œå¤åˆ¶è¿™ç¯‡æ–‡ç« æ‰€éœ€çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨ç¬”è®°æœ¬* [*è¿™é‡Œ*](https://github.com/Chris-hughes10/Yolov7-training/blob/main/blog%20post/Yolo7%20blog%20post.ipynb) *ä¸­æ‰¾åˆ°ã€‚è™½ç„¶åœ¨æ•´ç¯‡æ–‡ç« ä¸­ä½¿ç”¨äº†ä»£ç ç‰‡æ®µï¼Œä½†è¿™ä¸»è¦æ˜¯å‡ºäºç¾è§‚çš„ç›®çš„ï¼Œè¯·éµä»ç¬”è®°æœ¬ï¼Œè€Œ* [*å’Œ*](https://github.com/Chris-hughes10/Yolov7-training) *ä¸ºå·¥ä½œä»£ç ã€‚*

# æ‰¿è®¤

æˆ‘ä»¬è¦æ„Ÿè°¢è‹±å›½èˆªç©ºå…¬å¸ï¼Œå¦‚æœæ²¡æœ‰ä»–ä»¬æŒç»­å»¶è¯¯çš„èˆªç­ï¼Œè¿™ç¯‡æ–‡ç« å¯èƒ½å°±ä¸ä¼šå‡ºç°ã€‚

# æ•°æ®åŠ è½½

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä»¥ YOLOv7 æœŸæœ›çš„æ ¼å¼åŠ è½½æˆ‘ä»¬çš„æ•°æ®é›†ã€‚

## é€‰æ‹©æ•°æ®é›†

è´¯ç©¿æœ¬æ–‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Kaggle æ±½è½¦å¯¹è±¡æ£€æµ‹æ•°æ®é›†](https://www.kaggle.com/datasets/sshikamaru/car-object-detection)ï¼›ç„¶è€Œï¼Œç”±äºæˆ‘ä»¬çš„ç›®çš„æ˜¯æ¼”ç¤º YOLOv7 å¦‚ä½•åº”ç”¨äºä»»ä½•é—®é¢˜ï¼Œè¿™å®é™…ä¸Šæ˜¯è¿™é¡¹å·¥ä½œä¸­æœ€ä¸é‡è¦çš„éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œç”±äºå›¾åƒä¸ COCO éå¸¸ç›¸ä¼¼ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨è¿›è¡Œä»»ä½•è®­ç»ƒä¹‹å‰ï¼Œç”¨é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå®éªŒã€‚

è¯¥æ•°æ®é›†çš„æ³¨é‡Šé‡‡ç”¨. csv æ–‡ä»¶çš„å½¢å¼ï¼Œè¯¥æ–‡ä»¶å°†å›¾åƒåç§°ä¸ç›¸åº”çš„æ³¨é‡Šç›¸å…³è”ï¼›å…¶ä¸­æ¯è¡Œä»£è¡¨ä¸€ä¸ªè¾¹ç•Œæ¡†ã€‚è™½ç„¶åœ¨è®­ç»ƒé›†ä¸­æœ‰å¤§çº¦ 1000 å¹…å›¾åƒï¼Œä½†æ˜¯åªæœ‰é‚£äº›å¸¦æœ‰æ³¨é‡Šçš„å›¾åƒæ‰åŒ…å«åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å®ƒåŠ è½½åˆ°ç†ŠçŒ«æ•°æ®å¸§ä¸­æ¥æŸ¥çœ‹å®ƒçš„æ ¼å¼ã€‚

![](img/26289140b6e98974f33c470d904beb24.png)

ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†ä¸­å¹¶éæ‰€æœ‰å›¾åƒéƒ½åŒ…å«æˆ‘ä»¬è¯•å›¾æ£€æµ‹çš„å¯¹è±¡çš„å®ä¾‹ï¼Œå› æ­¤æˆ‘ä»¬è¿˜å¸Œæœ›åŒ…å«ä¸€äº›ä¸åŒ…å«æ±½è½¦çš„å›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åŠ è½½æ³¨é‡Šï¼Œå…¶ä¸­ä¹ŸåŒ…æ‹¬ 100 å¼ â€œè´Ÿé¢â€å›¾åƒã€‚æ­¤å¤–ï¼Œç”±äºæŒ‡å®šçš„æµ‹è¯•é›†æ˜¯æœªæ ‡è®°çš„ï¼Œè®©æˆ‘ä»¬éšæœºé€‰å–è¿™äº›å›¾åƒçš„ 20%ä½œä¸ºæˆ‘ä»¬çš„éªŒè¯é›†ã€‚

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/examples/train_cars.py

import pandas as pd
import random

def load_cars_df(annotations_file_path, images_path):
    all_images = sorted(set([p.parts[-1] for p in images_path.iterdir()]))
    image_id_to_image = {i: im for i, im in enumerate(all_images)}
    image_to_image_id = {v: k for k, v, in image_id_to_image.items()}

    annotations_df = pd.read_csv(annotations_file_path)
    annotations_df.loc[:, "class_name"] = "car"
    annotations_df.loc[:, "has_annotation"] = True

    # add 100 empty images to the dataset
    empty_images = sorted(set(all_images) - set(annotations_df.image.unique()))
    non_annotated_df = pd.DataFrame(list(empty_images)[:100], columns=["image"])
    non_annotated_df.loc[:, "has_annotation"] = False
    non_annotated_df.loc[:, "class_name"] = "background"

    df = pd.concat((annotations_df, non_annotated_df))

    class_id_to_label = dict(
        enumerate(df.query("has_annotation == True").class_name.unique())
    )
    class_label_to_id = {v: k for k, v in class_id_to_label.items()}

    df["image_id"] = df.image.map(image_to_image_id)
    df["class_id"] = df.class_name.map(class_label_to_id)

    file_names = tuple(df.image.unique())
    random.seed(42)
    validation_files = set(random.sample(file_names, int(len(df) * 0.2)))
    train_df = df[~df.image.isin(validation_files)]
    valid_df = df[df.image.isin(validation_files)]

    lookups = {
        "image_id_to_image": image_id_to_image,
        "image_to_image_id": image_to_image_id,
        "class_id_to_label": class_id_to_label,
        "class_label_to_id": class_label_to_id,
    }
    return train_df, valid_df, lookups
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥åŠ è½½æˆ‘ä»¬çš„æ•°æ®:

![](img/c1dc1add4808ac62a24f2a68367b3fd8.png)

ä¸ºäº†æ›´å®¹æ˜“åœ°å°†é¢„æµ‹ä¸å›¾åƒç›¸å…³è”ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå›¾åƒåˆ†é…äº†ä¸€ä¸ªå”¯ä¸€çš„ idï¼›åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªé€’å¢çš„æ•´æ•°è®¡æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªæ•´æ•°å€¼æ¥è¡¨ç¤ºæˆ‘ä»¬æƒ³è¦æ£€æµ‹çš„ç±»ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯ä¸€ä¸ªå•ç‹¬çš„ç±»ï¼Œå³â€œcarâ€ã€‚

ä¸€èˆ¬ç‰©ä½“æ£€æµ‹æ¨¡å‹éƒ½ä¼šé¢„ç•™`0`ä½œä¸ºèƒŒæ™¯ç±»ï¼Œæ‰€ä»¥ç±»æ ‡ç­¾è¦ä»`1`å¼€å§‹ã€‚YOLOv7 çš„æƒ…å†µæ˜¯**è€Œä¸æ˜¯**ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»`0`å¼€å§‹æˆ‘ä»¬çš„ç±»ç¼–ç ã€‚å¯¹äºä¸åŒ…å«æ±½è½¦çš„å›¾åƒï¼Œæˆ‘ä»¬ä¸éœ€è¦ç±»åˆ« idã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ£€æŸ¥å‡½æ•°è¿”å›çš„æŸ¥æ‰¾æ¥ç¡®è®¤è¿™ä¸€ç‚¹ã€‚

![](img/e61c57dc4e5cca3e4ca0f9c38be3c31a.png)

æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„è®­ç»ƒå’ŒéªŒè¯é›†çš„æ¯ä¸ªç±»ä¸­çš„å›¾åƒæ•°é‡ã€‚ç”±äºä¸€å¹…å›¾åƒå¯èƒ½æœ‰å¤šä¸ªæ³¨é‡Šï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿åœ¨è®¡ç®—è®¡æ•°æ—¶è€ƒè™‘åˆ°è¿™ä¸€ç‚¹:

![](img/73864701f31bce7f5b630f3c7db5f1f3.png)

## åˆ›å»ºæ•°æ®é›†é€‚é…å™¨

é€šå¸¸ï¼Œåœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬ä¼šåˆ›å»ºä¸€ä¸ª PyTorch æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç‰¹å®šäºæˆ‘ä»¬å°†è¦è®­ç»ƒçš„æ¨¡å‹ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨é¦–å…ˆåˆ›å»ºæ•°æ®é›†â€˜adaptorâ€™ç±»çš„æ¨¡å¼ï¼Œå•ç‹¬è´Ÿè´£åŒ…è£…åº•å±‚æ•°æ®æºå¹¶é€‚å½“åœ°åŠ è½½å®ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä½¿ç”¨ä¸åŒæ•°æ®é›†æ—¶è½»æ¾åˆ‡æ¢é€‚é…å™¨ï¼Œè€Œæ— éœ€æ›´æ”¹ä»»ä½•ç‰¹å®šäºæˆ‘ä»¬æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹çš„é¢„å¤„ç†é€»è¾‘ã€‚

å› æ­¤ï¼Œç°åœ¨è®©æˆ‘ä»¬ä¸“æ³¨äºåˆ›å»ºä¸€ä¸ª`CarsDatasetAdaptor`ç±»ï¼Œå®ƒå°†ç‰¹å®šçš„åŸå§‹æ•°æ®é›†æ ¼å¼è½¬æ¢æˆå›¾åƒå’Œç›¸åº”çš„æ³¨é‡Šã€‚æ­¤å¤–ï¼Œè®©æˆ‘ä»¬åŠ è½½æˆ‘ä»¬åˆ†é…çš„å›¾åƒ idï¼Œä»¥åŠæˆ‘ä»¬çš„å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å¯¹æˆ‘ä»¬ä»¥åæœ‰ç”¨ã€‚

è¿™ä¸€ç‚¹çš„å®ç°å¦‚ä¸‹æ‰€ç¤º:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/examples/train_cars.py

from torch.utils.data import Dataset

class CarsDatasetAdaptor(Dataset):
    def __init__(
        self,
        images_dir_path,
        annotations_dataframe,
        transforms=None,
    ):
        self.images_dir_path = Path(images_dir_path)
        self.annotations_df = annotations_dataframe
        self.transforms = transforms

        self.image_idx_to_image_id = {
            idx: image_id
            for idx, image_id in enumerate(self.annotations_df.image_id.unique())
        }
        self.image_id_to_image_idx = {
            v: k for k, v, in self.image_idx_to_image_id.items()
        }

    def __len__(self) -> int:
        return len(self.image_idx_to_image_id)

    def __getitem__(self, index):
        image_id = self.image_idx_to_image_id[index]
        image_info = self.annotations_df[self.annotations_df.image_id == image_id]
        file_name = image_info.image.values[0]
        assert image_id == image_info.image_id.values[0]

        image = Image.open(self.images_dir_path / file_name).convert("RGB")
        image = np.array(image)

        image_hw = image.shape[:2]

        if image_info.has_annotation.any():
            xyxy_bboxes = image_info[["xmin", "ymin", "xmax", "ymax"]].values
            class_ids = image_info["class_id"].values
        else:
            xyxy_bboxes = np.array([])
            class_ids = np.array([])

        if self.transforms is not None:
            transformed = self.transforms(
                image=image, bboxes=xyxy_bboxes, labels=class_ids
            )
            image = transformed["image"]
            xyxy_bboxes = np.array(transformed["bboxes"])
            class_ids = np.array(transformed["labels"])

        return image, xyxy_bboxes, class_ids, image_id, image_hw
```

æ³¨æ„ï¼Œå¯¹äºæˆ‘ä»¬çš„èƒŒæ™¯å›¾åƒï¼Œæˆ‘ä»¬åªæ˜¯ä¸ºè¾¹ç•Œæ¡†å’Œç±» id è¿”å›ä¸€ä¸ªç©ºæ•°ç»„ã€‚

åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®è®¤æ•°æ®é›†çš„é•¿åº¦ä¸æˆ‘ä»¬ä¹‹å‰è®¡ç®—çš„è®­ç»ƒå›¾åƒçš„æ€»æ•°ç›¸åŒã€‚

![](img/461454279fc93cbf65bdd808c5165a5c.png)

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å¯è§†åŒ–æˆ‘ä»¬çš„ä¸€äº›å›¾åƒï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/9d7c3792f903257b4a51f48dc40266f8.png)![](img/870a9e9fea7bd5cd73a6c4713546cfe8.png)

## åˆ›å»º YOLOv7 æ•°æ®é›†

ç°åœ¨æˆ‘ä»¬å·²ç»åˆ›å»ºäº†æ•°æ®é›†é€‚é…å™¨ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ï¼Œå®ƒå°†æˆ‘ä»¬çš„è¾“å…¥é¢„å¤„ç†æˆ YOLOv7 ä¸ç®¡æˆ‘ä»¬ä½¿ç”¨çš„é€‚é…å™¨æ˜¯ä»€ä¹ˆï¼Œè¿™äº›æ­¥éª¤éƒ½åº”è¯¥ä¿æŒä¸å˜ã€‚

è¿™ä¸€ç‚¹çš„å®ç°å¦‚ä¸‹æ‰€ç¤º:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/dataset.py

class Yolov7Dataset(Dataset):
    """
    A dataset which takes an object detection dataset returning (image, boxes, classes, image_id, image_hw)
    and applies the necessary preprocessing steps as required by Yolov7 models.

    By default, this class expects the image, boxes (N, 4) and classes (N,) to be numpy arrays,
    with the boxes in (x1,y1,x2,y2) format, but this behaviour can be modified by
    overriding the `load_from_dataset` method.
    """

    def __init__(self, dataset, transforms=None):
        self.ds = dataset
        self.transforms = transforms

    def __len__(self):
        return len(self.ds)

    def load_from_dataset(self, index):
        image, boxes, classes, image_id, shape = self.ds[index]
        return image, boxes, classes, image_id, shape

    def __getitem__(self, index):
        image, boxes, classes, image_id, original_image_size = self.load_from_dataset(
            index
        )

        if self.transforms is not None:
            transformed = self.transforms(image=image, bboxes=boxes, labels=classes)
            image = transformed["image"]
            boxes = np.array(transformed["bboxes"])
            classes = np.array(transformed["labels"])

        image = image / 255  # 0 - 1 range

        if len(boxes) != 0:
            # filter boxes with 0 area in any dimension
            valid_boxes = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])
            boxes = boxes[valid_boxes]
            classes = classes[valid_boxes]

            boxes = torchvision.ops.box_convert(
                torch.as_tensor(boxes, dtype=torch.float32), "xyxy", "cxcywh"
            )
            boxes[:, [1, 3]] /= image.shape[0]  # normalized height 0-1
            boxes[:, [0, 2]] /= image.shape[1]  # normalized width 0-1
            classes = np.expand_dims(classes, 1)

            labels_out = torch.hstack(
                (
                    torch.zeros((len(boxes), 1)),
                    torch.as_tensor(classes, dtype=torch.float32),
                    boxes,
                )
            )
        else:
            labels_out = torch.zeros((0, 6))

        try:
            if len(image_id) > 0:
                image_id_tensor = torch.as_tensor([])

        except TypeError:
            image_id_tensor = torch.as_tensor(image_id)

        return (
            torch.as_tensor(image.transpose(2, 0, 1), dtype=torch.float32),
            labels_out,
            image_id_tensor,
            torch.as_tensor(original_image_size),
        )
```

è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†åŒ…è£…æˆ‘ä»¬çš„æ•°æ®é€‚é…å™¨ï¼Œå¹¶æ£€æŸ¥ä¸€äº›è¾“å‡º:

![](img/cd2df566df55baf94b7f940e1b2506a3.png)

ç”±äºæˆ‘ä»¬æ²¡æœ‰å®šä¹‰ä»»ä½•è½¬æ¢ï¼Œè¾“å‡ºåŸºæœ¬ä¸Šæ˜¯ç›¸åŒçš„ï¼Œä¸»è¦çš„ä¾‹å¤–æ˜¯ç›’å­ç°åœ¨æ˜¯è§„èŒƒåŒ–çš„ cxcywh æ ¼å¼ï¼Œå¹¶ä¸”æˆ‘ä»¬æ‰€æœ‰çš„è¾“å‡ºéƒ½è¢«è½¬æ¢æˆå¼ é‡ã€‚æ³¨æ„`cx`ï¼Œ`cy`ä»£è¡¨ä¸­å¿ƒ x å’Œ yï¼Œè¿™æ„å‘³ç€åæ ‡å¯¹åº”äºç›’å­çš„ä¸­å¿ƒã€‚

éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬çš„æ ‡ç­¾é‡‡ç”¨äº†`[0, class_id, ncx, ncy, nw, nh]`çš„å½¢å¼ã€‚å¼ é‡å¼€å§‹å¤„çš„é›¶ç©ºé—´å°†è¢« collate å‡½æ•°ç¨åä½¿ç”¨ã€‚

## è½¬æ¢

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›è½¬æ¢ï¼ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¼˜ç§€çš„[albuminations åº“](https://albumentations.ai/)ï¼Œå®ƒæä¾›äº†è®¸å¤šè½¬æ¢å›¾åƒå’Œè¾¹ç•Œæ¡†çš„é€‰é¡¹ã€‚

è™½ç„¶æˆ‘ä»¬é€‰æ‹©çš„è½¬æ¢å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯é¢†åŸŸç‰¹å®šçš„ï¼Œä½†æ˜¯åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸åŸå§‹å®ç°ä¸­ä½¿ç”¨çš„è½¬æ¢ç›¸ä¼¼çš„è½¬æ¢ã€‚

è¿™äº›æ˜¯:

*   åœ¨ä¿æŒçºµæ¨ªæ¯”çš„åŒæ—¶ï¼Œæ ¹æ®ç»™å®šçš„è¾“å…¥(640 çš„å€æ•°)è°ƒæ•´å›¾åƒå¤§å°
*   å¦‚æœå›¾åƒä¸æ˜¯æ­£æ–¹å½¢ï¼Œåº”ç”¨å¡«å……ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†éµå¾ªçº¸åœ¨ä½¿ç”¨ç°è‰²å¡«å……ï¼Œè¿™æ˜¯ä¸€ä¸ªä»»æ„çš„é€‰æ‹©ã€‚

åŸ¹è®­æœŸé—´:

*   æ°´å¹³ç¿»è½¬ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°æ¥åˆ›å»ºè¿™äº›è½¬æ¢ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/dataset.py

def create_yolov7_transforms(
    image_size=(640, 640),
    training=False,
    training_transforms=(A.HorizontalFlip(p=0.5),),
):
    transforms = [
        A.LongestMaxSize(max(image_size)),
        A.PadIfNeeded(
            image_size[0],
            image_size[1],
            border_mode=0,
            value=(114, 114, 114),
        ),
    ]

    if training:
        transforms.extend(training_transforms)

    return A.Compose(
        transforms,
        bbox_params=A.BboxParams(format="pascal_voc", label_fields=["labels"]),
    )
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬é‡æ–°åˆ›å»ºæ•°æ®é›†ï¼Œè¿™ä¸€æ¬¡ä¼ é€’å°†åœ¨è¯„ä¼°æœŸé—´ä½¿ç”¨çš„é»˜è®¤è½¬æ¢ã€‚å¯¹äºæˆ‘ä»¬çš„ç›®æ ‡å›¾åƒå°ºå¯¸ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`640`ï¼Œè¿™æ˜¯è¾ƒå°çš„ YOLOv7 æ¨¡å‹çš„è®­ç»ƒå€¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹© 8 çš„ä»»æ„å€æ•°ã€‚

![](img/689b81ee644ffd8dad234e62c256513e.png)![](img/88059758f4a80e1abab9950b6693901e.png)

ä½¿ç”¨è¿™äº›å˜æ¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„å›¾åƒå·²ç»è¢«è°ƒæ•´åˆ°æˆ‘ä»¬çš„ç›®æ ‡å¤§å°ï¼Œå¹¶ä¸”åº”ç”¨äº†å¡«å……ã€‚ä½¿ç”¨å¡«å……çš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä¿æŒå›¾åƒä¸­å¯¹è±¡çš„é•¿å®½æ¯”ï¼Œä½†åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­å›¾åƒæœ‰ä¸€ä¸ªå…±åŒçš„å¤§å°ï¼›ä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰¹é‡å¤„ç†å®ƒä»¬ï¼

# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

æ—¢ç„¶æˆ‘ä»¬å·²ç»æ¢ç´¢äº†å¦‚ä½•åŠ è½½å’Œå‡†å¤‡æˆ‘ä»¬çš„æ•°æ®ï¼Œè®©æˆ‘ä»¬ç»§ç»­çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥è¿›è¡Œä¸€äº›é¢„æµ‹ï¼

## åŠ è½½æ¨¡å‹

ä¸ºäº†ç†è§£å¦‚ä½•ä¸æ¨¡å‹äº¤äº’ï¼Œè®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„æ£€æŸ¥ç‚¹ï¼Œå¹¶ä½¿ç”¨å®ƒå¯¹æ•°æ®é›†ä¸­çš„ä¸€äº›å›¾åƒè¿›è¡Œæ¨æ–­ã€‚ç”±äºè¿™ä¸ªæ£€æŸ¥ç‚¹æ˜¯åœ¨åŒ…å«æ±½è½¦å›¾åƒçš„ COCO ä¸Šè®­ç»ƒçš„ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾è¿™ä¸ªæ¨¡å‹åœ¨å¼€ç®±å³ç”¨çš„æƒ…å†µä¸‹åº”è¯¥åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å¾—ç›¸å½“å¥½ã€‚ä¸ºäº†æŸ¥çœ‹å¯ç”¨çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¯¼å…¥`AVAILABLE_MODELS`å˜é‡ã€‚

![](img/6f1c1f5a08c981de1305491ea10ab311.png)

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¯ç”¨çš„æ¨¡å‹æ˜¯åŸå§‹è®ºæ–‡ä¸­å®šä¹‰çš„ä½“ç³»ç»“æ„ã€‚è®©æˆ‘ä»¬ä½¿ç”¨`create_yolov7_model`å‡½æ•°åˆ›å»ºæ ‡å‡†çš„`yolov7`æ¨¡å‹ã€‚

![](img/9085f97f125f488731e68e249eb469a3.png)

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æ¨¡å‹çš„é¢„æµ‹ã€‚å‘å‰é€šè¿‡æ¨¡å‹å°†è¿”å› FPN å¤´ç»™å‡ºçš„åŸå§‹ç‰¹å¾åœ°å›¾ï¼Œä¸ºäº†å°†è¿™äº›è½¬æ¢æˆæœ‰æ„ä¹‰çš„é¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`postprocess`æ–¹æ³•ã€‚

![](img/99deaba3ef99196a11c5d702358a4ed4.png)

è€ƒå¯Ÿå½¢çŠ¶ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹å·²ç»åšäº† 25200 æ¬¡é¢„æµ‹ï¼æ¯ä¸ªé¢„æµ‹éƒ½æœ‰ä¸€ä¸ªå…³è”çš„é•¿åº¦ä¸º 6 çš„å¼ é‡-æ¡ç›®å¯¹åº”äº xyxy æ ¼å¼çš„è¾¹ç•Œæ¡†åæ ‡ã€ç½®ä¿¡åº¦å¾—åˆ†å’Œç±»åˆ«ç´¢å¼•ã€‚

é€šå¸¸ï¼Œå¯¹è±¡æ£€æµ‹æ¨¡å‹å€¾å‘äºåšå‡ºè®¸å¤šç›¸ä¼¼çš„ã€é‡å çš„é¢„æµ‹ã€‚è™½ç„¶æœ‰è®¸å¤šæ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨æœ€åˆçš„è®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†[éæœ€å¤§æŠ‘åˆ¶](https://paperswithcode.com/method/non-maximum-suppression) (NMS)æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°æ¥åº”ç”¨ NMS ä»¥åŠç¬¬äºŒè½®ç½®ä¿¡åº¦é˜ˆå€¼ã€‚æ­¤å¤–ï¼Œåœ¨åå¤„ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›è¿‡æ»¤ç½®ä¿¡åº¦ä½äºé¢„å®šä¹‰é˜ˆå€¼çš„ä»»ä½•é¢„æµ‹ï¼Œè®©æˆ‘ä»¬åœ¨æ­¤å¤„å¢åŠ ç½®ä¿¡åº¦é˜ˆå€¼ã€‚

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/trainer.py

def filter_eval_predictions(
    predictions: List[Tensor],
    confidence_threshold: float = 0.2,
    nms_threshold: float = 0.65,
) -> List[Tensor]:
    nms_preds = []
    for pred in predictions:
        pred = pred[pred[:, 4] > confidence_threshold]

        nms_idx = torchvision.ops.batched_nms(
            boxes=pred[:, :4],
            scores=pred[:, 4],
            idxs=pred[:, 5],
            iou_threshold=nms_threshold,
        )
        nms_preds.append(pred[nms_idx])

    return nms_preds
```

![](img/1ba13188f789b46a8867fb8c19902d78.png)

åº”ç”¨ NMS åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨æˆ‘ä»¬åªæœ‰ä¸€ä¸ªå•ä¸€çš„é¢„æµ‹è¿™ä¸ªå›¾åƒã€‚è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹è¿™æ˜¯ä»€ä¹ˆæ ·å­:

![](img/c992ed5c71fe1c416093438dc35394ff.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªçœ‹èµ·æ¥ç›¸å½“ä¸é”™ï¼æ¥è‡ªæ¨¡å‹çš„é¢„æµ‹å®é™…ä¸Šæ¯”åœ°é¢äº‹å®æ›´ç´§å¯†åœ°å›´ç»•ç€æ±½è½¦ï¼

ç°åœ¨æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„é¢„æµ‹ï¼Œå”¯ä¸€è¦æ³¨æ„çš„æ˜¯è¾¹ç•Œæ¡†æ˜¯ç›¸å¯¹äº*è°ƒæ•´åçš„*å›¾åƒå¤§å°çš„ã€‚ä¸ºäº†å°†æˆ‘ä»¬çš„é¢„æµ‹ç¼©æ”¾å›åŸå§‹å›¾åƒå¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/trainer.py

def scale_bboxes_to_original_image_size(
    xyxy_boxes, resized_hw, original_hw, is_padded=True
):
    scaled_boxes = xyxy_boxes.clone()
    scale_ratio = resized_hw[0] / original_hw[0], resized_hw[1] / original_hw[1]

    if is_padded:
        # remove padding
        pad_scale = min(scale_ratio)
        padding = (resized_hw[1] - original_hw[1] * pad_scale) / 2, (
            resized_hw[0] - original_hw[0] * pad_scale
        ) / 2
        scaled_boxes[:, [0, 2]] -= padding[0]  # x padding
        scaled_boxes[:, [1, 3]] -= padding[1]  # y padding
        scale_ratio = (pad_scale, pad_scale)

    scaled_boxes[:, [0, 2]] /= scale_ratio[1]
    scaled_boxes[:, [1, 3]] /= scale_ratio[0]

    # Clip bounding xyxy bounding boxes to image shape (height, width)
    scaled_boxes[:, 0].clamp_(0, original_hw[1])  # x1
    scaled_boxes[:, 1].clamp_(0, original_hw[0])  # y1
    scaled_boxes[:, 2].clamp_(0, original_hw[1])  # x2
    scaled_boxes[:, 3].clamp_(0, original_hw[0])  # y2

    return scaled_boxes
```

![](img/50689bb4a6cfca5fc25dc9b95472ed3b.png)

# ç†è§£æŸå¤±

åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œé™¤äº†æ¨¡å‹æ¶æ„ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå®ƒå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¡¡é‡æˆ‘ä»¬çš„æ¨¡å‹æ‰§è¡Œå¾—æœ‰å¤šå¥½ï¼›ä¸ºäº†æ›´æ–°æˆ‘ä»¬çš„å‚æ•°ã€‚ç”±äºå¯¹è±¡æ£€æµ‹æ˜¯æ•™å¯¼æ¨¡å‹çš„ä¸€ä¸ªéš¾é¢˜ï¼Œæ‰€ä»¥è¿™ç§æ¨¡å‹çš„æŸå¤±å‡½æ•°é€šå¸¸ç›¸å½“å¤æ‚ï¼ŒYOLOv7 ä¹Ÿä¸ä¾‹å¤–ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å°½æœ€å¤§åŠªåŠ›è¯´æ˜å…¶èƒŒåçš„ç›´è§‰ï¼Œä»¥ä¿ƒè¿›å…¶ç†è§£ã€‚

åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶å®é™…æŸå¤±å‡½æ•°ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€äº›éœ€è¦ç†è§£çš„èƒŒæ™¯æ¦‚å¿µã€‚

## é”šç®±

ç›®æ ‡æ£€æµ‹çš„ä¸»è¦å›°éš¾ä¹‹ä¸€æ˜¯è¾“å‡ºæ£€æµ‹æ¡†ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¦‚ä½•è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥åˆ›å»ºä¸€ä¸ªè¾¹ç•Œæ¡†ï¼Œå¹¶åœ¨å›¾åƒä¸­æ­£ç¡®å®šä½å®ƒï¼Ÿ

æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•ï¼Œä½† YOLOv7 å®¶æ—æ˜¯æˆ‘ä»¬æ‰€è¯´çš„åŸºäº**ä¸»æ’­çš„**æ¨¡å¼ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œä¸€èˆ¬çš„å“²å­¦æ˜¯é¦–å…ˆåˆ›å»ºè®¸å¤šæ½œåœ¨çš„åŒ…å›´ç›’ï¼Œç„¶åé€‰æ‹©æœ€æœ‰å¸Œæœ›çš„é€‰é¡¹æ¥åŒ¹é…æˆ‘ä»¬çš„ç›®æ ‡å¯¹è±¡ï¼›æ ¹æ®éœ€è¦ç¨å¾®ç§»åŠ¨å’Œè°ƒæ•´å®ƒä»¬çš„å¤§å°ï¼Œä»¥è·å¾—æœ€ä½³çš„åŒ¹é…ã€‚

åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªå›¾åƒçš„é¡¶éƒ¨ç»˜åˆ¶ä¸€ä¸ªç½‘æ ¼ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªç½‘æ ¼äº¤å‰ç‚¹(**é”šç‚¹**)ï¼ŒåŸºäºå¤šä¸ª**é”šç‚¹å¤§å°**ç”Ÿæˆå€™é€‰æ¡†(**é”šç‚¹æ¡†**)ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŒä¸€ç»„æ¡†åœ¨æ¯ä¸ªé”šç‚¹é‡å¤å‡ºç°ã€‚è¿™æ ·ï¼Œmodel éœ€è¦å­¦ä¹ çš„ä»»åŠ¡ï¼Œç¨å¾®è°ƒæ•´è¿™äº›ç›’å­çš„ä½ç½®å’Œå¤§å°ï¼Œæ¯”ä»å¤´å¼€å§‹ç”Ÿæˆç›’å­è¦ç®€å•ã€‚

![](img/e6de164a3e5a254407b940d8cf1b1ba5.png)

*åœ¨é”šç‚¹æ ·æœ¬å¤„ç”Ÿæˆçš„é”šç‚¹æ¡†çš„ç¤ºä¾‹ã€‚*

ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬çš„ç›®æ ‡ï¼Œåœ°é¢çœŸç›¸ï¼Œç›’å­çš„å¤§å°å¯ä»¥å˜åŒ–â€”â€”ä»å°åˆ°å¤§ï¼å› æ­¤ï¼Œé€šå¸¸ä¸å¯èƒ½å®šä¹‰ä¸€ç»„å¯ä»¥åŒ¹é…æ‰€æœ‰ç›®æ ‡çš„é”šå°ºå¯¸ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼ŒåŸºäºé”šçš„æ¨¡å‹æ¶æ„é€šå¸¸é‡‡ç”¨ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ(FPN)æ¥ååŠ©è¿™ä¸€ç‚¹ï¼›YOLOv7 å°±æ˜¯è¿™ç§æƒ…å†µã€‚

## è¦ç´ é‡‘å­—å¡”ç½‘ç»œ(FPN)

FPNs(åœ¨[ç”¨äºå¯¹è±¡æ£€æµ‹çš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ](https://arxiv.org/abs/1612.03144)ä¸­ä»‹ç»)èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨å·ç§¯å±‚çš„æ€§è´¨â€”â€”å‡å°‘ç‰¹å¾ç©ºé—´çš„å¤§å°å¹¶å¢åŠ åˆå§‹å›¾åƒä¸­æ¯ä¸ªç‰¹å¾çš„è¦†ç›–èŒƒå›´â€”â€”æ¥è¾“å‡ºä¸åŒæ¯”ä¾‹çš„é¢„æµ‹ã€‚fpn é€šå¸¸è¢«å®ç°ä¸ºå·ç§¯å±‚çš„å †æ ˆï¼Œæ­£å¦‚æˆ‘ä»¬é€šè¿‡æ£€æŸ¥ YOLOv7 æ¨¡å‹çš„æ£€æµ‹å¤´æ‰€çœ‹åˆ°çš„é‚£æ ·ã€‚

![](img/aa35c3492e644e5c6057ac38e88f44a2.png)

è™½ç„¶æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æœ€ç»ˆå±‚çš„è¾“å‡ºä½œä¸ºé¢„æµ‹ï¼Œä½†æ˜¯ç”±äºè¾ƒæ·±çš„å·ç§¯å±‚éšå«åœ°åˆ©ç”¨æ¥è‡ªå…ˆå‰å±‚çš„ä¿¡æ¯æ¥å­¦ä¹ æ›´å¤šçš„é«˜çº§ç‰¹å¾ï¼Œå› æ­¤å®ƒä»¬æ— æ³•è®¿é—®å¦‚ä½•æ£€æµ‹åŒ…å«åœ¨å…ˆå‰å±‚ä¸­çš„è¾ƒä½çº§ç‰¹å¾çš„ä¿¡æ¯ï¼›è¿™å¯èƒ½å¯¼è‡´æ£€æµ‹è¾ƒå°å¯¹è±¡æ—¶æ€§èƒ½ä¸ä½³ã€‚

ç”±äºè¿™ä¸ªåŸå› ï¼Œè‡ªä¸Šè€Œä¸‹çš„è·¯å¾„å’Œæ¨ªå‘è¿æ¥è¢«æ·»åŠ åˆ°å¸¸è§„çš„è‡ªä¸‹è€Œä¸Šçš„è·¯å¾„(å›æ—‹å±‚çš„æ­£å¸¸æµåŠ¨)ã€‚è‡ªä¸Šè€Œä¸‹çš„è·¯å¾„é€šè¿‡ä»æ›´é«˜çš„é‡‘å­—å¡”ç­‰çº§å‘ä¸Šé‡‡æ ·ç©ºé—´ä¸Šæ›´ç²—ç³™ä½†è¯­ä¹‰ä¸Šæ›´å¼ºçš„ç‰¹å¾åœ°å›¾æ¥äº§ç”Ÿæ›´é«˜åˆ†è¾¨ç‡çš„ç‰¹å¾ã€‚ç„¶åï¼Œé€šè¿‡æ¨ªå‘è¿æ¥ï¼Œç”¨è‡ªä¸‹è€Œä¸Šè·¯å¾„çš„ç‰¹å¾å¢å¼ºè¿™äº›ç‰¹å¾ã€‚è‡ªåº•å‘ä¸Šçš„ç‰¹å¾æ˜ å°„å…·æœ‰è¾ƒä½å±‚æ¬¡çš„è¯­ä¹‰ï¼Œä½†æ˜¯å®ƒçš„æ¿€æ´»è¢«æ›´ç²¾ç¡®åœ°å®šä½ï¼Œå› ä¸ºå®ƒè¢«äºŒæ¬¡æŠ½æ ·çš„æ¬¡æ•°æ›´å°‘ã€‚

æ€»ä¹‹ï¼ŒFPNs åœ¨å¤šä¸ªå°ºåº¦ä¸Šæä¾›äº†è¯­ä¹‰å¼ºçš„ç‰¹å¾ï¼Œè¿™ä½¿å¾—å®ƒä»¬éå¸¸é€‚åˆäºå¯¹è±¡æ£€æµ‹ã€‚ä¸‹å›¾æ˜¾ç¤ºäº† YOLOv7 åœ¨å…¶ FPN ä¸­å®ç°çš„è¿æ¥:

![](img/320764cf616115c9a7a0fe6613c049c7.png)

*yolov 7 ç³»åˆ—ç‰¹å¾æè®®ç½‘ç»œæ¶æ„çš„è¡¨ç¤ºã€‚æ¥æº:* [*YOLOv7 çº¸*](https://arxiv.org/abs/2207.02696) *ã€‚*

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªâ€œæ­£å¸¸æ¨¡å‹â€å’Œä¸€ä¸ªâ€œå¸¦è¾…åŠ©å¤´çš„æ¨¡å‹â€ã€‚è¿™æ˜¯å› ä¸º YOLOv7 å®¶æ—ä¸­ä¸€äº›ä½“å‹è¾ƒå¤§çš„è½¦å‹åœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº†æ·±åº¦ç›‘ç£ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ºäº†æ›´å¥½åœ°å­¦ä¹ ä»»åŠ¡ï¼Œä»–ä»¬åˆ©ç”¨äº†æŸå¤±ä¸­æ›´æ·±å±‚çš„è¾“å‡ºã€‚ç¨åæˆ‘ä»¬å°†è¿›ä¸€æ­¥æ¢è®¨è¿™ä¸€ç‚¹ã€‚

ä»å›¾åƒä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° FPN ä¸­çš„æ¯ä¸ªå›¾å±‚(ä¹Ÿç§°ä¸ºæ¯ä¸ª FPN å¤´)çš„ç‰¹å¾æ¯”ä¾‹æ˜¯å‰ä¸€ä¸ªå›¾å±‚çš„ä¸€åŠ(æ¯ä¸ªå¼•çº¿å¤´åŠå…¶å¯¹åº”çš„è¾…åŠ©å¤´çš„æ¯”ä¾‹ç›¸åŒ)ã€‚è¿™å¯ä»¥ç†è§£ä¸ºæ¯ä¸€ä¸ªéšåçš„ FPN å¤´éƒ¨â€œçœ‹åˆ°â€çš„ç‰©ä½“éƒ½æ˜¯å‰ä¸€ä¸ªçš„ä¸¤å€å¤§ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»™æ¯ä¸ª FPN å¤´åˆ†é…ä¸åŒ**æ­¥é•¿**(ç½‘æ ¼å•å…ƒè¾¹é•¿)å’Œæ¯”ä¾‹é”šå°ºå¯¸çš„ç½‘æ ¼æ¥åˆ©ç”¨è¿™ä¸€ç‚¹ã€‚

ä¾‹å¦‚ï¼ŒåŸºæœ¬`yolov7`æ¨¡å‹çš„é”šé…ç½®å¦‚ä¸‹æ‰€ç¤º:

![](img/4ef7790b1080b0c4dbccb3d7d929bc5b.png)

*yolov 7 ç³»åˆ—ä¸»æ¨¡å‹ä¸­æ¯ä¸ª fpn å¤´çš„é”šç½‘æ ¼å’Œä¸åŒ(é»˜è®¤)é”šæ¡†å°ºå¯¸çš„å›¾ç¤º*

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬æœ‰é”šæ¡†å¤§å°å’Œç½‘æ ¼ï¼Œè¦†ç›–äº†å®Œå…¨ä¸åŒçš„å°ºåº¦:ä»å¾®å°çš„å¯¹è±¡åˆ°å¯ä»¥å æ®æ•´ä¸ªå›¾åƒçš„å¯¹è±¡ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬ä»æ¦‚å¿µä¸Šç†è§£äº†è¿™äº›æƒ³æ³•ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­å¾—å‡ºçš„ FPN è¾“å‡ºï¼Œè¿™å°†ç”¨äºè®¡ç®—æˆ‘ä»¬çš„æŸå¤±ã€‚

*è¿™äº›ç« èŠ‚ç›´æ¥å–è‡ª* [*åŸ FPN è®ºæ–‡*](https://arxiv.org/abs/1612.03144) *ï¼Œå› ä¸ºæˆ‘ä»¬è§‰å¾—ä¸éœ€è¦è¿›ä¸€æ­¥è§£é‡Šã€‚*

## åˆ†è§£ FPN äº§å‡º

å›æƒ³ä¸€ä¸‹ï¼Œå½“æˆ‘ä»¬ä¹‹å‰è¿›è¡Œé¢„æµ‹æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹çš„`postprocess`æ–¹æ³•å°†åŸå§‹ FPN è¾“å‡ºè½¬æ¢æˆå¯ç”¨çš„è¾¹ç•Œæ¡†ã€‚æ—¢ç„¶æˆ‘ä»¬ç†è§£äº† FPN è¯•å›¾åšä»€ä¹ˆèƒŒåçš„ç›´è§‰ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥è¿™äº›åŸå§‹è¾“å‡ºã€‚

![](img/258adae0d3337c3a157cae2697995127.png)

æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºæ€»æ˜¯ä¸€ä¸ª`List[Tensor]`ï¼Œå…¶ä¸­æ¯ä¸ªç»„ä»¶å¯¹åº”ä¸€ä¸ª FPN çš„å¤´ã€‚å¯¹äºä½¿ç”¨æ·±åº¦ç›‘æ§çš„å‹å·ï¼Œè¾…åŠ©å¤´è¾“å‡ºåœ¨å¯¼è”å¤´è¾“å‡ºä¹‹å(æ¯ä¸€ä¸ªçš„æ•°é‡æ€»æ˜¯ç›¸åŒçš„ï¼Œçº¿å¯¹çš„ä¸¤ä¾§é¡ºåºç›¸åŒ)ã€‚å…¶ä½™çš„ï¼ŒåŒ…æ‹¬æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨çš„ï¼Œåªæœ‰å¯¼è”å¤´è¾“å‡ºã€‚

![](img/89debe6fbfe9cdd8162689801c288de5.png)

æ£€æŸ¥æ¯ä¸ª FPN è¾“å‡ºçš„å½¢çŠ¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸ªè¾“å‡ºéƒ½æœ‰ä»¥ä¸‹å°ºå¯¸:

```
[n_images, n_anchor_sizes, n_grid_rows, n_grid_cols, n_features]
```

å…¶ä¸­:

*   `n_images` â€”æ‰¹æ¬¡ä¸­å›¾åƒçš„æ•°é‡(æ‰¹æ¬¡å¤§å°)ã€‚
*   `n_anchor_sizes` -ä¸å¤´éƒ¨ç›¸å…³çš„é”šå°ºå¯¸(é€šå¸¸ä¸º 3)ã€‚
*   `n_grid_rows`â€”â€”å‚ç›´æ–¹å‘ä¸Šé”šçš„æ•°é‡ï¼Œ`img_height / stride`ã€‚
*   `n_grid_cols` -æ°´å¹³æ–¹å‘çš„é”šæ•°é‡ï¼Œ`img_width / stride`ã€‚
*   `n_features`-`5 + num_classes`-
    -`cx`-é”šç®±ä¸­å¿ƒæ°´å¹³æ ¡æ­£ã€‚
    - `cy` -é”šç®±ä¸­å¿ƒå‚ç›´æ ¡æ­£ã€‚
    - `w` -é”šç®±å®½åº¦ä¿®æ­£ã€‚
    - `h` -é”šç®±é«˜åº¦ä¿®æ­£ã€‚
    - `obj_score` -ä¸é”šç›’å†…åŒ…å«å¯¹è±¡çš„æ¦‚ç‡æˆæ¯”ä¾‹çš„åˆ†æ•°ã€‚
    - `cls_score` -æ¯ç±»ä¸€ä¸ªï¼Œå¾—åˆ†ä¸è¯¥å¯¹è±¡æ‰€å±ç±»åˆ«çš„æ¦‚ç‡æˆæ¯”ä¾‹ã€‚

å½“è¿™äº›è¾“å‡ºåœ¨åå¤„ç†æœŸé—´è¢«æ˜ å°„æˆæœ‰ç”¨çš„é¢„æµ‹æ—¶ï¼Œæˆ‘ä»¬åº”ç”¨ä»¥ä¸‹æ“ä½œ:

*   `cx`ã€`cy`:`final = 2 * sigmoid(initial) - 0.5`
    *[(âˆã€âˆ)ã€(âˆã€âˆ)]â†’[(0.5ï¼Œ1.5)ï¼Œ(-0.5ï¼Œ1.5)]*
    -æ¨¡å‹åªèƒ½å°†é”šç‚¹ä¸­å¿ƒä» 0.5 ä¸ªå•å…ƒåå‘å‰ç§»åŠ¨ 1.5 ä¸ªå•å…ƒã€‚è¯·æ³¨æ„ï¼Œå¯¹äºæŸå¤±(å³ï¼Œå½“æˆ‘ä»¬è®­ç»ƒæ—¶)ï¼Œæˆ‘ä»¬ä½¿ç”¨ç½‘æ ¼åæ ‡ã€‚
*   `w`ã€`h`:`final = (2 * sigmoid(initial)**2`
    *[(âˆã€âˆ)ã€(âˆã€âˆ)] â†’ [(0ï¼Œ4)ï¼Œ(0ï¼Œ4)]*
    â€”â€”æ¨¡å‹å¯ä»¥ä»»æ„å˜å°ï¼Œä½†æœ€å¤šå˜å¤§ 4 å€ã€‚æ›´å¤§çš„ç‰©ä½“ï¼Œåœ¨è¿™ä¸ªèŒƒå›´ä¹‹å¤–ï¼Œå¿…é¡»ç”±ä¸‹ä¸€ä¸ª FPN å¤´é¢„æµ‹ã€‚
*   `obj_score`:`final = sigmoid(initial)`
    *(âˆï¼Œâˆ) â†’ (0ï¼Œ1)*
    -ç¡®ä¿åˆ†æ•°æ˜ å°„åˆ°ä¸€ä¸ªæ¦‚ç‡ã€‚
*   `cls_score`:`final = sigmoid(initial)`
    *(âˆï¼Œâˆ) â†’ (0ï¼Œ1)*
    -ç¡®ä¿åˆ†æ•°æ˜ å°„åˆ°ä¸€ä¸ªæ¦‚ç‡ã€‚

## ä¸­å¿ƒå…ˆéªŒ

ç°åœ¨ï¼Œå¾ˆå®¹æ˜“çœ‹å‡ºï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ¯ä¸ªç½‘æ ¼çš„æ¯ä¸ªå®šä½ç‚¹æ”¾ç½® 3 ä¸ªå®šä½æ¡†ï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°å¾ˆå¤šæ¡†:`3*80*80 + 3*40*40 + 3*20*20=25200`å‡†ç¡®åœ°è¯´ï¼Œæ˜¯æ¯ä¸ª 640x640px å›¾åƒï¼é—®é¢˜æ˜¯ï¼Œè¿™äº›é¢„æµ‹ä¸­çš„å¤§éƒ¨åˆ†éƒ½ä¸ä¼šåŒ…å«ä¸€ä¸ªæˆ‘ä»¬å½’ç±»ä¸ºâ€œèƒŒæ™¯â€çš„ç‰©ä½“ã€‚æ ¹æ®æˆ‘ä»¬éœ€è¦åº”ç”¨äºæ¯ä¸ªé¢„æµ‹çš„æ“ä½œé¡ºåºï¼Œè®¡ç®—å¾ˆå®¹æ˜“å †ç§¯èµ·æ¥å¹¶å‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼

ä¸ºäº†é™ä½é—®é¢˜çš„è®¡ç®—æˆæœ¬ï¼ŒYOLOv7 loss é¦–å…ˆæ‰¾åˆ°å¯èƒ½ä¸æ¯ä¸ªç›®æ ‡æ¡†åŒ¹é…çš„é”šæ¡†ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œä¸åŒçš„å¤„ç†â€”â€”è¿™äº›é”šæ¡†è¢«ç§°ä¸º**ä¸­å¿ƒä¼˜å…ˆ**é”šæ¡†ã€‚è¯¥è¿‡ç¨‹åº”ç”¨äºæ¯ä¸ª FPN å¤´ï¼Œå¯¹äºæ¯ä¸ªç›®æ ‡æ¡†ï¼Œä¸€æ¬¡æ‰¹é‡è·¨è¶Šæ‰€æœ‰å›¾åƒã€‚

æ¯ä¸ªé”šâ€”â€”æˆ‘ä»¬ç½‘æ ¼ä¸­çš„åæ ‡â€”â€”å®šä¹‰ä¸€ä¸ªç½‘æ ¼å•å…ƒï¼›å…¶ä¸­æˆ‘ä»¬è®¤ä¸ºé”šç‚¹ä½äºå…¶å¯¹åº”ç½‘æ ¼å•å…ƒçš„å·¦ä¸Šæ–¹ã€‚éšåï¼Œæ¯ä¸ªå•å…ƒæ ¼(è¾¹ç•Œä¸Šçš„å•å…ƒæ ¼é™¤å¤–)æœ‰ 4 ä¸ªç›¸é‚»çš„å•å…ƒæ ¼(ä¸Šã€ä¸‹ã€å·¦ã€å³)ã€‚å¯¹äºæ¯ä¸ª FPN å¤´éƒ¨ï¼Œæ¯ä¸ªç›®æ ‡æ¡†ä½äºç½‘æ ¼å•å…ƒå†…çš„æŸä¸ªä½ç½®ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸‹é¢çš„ç½‘æ ¼ï¼Œç›®æ ‡æ¡†çš„ä¸­å¿ƒç”¨ä¸€ä¸ª`*`è¡¨ç¤º:

![](img/2cd849bc3480bc56a26a8ff1a0cc765c.png)

åŸºäºæ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒæ–¹å¼ï¼Œå®ƒèƒ½å¤Ÿè¾“å‡ºçš„`x`å’Œ`y`ä¿®æ­£é‡åœ¨`[-0.5, 1.5]`ç½‘æ ¼å•å…ƒçš„èŒƒå›´å†…ã€‚å› æ­¤ï¼Œåªæœ‰æœ€è¿‘é”šç›’çš„å­é›†èƒ½å¤ŸåŒ¹é…ç›®æ ‡ä¸­å¿ƒã€‚æˆ‘ä»¬é€‰æ‹©è¿™äº›é”šæ¡†ä¸­çš„ä¸€äº›æ¥ä»£è¡¨ç›®æ ‡æ¡†çš„ä¹‹å‰çš„**ä¸­å¿ƒã€‚**

*   å¯¹äºå¼•çº¿å¤´ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰ä½¿ç”¨**ç²¾ç»†ä¸­å¿ƒï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´æœ‰é’ˆå¯¹æ€§çš„é€‰æ‹©ã€‚è¿™ç”±æ¯ä¸ªå¤´**çš„ **3 ä¸ªé”šç»„æˆ:é”šä¸åŒ…å«ç›®æ ‡æ¡†ä¸­å¿ƒçš„å•å…ƒç›¸å…³è”ï¼Œæ—è¾¹æ˜¯ç¦»ç›®æ ‡æ¡†ä¸­å¿ƒæœ€è¿‘çš„ 2 ä¸ªç½‘æ ¼å•å…ƒçš„é”šã€‚åœ¨å›¾ä¸­ï¼Œä¸­å¿ƒå‰é”šæ ‡æœ‰`X`ã€‚**

![](img/4f2bdc897be730c6fcfe27ae3f25b650.png)

é“…æ£€æµ‹å¤´çš„é€‰å®šä¸­å¿ƒå…ˆéªŒ

*   å¯¹äºè¾…åŠ©å¤´(å¯¹äºä½¿ç”¨æ·±åº¦ç›‘æ§çš„å‹å·)ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰ä½¿ç”¨**ç²—ä¸­å¿ƒï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ€§è¾ƒä½çš„é€‰æ‹©ã€‚è¿™ç”±æ¯ä¸ªå¤´çš„ **5 ä¸ªé”šç»„æˆ**:åŒ…å«ç›®æ ‡æ¡†ä¸­å¿ƒçš„å•å…ƒçš„é”šï¼Œåœ¨æ‰€æœ‰ 4 ä¸ªç›¸é‚»ç½‘æ ¼å•å…ƒæ—è¾¹ã€‚**

![](img/4fcf04a1186b4c3cc16cc6c09ac4eda4.png)

è¾…åŠ©æ¢æµ‹å¤´çš„é€‰å®šä¸­å¿ƒå…ˆéªŒ

è¿™ç§ç»†ä¸ç²—çš„åŒºåˆ†èƒŒåçš„æ¨ç†æ˜¯ï¼Œè¾…åŠ©å¤´çš„å­¦ä¹ èƒ½åŠ›ä½äºé¢†å¤´å¤´ï¼Œå› ä¸ºé¢†å¤´å¤´åœ¨ç½‘ç»œä¸­çš„ä½ç½®æ›´æ·±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°½é‡é¿å…ä»è¾…åŠ©å¤´å¯ä»¥å­¦ä¹ çš„åœ°æ–¹é™åˆ¶å¤ªå¤šï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šä¸¢å¤±æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

ç±»ä¼¼äºåæ ‡æ ¡æ­£ï¼Œæ¨¡å‹åªèƒ½åœ¨é—´éš”`[0, 4]`ä¸­å¯¹æ¯ä¸ªé”šæ¡†çš„å®½åº¦å’Œé«˜åº¦åº”ç”¨ä¹˜æ³•ä¿®æ”¹å™¨ã€‚è¿™æ„å‘³ç€ï¼Œå®ƒæœ€å¤šå¯ä»¥ä½¿é”šç›’çš„ä¾§é¢æ‰©å¤§ 4 å€ã€‚å› æ­¤ï¼Œä»è¢«é€‰ä¸ºä¸­å¿ƒå…ˆéªŒçš„é”šæ¡†ä¸­ï¼Œæˆ‘ä»¬è¿‡æ»¤é‚£äº›æ¯”ç›®æ ‡æ¡†å¤§æˆ–å° 4 å€çš„é”šæ¡†ã€‚

æ€»ä¹‹ï¼Œä¸­å¿ƒå…ˆéªŒç”±é”šæ¡†ç»„æˆï¼Œé”šæ¡†çš„é”šè¶³å¤Ÿé è¿‘ç›®æ ‡æ¡†ä¸­å¿ƒï¼Œå¹¶ä¸”å…¶è¾¹ä¸å¤ªåç¦»ç›®æ ‡æ¡†è¾¹å°ºå¯¸ã€‚

## æœ€ä¼˜è¿è¾“åˆ†é…

è¯„ä¼°å¯¹è±¡æ£€æµ‹æ¨¡å‹æ—¶çš„å›°éš¾ä¹‹ä¸€æ˜¯èƒ½å¤Ÿå°†é¢„æµ‹æ¡†ä¸ç›®æ ‡æ¡†è¿›è¡ŒåŒ¹é…ï¼Œä»¥ä¾¿é‡åŒ–æ¨¡å‹æ˜¯å¦åšå¾—å¥½ã€‚

æœ€ç®€å•çš„æ–¹æ³•æ˜¯å®šä¹‰ Union (IoU)é˜ˆå€¼ä¸Šçš„[äº¤é›†ï¼Œå¹¶åŸºäºæ­¤åšå‡ºå†³å®šã€‚è™½ç„¶è¿™é€šå¸¸æ˜¯å¯è¡Œçš„ï¼Œä½†å½“å­˜åœ¨é®æŒ¡ã€æ¨¡ç³Šæˆ–å¤šä¸ªå¯¹è±¡éå¸¸é è¿‘æ—¶ï¼Œå°±ä¼šå‡ºç°é—®é¢˜ã€‚](https://en.wikipedia.org/wiki/Jaccard_index)[æœ€ä¼˜ä¼ è¾“åˆ†é…](https://arxiv.org/abs/2103.14259) (OTA)æ—¨åœ¨é€šè¿‡å°†æ ‡ç­¾åˆ†é…è§†ä¸ºæ¯ä¸ªå›¾åƒçš„å…¨å±€ä¼˜åŒ–é—®é¢˜æ¥è§£å†³å…¶ä¸­ä¸€äº›é—®é¢˜ã€‚

ä¸»è¦ç›´è§‰åœ¨äºå°†æ¯ä¸ªç›®æ ‡æ¡†è§†ä¸º`k`æ­£æ ‡ç­¾åˆ†é…çš„æä¾›è€…ï¼Œè€Œå°†æ¯ä¸ªé¢„æµ‹æ¡†è§†ä¸ºä¸€ä¸ªæ­£æ ‡ç­¾åˆ†é…æˆ–ä¸€ä¸ªèƒŒæ™¯åˆ†é…çš„éœ€æ±‚è€…ã€‚`k`æ˜¯åŠ¨æ€çš„ï¼Œä¾èµ–äºæ¯ä¸ªç›®æ ‡æ¡†ã€‚ç„¶åï¼Œå°†ä¸€ä¸ªæ­£æ ‡ç­¾åˆ†é…ä»ç›®æ ‡ç›’ä¼ é€åˆ°é¢„æµ‹ç›’å…·æœ‰åŸºäºåˆ†ç±»å’Œå›å½’çš„æˆæœ¬ã€‚æœ€åï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªè¿è¾“è®¡åˆ’(æ ‡ç­¾åˆ†é…)ï¼Œä½¿å›¾åƒçš„æ€»æˆæœ¬æœ€å°åŒ–ã€‚

è¿™å¯ä»¥ä½¿ç”¨ç°æˆçš„è§£ç®—å™¨æ¥å®Œæˆï¼Œä½† YOLOv7 å®ç°äº† **simOTA** (åœ¨ [YOLOX è®ºæ–‡](https://arxiv.org/abs/2107.08430)ä¸­ä»‹ç»)ï¼Œè¿™æ˜¯ OTA é—®é¢˜çš„ç®€åŒ–ç‰ˆæœ¬ã€‚ä»¥å‡å°‘æ ‡ç­¾åˆ†é…çš„è®¡ç®—æˆæœ¬ä¸ºç›®æ ‡ï¼Œå®ƒä¸ºæ¯ä¸ªç›®æ ‡åˆ†é…å…·æœ‰æœ€ä½è¿è¾“æˆæœ¬çš„ğ‘˜é¢„æµ‹ç›’ï¼Œè€Œä¸æ˜¯è§£å†³å…¨å±€é—®é¢˜ã€‚ä¸­å¿ƒå…ˆéªŒæ¡†è¢«ç”¨ä½œè¯¥è¿‡ç¨‹çš„å€™é€‰è€…ã€‚

è¿™æœ‰åŠ©äºæˆ‘ä»¬è¿›ä¸€æ­¥ç­›é€‰å¯èƒ½ä¸çœŸå®ç›®æ ‡ç›¸åŒ¹é…çš„æ¨¡å‹è¾“å‡ºæ•°é‡ã€‚

## YOLOv7 æŸå¤±ç®—æ³•

æ—¢ç„¶æˆ‘ä»¬å·²ç»ä»‹ç»äº† YOLOv7 æŸè€—è®¡ç®—ä¸­ä½¿ç”¨çš„æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä½¿ç”¨çš„ç®—æ³•åˆ†è§£ä¸ºä»¥ä¸‹æ­¥éª¤:

1.  å¯¹äºæ¯ä¸ª FPN å¤´(æˆ–æ¯ä¸ª FPN å¤´å’Œè¾…åŠ© FPN å¤´å¯¹ï¼Œå¦‚æœä½¿ç”¨è¾…åŠ©å¤´):

*   æ‰¾åˆ°ä¸­å¿ƒä¼˜å…ˆé”šç›’ã€‚
*   é€šè¿‡ simOTA ç®—æ³•ä¼˜åŒ–å€™é€‰é€‰æ‹©ã€‚ä¸ºæ­¤ï¼Œè¯·å§‹ç»ˆä½¿ç”¨é“… FPN å¤´ã€‚
*   ä½¿ç”¨é¢„æµ‹çš„å¯¹è±¡æ¦‚ç‡å’Œ Union ä¸Šçš„[å®Œå…¨äº¤é›†](https://arxiv.org/abs/1911.08287) (CIoU)ä¹‹é—´çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±è·å¾—**å¯¹è±¡æŸå¤±**åˆ†æ•°ï¼Œå°†åŒ¹é…çš„ç›®æ ‡ä½œä¸ºåŸºç¡€äº‹å®ã€‚å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿™æ˜¯ 0ã€‚
*   å¦‚æœæœ‰é€‰æ‹©çš„å€™é€‰é”šç›’ï¼Œä¹Ÿè®¡ç®—(å¦åˆ™éƒ½æ˜¯ 0):
    -**ç›’(æˆ–å›å½’)æŸå¤±**ï¼Œå®šä¹‰ä¸ºæ‰€æœ‰å€™é€‰é”šç›’ä¸å…¶åŒ¹é…ç›®æ ‡ä¹‹é—´çš„`mean(1 - CIoU)`ã€‚
    -**åˆ†ç±»æŸå¤±**ï¼Œä½¿ç”¨æ¯ä¸ªé”šç›’çš„é¢„æµ‹ç±»åˆ«æ¦‚ç‡å’ŒåŒ¹é…ç›®æ ‡çš„çœŸå®ç±»åˆ«çš„ç‹¬çƒ­ç¼–ç å‘é‡ä¹‹é—´çš„äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±ã€‚
*   å¦‚æœæ¨¡å‹ä½¿ç”¨è¾…åŠ©å¤´ï¼Œå°†ä»è¾…åŠ©å¤´è·å¾—çš„æ¯ä¸ªåˆ†é‡åŠ åˆ°ç›¸åº”çš„ä¸»æŸè€—åˆ†é‡ä¸Š(å³`x = x + aux_wt*aux_x`)ã€‚è´¡çŒ®æƒé‡(`aux_wt`)ç”±é¢„å®šä¹‰çš„è¶…å‚æ•°å®šä¹‰ã€‚
*   å°†ç›®æ ‡æŸå¤±ä¹˜ä»¥ç›¸åº”çš„ FPN å¤´æƒé‡(é¢„å®šä¹‰çš„è¶…å‚æ•°)ã€‚

2.å°†æ¯ä¸ªæŸå¤±æˆåˆ†(å®¢ä½“ã€åˆ†ç±»ã€å›å½’)ä¹˜ä»¥å…¶è´¡çŒ®æƒé‡(é¢„å®šä¹‰çš„è¶…å‚æ•°)ã€‚

3.åˆè®¡å·²ç»åŠ æƒçš„æŸå¤±éƒ¨åˆ†ã€‚

4.å°†æœ€ç»ˆæŸå¤±å€¼ä¹˜ä»¥æ‰¹é‡ã€‚

ä½œä¸ºä¸€ä¸ªæŠ€æœ¯ç»†èŠ‚ï¼Œè¯„ä¼°æœŸé—´æŠ¥å‘Šçš„æŸå¤±é€šè¿‡è·³è¿‡ simOTA å’Œä»ä¸ä½¿ç”¨è¾…åŠ©å¤´åœ¨è®¡ç®—ä¸Šæ›´ä¾¿å®œï¼Œå³ä½¿å¯¹äºæ—¶å°šæ·±åº¦ç›‘æ§çš„æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

è™½ç„¶è¿™ä¸ªè¿‡ç¨‹åŒ…å«å¾ˆå¤šå¤æ‚æ€§ï¼Œä½†åœ¨å®è·µä¸­ï¼Œè¿™äº›éƒ½è¢«å°è£…åœ¨ä¸€ä¸ªç±»ä¸­ï¼Œè¯¥ç±»å¯ä»¥å¦‚ä¸‹æ‰€ç¤ºåˆ›å»º:

![](img/e9de6ef192c82bc1aced7eb454ec1731.png)

# å¾®è°ƒæ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œä»¥åŠæˆ‘ä»¬çš„æŸå¤±å‡½æ•°å¦‚ä½•è¡¡é‡è¿™äº›é¢„æµ‹çš„è´¨é‡ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ ¹æ®è‡ªå®šä¹‰ä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†è·å¾—è®ºæ–‡ä¸­æŠ¥å‘Šçš„æ€§èƒ½æ°´å¹³ï¼ŒYOLOv7 ä½¿ç”¨å„ç§æŠ€æœ¯è¿›è¡Œäº†è®­ç»ƒã€‚ç„¶è€Œï¼Œå‡ºäºæˆ‘ä»¬çš„ç›®çš„ï¼Œåœ¨é€æ­¥å¼•å…¥ä¸åŒçš„æŠ€æœ¯ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ä»æ‰€éœ€çš„å°½å¯èƒ½å°‘çš„è®­ç»ƒå¾ªç¯å¼€å§‹ã€‚

ä¸ºäº†å¤„ç†è®­ç»ƒå¾ªç¯çš„æ ·æ¿æ–‡ä»¶ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ [PyTorch åŠ é€Ÿçš„](https://github.com/Chris-hughes10/pytorch-accelerated)ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåªå®šä¹‰ä¸æˆ‘ä»¬çš„ç”¨ä¾‹ç›¸å…³çš„è®­ç»ƒå¾ªç¯çš„éƒ¨åˆ†ï¼Œè€Œä¸å¿…ç®¡ç†æ‰€æœ‰çš„æ ·æ¿æ–‡ä»¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¦†ç›–[é»˜è®¤ PyTorch åŠ é€Ÿ](https://pytorch-accelerated.readthedocs.io/en/latest/trainer.html#pytorch_accelerated.trainer.Trainer) `[Trainer](https://pytorch-accelerated.readthedocs.io/en/latest/trainer.html#pytorch_accelerated.trainer.Trainer)`çš„éƒ¨åˆ†å†…å®¹ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç‰¹å®šäº YOLOv7 å‹å·çš„è®­ç»ƒå™¨ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/trainer.py

from pytorch_accelerated import Trainer

class Yolov7Trainer(Trainer):
    YOLO7_PADDING_VALUE = -2.0

    def __init__(
        self,
        model,
        loss_func,
        optimizer,
        callbacks,
        filter_eval_predictions_fn=None,
    ):
        super().__init__(
            model=model, loss_func=loss_func, optimizer=optimizer, callbacks=callbacks
        )
        self.filter_eval_predictions = filter_eval_predictions_fn

    def training_run_start(self):
        self.loss_func.to(self.device)

    def evaluation_run_start(self):
        self.loss_func.to(self.device)

    def train_epoch_start(self):
        super().train_epoch_start()
        self.loss_func.train()

    def eval_epoch_start(self):
        super().eval_epoch_start()
        self.loss_func.eval()

    def calculate_train_batch_loss(self, batch) -> dict:
        images, labels = batch[0], batch[1]

        fpn_heads_outputs = self.model(images)
        loss, _ = self.loss_func(
            fpn_heads_outputs=fpn_heads_outputs, targets=labels, images=images
        )

        return {
            "loss": loss,
            "model_outputs": fpn_heads_outputs,
            "batch_size": images.size(0),
        }

    def calculate_eval_batch_loss(self, batch) -> dict:
        with torch.no_grad():
            images, labels, image_ids, original_image_sizes = (
                batch[0],
                batch[1],
                batch[2],
                batch[3].cpu(),
            )
            fpn_heads_outputs = self.model(images)
            val_loss, _ = self.loss_func(
                fpn_heads_outputs=fpn_heads_outputs, targets=labels
            )

            preds = self.model.postprocess(fpn_heads_outputs, conf_thres=0.001)

            if self.filter_eval_predictions is not None:
                preds = self.filter_eval_predictions(preds)

            resized_image_sizes = torch.as_tensor(
                images.shape[2:], device=original_image_sizes.device
            )[None].repeat(len(preds), 1)

        formatted_predictions = self.get_formatted_preds(
            image_ids, preds, original_image_sizes, resized_image_sizes
        )

        gathered_predictions = (
            self.gather(formatted_predictions, padding_value=self.YOLO7_PADDING_VALUE)
            .detach()
            .cpu()
        )

        return {
            "loss": val_loss,
            "model_outputs": fpn_heads_outputs,
            "predictions": gathered_predictions,
            "batch_size": images.size(0),
        }

    def get_formatted_preds(
        self, image_ids, preds, original_image_sizes, resized_image_sizes
    ):
        """
        scale bboxes to original image dimensions, and associate image id with predictions
        """
        formatted_preds = []
        for i, (image_id, image_preds) in enumerate(zip(image_ids, preds)):
            # image_id, x1, y1, x2, y2, score, class_id
            formatted_preds.append(
                torch.cat(
                    (
                        scale_bboxes_to_original_image_size(
                            image_preds[:, :4],
                            resized_hw=resized_image_sizes[i],
                            original_hw=original_image_sizes[i],
                            is_padded=True,
                        ),
                        image_preds[:, 4:],
                        image_id.repeat(image_preds.shape[0])[None].T,
                    ),
                    1,
                )
            )

        if not formatted_preds:
            # if no predictions, create placeholder so that it can be gathered across processes
            stacked_preds = torch.tensor(
                [self.YOLO7_PADDING_VALUE] * 7, device=self.device
            )[None]
        else:
            stacked_preds = torch.vstack(formatted_preds)

        return stacked_preds
```

æˆ‘ä»¬çš„è®­ç»ƒæ­¥éª¤éå¸¸ç®€å•ï¼Œå”¯ä¸€çš„ä¿®æ”¹æ˜¯æˆ‘ä»¬éœ€è¦ä»è¿”å›çš„å­—å…¸ä¸­æå–æ€»æŸå¤±ã€‚å¯¹äºè¯„ä¼°æ­¥éª¤ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—æŸå¤±ï¼Œç„¶åæ£€ç´¢æ£€æµ‹ã€‚

## è¯„ä¼°é€»è¾‘

ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[å¹³å‡ç²¾åº¦](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)(mAP)ï¼›å¯¹è±¡æ£€æµ‹ä»»åŠ¡çš„æ ‡å‡†åº¦é‡ã€‚ä¹Ÿè®¸æœ€å¹¿æ³›ä½¿ç”¨(å’Œä¿¡ä»»)çš„ mAP å®ç°æ˜¯åŒ…å«åœ¨ [PyCOCOTools åŒ…](https://pypi.org/project/pycocotools/)ä¸­çš„ç±»ï¼Œå®ƒç”¨äºè¯„ä¼°å®˜æ–¹ COCO æ’è¡Œæ¦œæäº¤ã€‚

ç„¶è€Œï¼Œç”±äºå®ƒæ²¡æœ‰æœ€å…·åˆ›æ„çš„ç•Œé¢ï¼Œæˆ‘ä»¬[å›´ç»•å®ƒåˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„åŒ…è£…å™¨](https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/evaluation/coco_evaluator.py)ï¼Œä½¿å®ƒæ›´åŠ ç”¨æˆ·å‹å¥½ã€‚æ­¤å¤–ï¼Œå¯¹äº COCO ç«èµ›æ’è¡Œæ¦œä¹‹å¤–çš„è®¸å¤šæ¡ˆä¾‹ï¼Œä½¿ç”¨å›ºå®šçš„å€Ÿæ®é˜ˆå€¼è¯„ä¼°é¢„æµ‹å¯èƒ½æ˜¯æœ‰åˆ©çš„ï¼Œè€Œä¸æ˜¯é»˜è®¤ä½¿ç”¨çš„å€Ÿæ®èŒƒå›´ï¼Œæˆ‘ä»¬åœ¨è¯„ä¼°å™¨ä¸­æ·»åŠ äº†ä¸€ä¸ªé€‰é¡¹æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚

ä¸ºäº†å°è£…æˆ‘ä»¬çš„è¯„ä¼°é€»è¾‘ä»¥ä¾¿åœ¨è®­ç»ƒä¸­ä½¿ç”¨ï¼Œè®©æˆ‘ä»¬[ä¸ºè¿™ä¸ª](https://pytorch-accelerated.readthedocs.io/en/latest/callbacks.html#creating-new-callbacks)åˆ›å»ºä¸€ä¸ªå›è°ƒï¼›å…¶å°†åœ¨æ¯ä¸ªè¯„ä¼°æ­¥éª¤ç»“æŸæ—¶è¢«æ›´æ–°ï¼Œç„¶ååœ¨æ¯ä¸ªè¯„ä¼°æ—¶æœŸç»“æŸæ—¶è¢«è®¡ç®—ã€‚

```
 # https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/evaluation/calculate_map_callback.py

from pytorch_accelerated.callbacks import TrainerCallback

class CalculateMeanAveragePrecisionCallback(TrainerCallback):
    """
    A callback which accumulates predictions made during an epoch and uses these to calculate the Mean Average Precision
    from the given targets.

    .. Note:: If using distributed training or evaluation, this callback assumes that predictions have been gathered
    from all processes during the evaluation step of the main training loop.
    """

    def __init__(
        self,
        targets_json,
        iou_threshold=None,
        save_predictions_output_dir_path=None,
        verbose=False,
    ):
        """
        :param targets_json: a COCO-formatted dictionary with the keys "images", "categories" and "annotations"
        :param iou_threshold: If set, the IoU threshold at which mAP will be calculated. Otherwise, the COCO default range of IoU thresholds will be used.
        :param save_predictions_output_dir_path: If provided, the path to which the accumulated predictions will be saved, in coco json format.
        :param verbose: If True, display the output provided by pycocotools, containing the average precision and recall across a range of box sizes.
        """
        self.evaluator = COCOMeanAveragePrecision(iou_threshold)
        self.targets_json = targets_json
        self.verbose = verbose
        self.save_predictions_path = (
            Path(save_predictions_output_dir_path)
            if save_predictions_output_dir_path is not None
            else None
        )

        self.eval_predictions = []
        self.image_ids = set()

    def on_eval_step_end(self, trainer, batch, batch_output, **kwargs):
        predictions = batch_output["predictions"]
        if len(predictions) > 0:
            self._update(predictions)

    def on_eval_epoch_end(self, trainer, **kwargs):
        preds_df = pd.DataFrame(
            self.eval_predictions,
            columns=[
                XMIN_COL,
                YMIN_COL,
                XMAX_COL,
                YMAX_COL,
                SCORE_COL,
                CLASS_ID_COL,
                IMAGE_ID_COL,
            ],
        )

        predictions_json = self.evaluator.create_predictions_coco_json_from_df(preds_df)
        self._save_predictions(trainer, predictions_json)

        if self.verbose and trainer.run_config.is_local_process_zero:
            self.evaluator.verbose = True

        map_ = self.evaluator.compute(self.targets_json, predictions_json)
        trainer.run_history.update_metric(f"map", map_)

        self._reset()

    @classmethod
    def create_from_targets_df(
        cls,
        targets_df,
        image_ids,
        iou_threshold=None,
        save_predictions_output_dir_path=None,
        verbose=False,
    ):
        """
        Create an instance of :class:`CalculateMeanAveragePrecisionCallback` from a dataframe containing the ground
        truth targets and a collections of all image ids in the dataset.

        :param targets_df: DF w/ cols: ["image_id", "xmin", "ymin", "xmax", "ymax", "class_id"]
        :param image_ids: A collection of all image ids in the dataset, including those without annotations.
        :param iou_threshold:  If set, the IoU threshold at which mAP will be calculated. Otherwise, the COCO default range of IoU thresholds will be used.
        :param save_predictions_output_dir_path: If provided, the path to which the accumulated predictions will be saved, in coco json format.
        :param verbose: If True, display the output provided by pycocotools, containing the average precision and recall across a range of box sizes.
        :return: An instance of :class:`CalculateMeanAveragePrecisionCallback`
        """

        targets_json = COCOMeanAveragePrecision.create_targets_coco_json_from_df(
            targets_df, image_ids
        )

        return cls(
            targets_json=targets_json,
            iou_threshold=iou_threshold,
            save_predictions_output_dir_path=save_predictions_output_dir_path,
            verbose=verbose,
        )

    def _remove_seen(self, labels):
        """
        Remove any image id that has already been seen during the evaluation epoch. This can arise when performing
        distributed evaluation on a dataset where the batch size does not evenly divide the number of samples.

        """
        image_ids = labels[:, -1].tolist()

        # remove any image_idx that has already been seen
        # this can arise from distributed training where batch size does not evenly divide dataset
        seen_id_mask = torch.as_tensor(
            [False if idx not in self.image_ids else True for idx in image_ids]
        )

        if seen_id_mask.all():
            # no update required as all ids already seen this pass
            return []
        elif seen_id_mask.any():  # at least one True
            # remove predictions for images already seen this pass
            labels = labels[~seen_id_mask]

        return labels

    def _update(self, predictions):
        filtered_predictions = self._remove_seen(predictions)

        if len(filtered_predictions) > 0:
            self.eval_predictions.extend(filtered_predictions.tolist())
            updated_ids = filtered_predictions[:, -1].unique().tolist()
            self.image_ids.update(updated_ids)

    def _reset(self):
        self.image_ids = set()
        self.eval_predictions = []

    def _save_predictions(self, trainer, predictions_json):
        if (
            self.save_predictions_path is not None
            and trainer.run_config.is_world_process_zero
        ):
            with open(self.save_predictions_path / "predictions.json", "w") as f:
                json.dump(predictions_json, f)
```

ç°åœ¨ï¼Œæˆ‘ä»¬æ‰€è¦åšçš„å°±æ˜¯å°†æˆ‘ä»¬çš„å›è°ƒæ’å…¥æˆ‘ä»¬çš„è®­ç»ƒå™¨ï¼Œæˆ‘ä»¬çš„åœ°å›¾å°†åœ¨æ¯ä¸ªæ—¶æœŸè¢«è®°å½•ä¸‹æ¥ï¼

## è·‘æ­¥è®­ç»ƒ

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†ç›®å‰ä¸ºæ­¢çœ‹åˆ°çš„æ‰€æœ‰å†…å®¹æ”¾å…¥ä¸€ä¸ªç®€å•çš„åŸ¹è®­è„šæœ¬ä¸­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç®€å•çš„è®­ç»ƒæ–¹æ³•ï¼Œå®ƒé€‚ç”¨äºå„ç§ä»»åŠ¡ï¼Œå¹¶ä¸”è¿›è¡Œäº†æœ€å°çš„è¶…å‚æ•°è°ƒæ•´ã€‚

å› ä¸ºæˆ‘ä»¬æ³¨æ„åˆ°è¿™ä¸ªæ•°æ®é›†çš„åŸºç¡€äº‹å®æ¡†å¯ä»¥åŒ…å«å¯¹è±¡å‘¨å›´ç›¸å½“å¤šçš„ç©ºé—´ï¼Œæ‰€ä»¥æˆ‘ä»¬å†³å®šå°†ç”¨äºè¯„ä¼°çš„ IoU é˜ˆå€¼è®¾ç½®å¾—ç›¸å½“ä½ï¼›å› ä¸ºç”±è¯¥æ¨¡å‹äº§ç”Ÿçš„ç›’å­å¾ˆå¯èƒ½ä¼šæ›´ç´§åœ°å›´ç»•è¯¥å¯¹è±¡ã€‚

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/examples/minimal_finetune_cars.py

import os
import random
from functools import partial
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from func_to_script import script
from PIL import Image
from pytorch_accelerated.callbacks import (
    EarlyStoppingCallback,
    SaveBestModelCallback,
    get_default_callbacks,
)
from pytorch_accelerated.schedulers import CosineLrScheduler
from torch.utils.data import Dataset

from yolov7 import create_yolov7_model
from yolov7.dataset import Yolov7Dataset, create_yolov7_transforms, yolov7_collate_fn
from yolov7.evaluation import CalculateMeanAveragePrecisionCallback
from yolov7.loss_factory import create_yolov7_loss
from yolov7.trainer import Yolov7Trainer, filter_eval_predictions

def load_cars_df(annotations_file_path, images_path):
    all_images = sorted(set([p.parts[-1] for p in images_path.iterdir()]))
    image_id_to_image = {i: im for i, im in enumerate(all_images)}
    image_to_image_id = {v: k for k, v, in image_id_to_image.items()}

    annotations_df = pd.read_csv(annotations_file_path)
    annotations_df.loc[:, "class_name"] = "car"
    annotations_df.loc[:, "has_annotation"] = True

    # add 100 empty images to the dataset
    empty_images = sorted(set(all_images) - set(annotations_df.image.unique()))
    non_annotated_df = pd.DataFrame(list(empty_images)[:100], columns=["image"])
    non_annotated_df.loc[:, "has_annotation"] = False
    non_annotated_df.loc[:, "class_name"] = "background"

    df = pd.concat((annotations_df, non_annotated_df))

    class_id_to_label = dict(
        enumerate(df.query("has_annotation == True").class_name.unique())
    )
    class_label_to_id = {v: k for k, v in class_id_to_label.items()}

    df["image_id"] = df.image.map(image_to_image_id)
    df["class_id"] = df.class_name.map(class_label_to_id)

    file_names = tuple(df.image.unique())
    random.seed(42)
    validation_files = set(random.sample(file_names, int(len(df) * 0.2)))
    train_df = df[~df.image.isin(validation_files)]
    valid_df = df[df.image.isin(validation_files)]

    lookups = {
        "image_id_to_image": image_id_to_image,
        "image_to_image_id": image_to_image_id,
        "class_id_to_label": class_id_to_label,
        "class_label_to_id": class_label_to_id,
    }
    return train_df, valid_df, lookups

class CarsDatasetAdaptor(Dataset):
    def __init__(
        self,
        images_dir_path,
        annotations_dataframe,
        transforms=None,
    ):
        self.images_dir_path = Path(images_dir_path)
        self.annotations_df = annotations_dataframe
        self.transforms = transforms

        self.image_idx_to_image_id = {
            idx: image_id
            for idx, image_id in enumerate(self.annotations_df.image_id.unique())
        }
        self.image_id_to_image_idx = {
            v: k for k, v, in self.image_idx_to_image_id.items()
        }

    def __len__(self) -> int:
        return len(self.image_idx_to_image_id)

    def __getitem__(self, index):
        image_id = self.image_idx_to_image_id[index]
        image_info = self.annotations_df[self.annotations_df.image_id == image_id]
        file_name = image_info.image.values[0]
        assert image_id == image_info.image_id.values[0]

        image = Image.open(self.images_dir_path / file_name).convert("RGB")
        image = np.array(image)

        image_hw = image.shape[:2]

        if image_info.has_annotation.any():
            xyxy_bboxes = image_info[["xmin", "ymin", "xmax", "ymax"]].values
            class_ids = image_info["class_id"].values
        else:
            xyxy_bboxes = np.array([])
            class_ids = np.array([])

        if self.transforms is not None:
            transformed = self.transforms(
                image=image, bboxes=xyxy_bboxes, labels=class_ids
            )
            image = transformed["image"]
            xyxy_bboxes = np.array(transformed["bboxes"])
            class_ids = np.array(transformed["labels"])

        return image, xyxy_bboxes, class_ids, image_id, image_hw

DATA_PATH = Path("/".join(Path(__file__).absolute().parts[:-2])) / "data/cars"

@script
def main(
    data_path: str = DATA_PATH,
    image_size: int = 640,
    pretrained: bool = True,
    num_epochs: int = 30,
    batch_size: int = 8,
):

    # Load data
    data_path = Path(data_path)
    images_path = data_path / "training_images"
    annotations_file_path = data_path / "annotations.csv"

    train_df, valid_df, lookups = load_cars_df(annotations_file_path, images_path)
    num_classes = 1

    # Create datasets
    train_ds = CarsDatasetAdaptor(
        images_path,
        train_df,
    )
    eval_ds = CarsDatasetAdaptor(images_path, valid_df)

    train_yds = Yolov7Dataset(
        train_ds,
        create_yolov7_transforms(training=True, image_size=(image_size, image_size)),
    )
    eval_yds = Yolov7Dataset(
        eval_ds,
        create_yolov7_transforms(training=False, image_size=(image_size, image_size)),
    )

    # Create model, loss function and optimizer
    model = create_yolov7_model(
        architecture="yolov7", num_classes=num_classes, pretrained=pretrained
    )

    loss_func = create_yolov7_loss(model, image_size=image_size)

    optimizer = torch.optim.SGD(
        model.parameters(), lr=0.01, momentum=0.9, nesterov=True
    )
    # Create trainer and train
    trainer = Yolov7Trainer(
        model=model,
        optimizer=optimizer,
        loss_func=loss_func,
        filter_eval_predictions_fn=partial(
            filter_eval_predictions, confidence_threshold=0.01, nms_threshold=0.3
        ),
        callbacks=[
            CalculateMeanAveragePrecisionCallback.create_from_targets_df(
                targets_df=valid_df.query("has_annotation == True")[
                    ["image_id", "xmin", "ymin", "xmax", "ymax", "class_id"]
                ],
                image_ids=set(valid_df.image_id.unique()),
                iou_threshold=0.2,
            ),
            SaveBestModelCallback(watch_metric="map", greater_is_better=True),
            EarlyStoppingCallback(
                early_stopping_patience=3,
                watch_metric="map",
                greater_is_better=True,
                early_stopping_threshold=0.001,
            ),
            *get_default_callbacks(progress_bar=True),
        ],
    )

    trainer.train(
        num_epochs=num_epochs,
        train_dataset=train_yds,
        eval_dataset=eval_yds,
        per_device_batch_size=batch_size,
        create_scheduler_fn=CosineLrScheduler.create_scheduler_fn(
            num_warmup_epochs=5,
            num_cooldown_epochs=5,
            k_decay=2,
        ),
        collate_fn=yolov7_collate_fn,
    )

if __name__ == "__main__":
    main()
```

å¯åŠ¨è®­ç»ƒ[å¦‚è¿™é‡Œæ‰€è¿°](https://pytorch-accelerated.readthedocs.io/en/latest/quickstart.html)ï¼Œä½¿ç”¨å•ä¸ª V100 GPU å¹¶å¯ç”¨ fp16ï¼Œç»è¿‡ 3 ä¸ªæ—¶æœŸåï¼Œæˆ‘ä»¬è·å¾—äº†`0.995`çš„åœ°å›¾ï¼Œè¿™è¡¨æ˜æ¨¡å‹å·²ç»å‡ ä¹å®Œç¾åœ°å­¦ä¹ äº†ä»»åŠ¡ï¼

ç„¶è€Œï¼Œè™½ç„¶è¿™æ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„ç»“æœï¼Œä½†å®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯æ„æ–™ä¹‹ä¸­çš„ï¼Œå› ä¸º COCO åŒ…å«äº†æ±½è½¦çš„å›¾åƒã€‚

# ä»å¤´å¼€å§‹è®­ç»ƒ

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸåœ°å¾®è°ƒäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„ YOLOv7 æ¨¡å‹ï¼Œè®©æˆ‘ä»¬æ¢ç´¢å¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚è™½ç„¶è¿™å¯ä»¥ä½¿ç”¨è®¸å¤šä¸åŒçš„è®­ç»ƒé£Ÿè°±æ¥å®Œæˆï¼Œä½†è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä½œè€…åœ¨ COCO ä¸Šè®­ç»ƒæ—¶ä½¿ç”¨çš„ä¸€äº›å…³é”®æŠ€æœ¯ã€‚

## é•¶åµŒå¢å¼º

æ•°æ®æ‰©å……æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€é¡¹é‡è¦æŠ€æœ¯ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡åœ¨è®­ç»ƒæœŸé—´å¯¹æˆ‘ä»¬çš„æ•°æ®åº”ç”¨ä¸€ç³»åˆ—æ‰©å……æ¥ç»¼åˆæ‰©å±•æˆ‘ä»¬çš„æ•°æ®é›†ã€‚è™½ç„¶å¯¹è±¡æ£€æµ‹ä¸­å¸¸è§çš„å˜æ¢å¾€å¾€æ˜¯å¢å¼ºï¼Œå¦‚ç¿»è½¬å’Œæ—‹è½¬ï¼Œä½† YOLO çš„ä½œè€…é‡‡ç”¨äº†ä¸€ç§ç•¥æœ‰ä¸åŒçš„æ–¹æ³•ï¼Œå³åº”ç”¨é©¬èµ›å…‹å¢å¼ºï¼›ä¹‹å‰ç”± YOLOv4ã€YOLOv5 å’Œ YOLOX å‹å·ä½¿ç”¨ã€‚

é•¶åµŒå¢å¼ºçš„ç›®çš„æ˜¯å…‹æœå¯¹è±¡æ£€æµ‹æ¨¡å‹å€¾å‘äºé›†ä¸­äºæ£€æµ‹æœå‘å›¾åƒä¸­å¿ƒçš„é¡¹ç›®çš„è§‚å¯Ÿã€‚å…³é”®çš„æƒ³æ³•æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†å¤šå¹…å›¾åƒæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¯¹è±¡å¾ˆå¯èƒ½ä½äºé€šå¸¸åœ¨æ•°æ®é›†ä¸­çœ‹åˆ°çš„å›¾åƒä¸­è§‚å¯Ÿä¸åˆ°çš„ä½ç½®å’Œä¸Šä¸‹æ–‡ä¸­ï¼›è¿™å°†è¿«ä½¿æ¨¡å‹å­¦ä¹ çš„ç‰¹å¾æ›´åŠ ä½ç½®ä¸å˜ã€‚

è™½ç„¶ mosaic æœ‰å‡ ä¸ªä¸åŒçš„å®ç°ï¼Œæ¯ä¸ªéƒ½æœ‰å¾®å°çš„å·®å¼‚ï¼Œä½†è¿™é‡Œæˆ‘ä»¬å°†å±•ç¤ºä¸€ä¸ªç»„åˆäº†å››ä¸ªä¸åŒå›¾åƒçš„å®ç°ã€‚è¿™ç§å®ç°åœ¨è¿‡å»å¯¹æˆ‘ä»¬å¾ˆæœ‰æ•ˆï¼Œæœ‰å„ç§å„æ ·çš„å¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚

è™½ç„¶åœ¨åˆ›å»ºé•¶åµŒå›¾ä¹‹å‰ä¸éœ€è¦è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œä½†è¿™ç¡®å®ä¼šå¯¼è‡´åˆ›å»ºçš„é•¶åµŒå›¾å¤§å°ç›¸ä¼¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œé‡‡ç”¨è¿™ç§æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªç®€å•çš„è°ƒæ•´å¤§å°è½¬æ¢å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†é€‚é…å™¨ä¸­æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/dataset.py

import albumentations as A

def create_base_transforms(target_image_size):
    return A.Compose(
        [
            A.LongestMaxSize(target_image_size),
        ],
        bbox_params=A.BboxParams(format="pascal_voc", label_fields=["labels"]),
    )
```

![](img/e5f51b272b0805faed8ffed9faafb72b.png)

ä¸ºäº†åº”ç”¨æˆ‘ä»¬çš„å¢å¼ºï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨ç™½è›‹ç™½ï¼Œå®ƒæ”¯æŒè®¸å¤šå¯¹è±¡æ£€æµ‹è½¬æ¢ã€‚

è™½ç„¶æ•°æ®æ‰©å……é€šå¸¸ä»¥å‡½æ•°çš„å½¢å¼å®ç°ï¼Œä¼ é€’ç»™ PyTorch æ•°æ®é›†å¹¶åœ¨åŠ è½½å›¾åƒåç«‹å³åº”ç”¨ï¼Œä½†ç”±äºé•¶åµŒéœ€è¦ä»æ•°æ®é›†ä¸­åŠ è½½å¤šå¹…å›¾åƒï¼Œå› æ­¤è¿™ç§æ–¹æ³•åœ¨è¿™é‡Œä¸èµ·ä½œç”¨ã€‚æˆ‘ä»¬å†³å®šå°† mosaic å®ç°ä¸ºæ•°æ®é›†åŒ…è£…ç±»ï¼Œä»¥æ¸…æ™°åœ°å°è£…è¿™ä¸€é€»è¾‘ã€‚æˆ‘ä»¬å¯ä»¥å¯¼å…¥å¹¶ä½¿ç”¨å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/afaf1a09636955515583069b6862c195.png)

è®©æˆ‘ä»¬çœ‹ä¸€äº›äº§ç”Ÿçš„å›¾åƒç±»å‹çš„ä¾‹å­ã€‚ç”±äºæˆ‘ä»¬è¿˜æ²¡æœ‰å‘é•¶åµŒæ•°æ®é›†ä¼ é€’ä»»ä½•è°ƒæ•´å¤§å°çš„å˜æ¢ï¼Œè¿™äº›å›¾åƒç›¸å½“å¤§ã€‚

![](img/61f04171380f6f32acdf3185d2d94c45.png)![](img/72721c959c8cc79ed5a39812438b99cd.png)

è¯·æ³¨æ„ï¼Œè™½ç„¶é•¶åµŒå›¾åƒçœ‹èµ·æ¥éå¸¸ä¸åŒï¼Œä½†å®ƒä»¬éƒ½è¢«ç§°ä¸ºå…·æœ‰ç›¸åŒç´¢å¼•çš„*ï¼Œå› æ­¤è¢«åº”ç”¨äºç›¸åŒçš„å›¾åƒï¼åˆ›å»ºé•¶åµŒå›¾æ—¶ï¼Œå®ƒä¼šä»æ•°æ®é›†ä¸­éšæœºé€‰æ‹©å¦å¤– 3 å¹…å›¾åƒï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨éšæœºä½ç½®ï¼Œè¿™æ ·æ¯æ¬¡éƒ½ä¼šç”Ÿæˆä¸åŒå¤–è§‚çš„å›¾åƒã€‚å› æ­¤ï¼Œåº”ç”¨è¿™ç§å¢å¼ºç¡®å®æ‰“ç ´äº†æˆ‘ä»¬å¯¹è®­ç»ƒæ—¶æœŸçš„æ¦‚å¿µâ€”â€”æ•°æ®é›†ä¸­çš„æ¯å¹…å›¾åƒåªè¢«çœ‹åˆ°ä¸€æ¬¡â€”â€”å› ä¸ºå›¾åƒå¯ä»¥è¢«çœ‹åˆ°å¤šæ¬¡ï¼*

å› æ­¤ï¼Œåœ¨ä½¿ç”¨ mosaic è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„ç­–ç•¥æ˜¯ä¸è¦è¿‡å¤šè€ƒè™‘å†å…ƒçš„æ•°é‡ï¼Œå°½å¯èƒ½é•¿æ—¶é—´åœ°è®­ç»ƒæ¨¡å‹ï¼Œç›´åˆ°å®ƒåœæ­¢æ”¶æ•›ã€‚æ¯•ç«Ÿï¼Œçºªå…ƒçš„æ¦‚å¿µåªæœ‰åœ¨å¸®åŠ©æˆ‘ä»¬è·Ÿè¸ªè®­ç»ƒæ—¶æ‰çœŸæ­£æœ‰ç”¨â€”â€”è¯¥æ¨¡å‹åªèƒ½çœ‹åˆ°è¿ç»­çš„å›¾åƒæµï¼

## æ··åˆå¢å¼º

é•¶åµŒå¢å¼ºé€šå¸¸ä¸å¦ä¸€ç§å˜æ¢ä¸€èµ·åº”ç”¨â€” [æ··åˆ](https://arxiv.org/abs/1710.09412v2)ã€‚ä¸ºäº†å½¢è±¡åŒ–è¿™æ˜¯åšä»€ä¹ˆçš„ï¼Œè®©æˆ‘ä»¬æš‚æ—¶ç¦ç”¨é©¬èµ›å…‹ï¼Œå¹¶å¯ç”¨å®ƒè‡ªå·±çš„æ··éŸ³ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/f03569e44deff52038dbda34c30d22e0.png)

æœ‰æ„æ€ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå®ƒå·²ç»ç»“åˆäº†ä¸¤ä¸ªå›¾åƒåœ¨ä¸€èµ·ï¼Œè¿™å¯¼è‡´äº†ä¸€äº›'å¹½çµ'å¯»æ‰¾æ±½è½¦å’ŒèƒŒæ™¯ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¯ç”¨ä¸¤ç§è½¬æ¢å¹¶æ£€æŸ¥æˆ‘ä»¬çš„è¾“å‡ºã€‚

![](img/fb0dd3e4e5276de462820edebdea96bc.png)

å“‡ï¼åœ¨æˆ‘ä»¬ç”Ÿæˆçš„å›¾åƒä¸­æœ‰ç›¸å½“å¤šçš„æ±½è½¦è¦æ£€æµ‹ï¼Œåœ¨è®¸å¤šä¸åŒçš„ä½ç½®â€”â€”è¿™å¯¹æ¨¡å‹æ¥è¯´è‚¯å®šæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬ä¸€èµ·åº”ç”¨é©¬èµ›å…‹å’Œ mixup æ—¶ï¼Œå•ä¸ªå›¾åƒä¸é©¬èµ›å…‹æ··åˆåœ¨ä¸€èµ·ã€‚

## é•¶åµŒåä»¿å°„å˜æ¢

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬æ­£åœ¨åˆ›å»ºçš„é•¶åµŒå›¾æ¯”æˆ‘ä»¬å°†ç”¨æ¥è®­ç»ƒæ¨¡å‹çš„å›¾åƒå°ºå¯¸è¦å¤§å¾—å¤šï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œåšä¸€äº›å¤§å°è°ƒæ•´ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯åœ¨åˆ›å»ºé©¬èµ›å…‹åç®€å•åœ°åº”ç”¨è°ƒæ•´å¤§å°å˜æ¢ã€‚

è™½ç„¶è¿™å¯ä»¥å·¥ä½œï¼Œä½†è¿™å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›éå¸¸å°çš„å¯¹è±¡ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯å°†å››ä¸ªå›¾åƒçš„å¤§å°è°ƒæ•´ä¸ºä¸€ä¸ªå›¾åƒçš„å¤§å°â€”â€”è¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºåŸŸå·²ç»åŒ…å«éå¸¸å°çš„è¾¹ç•Œæ¡†äº†ï¼æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¯ä¸ªé©¬èµ›å…‹åœ¨ç»“æ„ä¸Šéå¸¸ç›¸ä¼¼ï¼Œæ¯ä¸ªè±¡é™éƒ½æœ‰ä¸€ä¸ªå›¾åƒã€‚å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹å¯¹ä½ç½®å˜åŒ–æ›´åŠ å¥å£®ï¼Œè¿™å®é™…ä¸Šå¯èƒ½æ²¡æœ‰å¤šå¤§å¸®åŠ©ï¼›å› ä¸ºæ¨¡å‹å¯èƒ½åªæ˜¯ä»æ¯ä¸ªè±¡é™çš„ä¸­é—´å¼€å§‹å¯»æ‰¾ã€‚

ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–çš„ä¸€ç§æ–¹æ³•æ˜¯ç®€å•åœ°ä»æˆ‘ä»¬çš„é©¬èµ›å…‹ä¸­éšæœºé€‰å–ä¸€éƒ¨åˆ†ã€‚è¿™å°†ä»ç„¶æä¾›å®šä½çš„å¯å˜æ€§ï¼ŒåŒæ—¶ä¿æŒç›®æ ‡å¯¹è±¡çš„å¤§å°å’Œçºµæ¨ªæ¯”ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œè¿™ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æœºä¼šï¼ŒåŠ å…¥ä¸€äº›å…¶ä»–çš„å˜æ¢ï¼Œå¦‚ç¼©æ”¾å’Œæ—‹è½¬ï¼Œä»¥å¢åŠ æ›´å¤šçš„å¯å˜æ€§ã€‚

æ‰€ä½¿ç”¨çš„ç²¾ç¡®å˜æ¢å’Œå¤§å°å°†åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‚¨æ‰€ä½¿ç”¨çš„å›¾åƒï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œå…ˆè¯•éªŒè¿™äº›è®¾ç½®ï¼Œä»¥ç¡®ä¿æ‰€æœ‰å¯¹è±¡ä»ç„¶å¯è§å’Œå¯è¯†åˆ«ï¼

æˆ‘ä»¬å¯ä»¥å®šä¹‰åº”ç”¨äºé•¶åµŒå›¾åƒçš„å˜æ¢ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ç³»åˆ—ä»¿å°„å˜æ¢â€”â€”åœ¨æˆ‘ä»¬ç›®æ ‡æ•°æ®çš„åˆç†èŒƒå›´å†…â€”â€”ç„¶åæ˜¯éšæœºè£å‰ªã€‚åœ¨æœ€åˆçš„å®ç°ä¹‹åï¼Œæˆ‘ä»¬ä¹Ÿæ¯” mosaic æ›´å°‘åœ°åº”ç”¨ mixupã€‚

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/mosaic.py

def create_post_mosaic_transform(
        output_height,
        output_width,
        pad_colour=(0, 0, 0),
        rotation_range=(-10, 10),
        shear_range=(-10, 10),
        translation_percent_range=(-0.2, 0.2),
        scale_range=(0.08, 1.0),
        apply_prob=0.8,
):
    return A.Compose(
        [
            A.Affine(
                cval=pad_colour,
                rotate=rotation_range,
                shear=shear_range,
                translate_percent=translation_percent_range,
                scale=None,
                keep_ratio=True,
                p=apply_prob,
            ),
            A.HorizontalFlip(),
            A.RandomResizedCrop(height=output_height, width=output_width, scale=scale_range),
        ],
        bbox_params=A.BboxParams(format="pascal_voc", label_fields=["labels"], min_visibility=0.25),
    )
```

![](img/9ce5d84de920eef10fa4a14ff237eb12.png)![](img/e5e65fe15d1612f1cbb1e365e09340f3.png)![](img/32c7e1bc9dabe3f053d3381d53a30b6a.png)![](img/d6dde4960e35535731d2aa636a30733c.png)

æŸ¥çœ‹è¿™äº›å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§é‡çš„å˜åŒ–ï¼Œè¿™äº›å›¾åƒç°åœ¨æ˜¯ç”¨äºè®­ç»ƒçš„æ­£ç¡®å¤§å°ã€‚ç”±äºæˆ‘ä»¬é€‰æ‹©äº†éšæœºæ¯”ä¾‹ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œå¹¶éæ¯ä¸ªå›¾åƒçœ‹èµ·æ¥éƒ½åƒé©¬èµ›å…‹ï¼Œå› æ­¤è¿™äº›è¾“å‡ºä¸åº”ä¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çœ‹åˆ°çš„å›¾åƒå¤ªä¸ç›¸ä¼¼ã€‚å¦‚æœä½¿ç”¨æ›´æç«¯çš„å¢å¼ºï¼Œä¾‹å¦‚åœ¨è®­ç»ƒå›¾åƒå’Œæ¨æ–­å›¾åƒä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œåˆ™åœ¨è®­ç»ƒç»“æŸå‰ä¸ä¹…ç¦ç”¨è¿™äº›å¢å¼ºå¯èƒ½æ˜¯æœ‰åˆ©çš„ã€‚

åœ¨å®˜æ–¹å®ç°ä¸­ï¼Œä½œè€…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ 4 å’Œ 9 å›¾åƒçš„é©¬èµ›å…‹ã€‚ç„¶è€Œï¼Œå½“ç»“åˆç¼©æ”¾å’Œè£å‰ªæ¥æ£€æŸ¥è¿™äº›å¢å¼ºçš„è¾“å‡ºæ—¶ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¾“å‡ºçœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹©åœ¨è¿™é‡Œçœç•¥è¿™ä¸€ç‚¹ã€‚

## å°†æƒé‡è¡°å‡åº”ç”¨äºå‚æ•°ç»„

åœ¨å‰é¢çš„ç®€å•ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¼˜åŒ–å™¨ï¼Œä»¥ä¾¿å®ƒå¯ä»¥ä¼˜åŒ–æˆ‘ä»¬æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬æƒ³è·Ÿéšä½œè€…ä»‹ç»[æƒé‡è¡°å‡æ­£åˆ™åŒ–](https://medium.com/analytics-vidhya/deep-learning-basics-weight-decay-3c68eb4344e9)ï¼Œéµå¾ª[çš„ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå›¾åƒåˆ†ç±»çš„é”¦å›Šå¦™è®¡](https://arxiv.org/pdf/1812.01187.pdf)ä¸­ç»™å‡ºçš„æŒ‡å¯¼ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€ä½³çš„ï¼›è¯¥è®ºæ–‡å»ºè®®æƒé‡è¡°å‡åº”è¯¥ä»…åº”ç”¨äºå·ç§¯å’Œå…¨è¿æ¥å±‚ã€‚

ä¸ºäº†åœ¨ PyTorch ä¸­å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸¤ä¸ªä¸åŒçš„å‚æ•°ç»„æ¥è¿›è¡Œä¼˜åŒ–ï¼›ä¸€ä¸ªåŒ…å«æˆ‘ä»¬çš„å·ç§¯æƒé‡ï¼Œå¦ä¸€ä¸ªåŒ…å«å‰©ä½™çš„å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/1b74745c6f40bbfcc1a54bbd99170b50.png)

æ£€æŸ¥æ–¹æ³•å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªç®€å•çš„è¿‡æ»¤æ“ä½œ:

![](img/d9bd6e2b82d0fbe8aa2303a9e8d06131.png)

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†è¿™äº›ä¼ é€’ç»™ä¼˜åŒ–å™¨:

```
optimizer = torch.optim.SGD(
        param_groups["other_params"], lr=0.01, momentum=0.937, nesterov=True
    )

optimizer.add_param_group(
        {"params": param_groups["conv_weights"], "weight_decay": weight_decay}
    )
```

## *å­¦ä¹ ç‡è°ƒåº¦*

å½“è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬ç»å¸¸å¸Œæœ›åœ¨è®­ç»ƒæœŸé—´è°ƒæ•´æˆ‘ä»¬çš„å­¦ä¹ ç‡çš„å€¼ï¼›è¿™æ˜¯ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨æ¥å®Œæˆçš„ã€‚è™½ç„¶æœ‰è®¸å¤šæµè¡Œçš„æ—¶é—´è¡¨ï¼Œä½†ä½œè€…é€‰æ‹©äº†ä½™å¼¦å­¦ä¹ ç‡æ—¶é—´è¡¨â€”â€”åœ¨è®­ç»ƒå¼€å§‹æ—¶è¿›è¡Œçº¿æ€§çƒ­èº«ã€‚å®ƒå…·æœ‰ä»¥ä¸‹å½¢çŠ¶:

![](img/557db6f868b1c80fd5a4cac00787af0a.png)

ä½™å¼¦å­¦ä¹ ç‡è®¡åˆ’(å¸¦çƒ­èº«)

åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç°ä¸€æ®µæ—¶é—´çš„é¢„çƒ­å’Œå†·å´â€”â€”å­¦ä¹ ç‡ä¿æŒåœ¨æœ€å°å€¼â€”â€”é€šå¸¸æ˜¯è¿™ä¸ªè°ƒåº¦å™¨çš„ä¸€ä¸ªå¥½ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè°ƒåº¦ç¨‹åº PyTorch-accelerated æ”¯æŒä¸€ä¸ª [k-decay å‚æ•°](https://arxiv.org/abs/2004.05909)ï¼Œè¯¥å‚æ•°å¯ç”¨äºè°ƒæ•´é€€ç«çš„ç§¯æç¨‹åº¦ã€‚

å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ k-decay å°†å­¦ä¹ é€Ÿç‡ä¿æŒåœ¨ä¸€ä¸ªæ›´é«˜çš„å€¼æ›´é•¿çš„æ—¶é—´æ•ˆæœå¾ˆå¥½ã€‚è¯¥æ—¶é—´è¡¨ä»¥åŠé¢„çƒ­å’Œå†·å´æ—¶é—´å¦‚ä¸‹æ‰€ç¤º:

![](img/bee7685e35453eb1cf0840016a17e532.png)

ä½™å¼¦å­¦ä¹ ç‡è®¡åˆ’(å¸¦é¢„çƒ­)ï¼Œè®¾ç½®ä¸º k_decay = 2

## æ¢¯åº¦ç´¯ç§¯ã€ç¼©æ”¾æƒé‡è¡°å‡

åœ¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ‰¹é‡å¤§å°å¾€å¾€æ˜¯ç”±æˆ‘ä»¬çš„ç¡¬ä»¶å†³å®šçš„ï¼›å› ä¸ºæˆ‘ä»¬æƒ³å°½é‡å¢åŠ æˆ‘ä»¬å¯ä»¥æ”¾åœ¨ GPU ä¸Šçš„æ•°æ®é‡ã€‚ä½†æ˜¯ï¼Œå¿…é¡»è€ƒè™‘ä¸€äº›å› ç´ :

*   å¯¹äºéå¸¸å°çš„æ‰¹é‡ï¼Œæˆ‘ä»¬æ— æ³•ä¼°è®¡æ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦ã€‚è¿™å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚
*   ä¿®æ”¹æ‰¹é‡å¤§å°ä¼šå¯¼è‡´è¶…å‚æ•°éœ€è¦ä¸åŒçš„è®¾ç½®ï¼Œä¾‹å¦‚å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ã€‚è¿™ä½¿å¾—å¾ˆéš¾æ‰¾åˆ°ä¸€ç»„ä¸€è‡´çš„è¶…å‚æ•°ã€‚

ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ç§ç§°ä¸º [*æ¢¯åº¦ç´¯ç§¯*](/what-is-gradient-accumulation-in-deep-learning-ec034122cfa) çš„æŠ€æœ¯ï¼Œå…¶ä¸­æ¥è‡ªå¤šä¸ªæ­¥éª¤çš„æ¢¯åº¦è¢«ç´¯ç§¯ä»¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹é‡ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åœ¨ GPU ä¸Šå¯ä»¥å®¹çº³çš„æœ€å¤§æ‰¹é‡æ˜¯ 8ã€‚æˆ‘ä»¬å¯ä»¥ä¿å­˜æ¢¯åº¦å€¼ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹å¹¶æ·»åŠ è¿™äº›æ–°æ¢¯åº¦ï¼Œè€Œä¸æ˜¯åœ¨æ¯æ‰¹ç»“æŸæ—¶æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚åœ¨æŒ‡å®šæ•°é‡çš„æ­¥éª¤ä¹‹åï¼Œæˆ‘ä»¬æ‰§è¡Œæ›´æ–°ï¼›å¦‚æœæˆ‘ä»¬å°†æ­¥éª¤æ•°è®¾ç½®ä¸º 4ï¼Œè¿™å¤§è‡´ç›¸å½“äºä½¿ç”¨ 32 çš„æ‰¹é‡å¤§å°ï¼

åœ¨ PyTorch ä¸­ï¼Œè¿™å¯ä»¥æ‰‹åŠ¨æ‰§è¡Œï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
num_accumulation_steps = 4  

# loop through ennumerated batches
for step, (inputs, labels) in enumerate(data_loader):

        model_outputs = model(inputs)
        loss  = loss_fn(model_outputs, labels)

        # normalize loss to account for batch accumulation
        loss = loss / num_accumulation_steps 

        # calculate gradients, these are summed automatically
        loss.backward()

        if ((step + 1) % num_accumulation_steps == 0) or
            (step + 1 == len(data_loader)):
            # perform weight update
            optimizer.step()
            optimizer.zero_grad()
```

åœ¨æœ€åˆçš„ YOLOv7 å®æ–½ä¸­ï¼Œé€‰æ‹©æ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„æ•°é‡ï¼Œä½¿å¾—æ€»æ‰¹é‡(åœ¨æ‰€æœ‰è¿‡ç¨‹ä¸­)è‡³å°‘ä¸º 64ï¼›è¿™ç¼“è§£äº†å‰é¢è®¨è®ºçš„ä¸¤ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œä½œè€…ä»¥ä¸‹åˆ—æ–¹å¼æ ¹æ®æ‰¹æ¬¡å¤§å°è°ƒæ•´é‡é‡è¡°å‡:

```
nominal_batch_size = 64
num_accumulate_steps = max(round(nominal_batch_size / total_batch_size), 1)

base_weight_decay = 0.0005
scaled_weight_decay = (
    base_weight_decay * total_batch_size * num_accumulate_steps / nominal_batch_size
)
```

æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å…³ç³»å½¢è±¡åŒ–å¦‚ä¸‹:

![](img/8bace4f268b2708fa07641ab5a67b0d2.png)

é¦–å…ˆæŸ¥çœ‹ç´¯ç§¯æ­¥éª¤çš„æ•°é‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç´¯ç§¯æ­¥éª¤çš„æ•°é‡ä¼šå‡å°‘ï¼Œç›´åˆ°è¾¾åˆ°æˆ‘ä»¬çš„åä¹‰æ‰¹é‡ï¼Œç„¶åä¸å†éœ€è¦æ¢¯åº¦ç´¯ç§¯ã€‚

![](img/2f128b4dc0eb5573e25a451ff5f0c7b4.png)

ç°åœ¨æ¥çœ‹çœ‹æ‰€ä½¿ç”¨çš„é‡é‡è¡°å‡é‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¾¾åˆ°æ ‡ç§°æ‰¹é‡ä¹‹å‰ï¼Œé‡é‡è¡°å‡é‡ä¿æŒåœ¨åŸºç¡€å€¼ï¼Œç„¶åéšç€æ‰¹é‡çš„å¢åŠ è€Œçº¿æ€§å¢åŠ ï¼›éšç€æ‰¹é‡å˜å¤§ï¼Œåº”ç”¨æ›´å¤šçš„é‡é‡è¡°å‡ã€‚

## æ¨¡å‹ EMA

åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œé€šè¿‡å¯¹åœ¨æ•´ä¸ªè®­ç»ƒè¿è¡Œä¸­è§‚å¯Ÿåˆ°çš„å‚æ•°è¿›è¡Œç§»åŠ¨å¹³å‡æ¥è®¾ç½®æ¨¡å‹æƒé‡å€¼å¯èƒ½æ˜¯æœ‰ç›Šçš„ï¼Œè¿™ä¸ä½¿ç”¨åœ¨æœ€åä¸€æ¬¡å¢é‡æ›´æ–°ä¹‹åè·å¾—çš„å‚æ•°ç›¸åã€‚è¿™é€šå¸¸é€šè¿‡ç»´æŠ¤æ¨¡å‹å‚æ•°çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼(EMA)æ¥å®ç°ï¼Œåœ¨å®è·µä¸­ï¼Œè¿™é€šå¸¸æ„å‘³ç€ç»´æŠ¤æ¨¡å‹çš„å¦ä¸€ä¸ªå‰¯æœ¬æ¥å­˜å‚¨è¿™äº›å¹³å‡æƒé‡ã€‚ç„¶è€Œï¼Œä¸æ˜¯åœ¨æ¯ä¸ªæ›´æ–°æ­¥éª¤ä¹‹åæ›´æ–°è¯¥æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨ç°æœ‰å‚æ•°å€¼å’Œæ›´æ–°å€¼çš„çº¿æ€§ç»„åˆæ¥è®¾ç½®è¿™äº›å‚æ•°ã€‚

è¿™æ˜¯ä½¿ç”¨ä»¥ä¸‹å…¬å¼å®Œæˆçš„:

```
updated_EMA_model_weights = decay * EMA_model_weights + (1\. - decay) * updated_model_weights
```

å…¶ä¸­*è¡°å‡*æ˜¯æˆ‘ä»¬è®¾å®šçš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½® decay=0.99ï¼Œæˆ‘ä»¬æœ‰:

```
updated_EMA_model_weights = 0.99 * EMA_model_weights + 0.01 * updated_model_wei.99 * EMA_model_weights + 0.01 * updated_model_weights
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä¿ç•™äº† 99%çš„ç°æœ‰çŠ¶æ€ï¼Œåªä¿ç•™äº† 1%çš„æ–°çŠ¶æ€ï¼

ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆè¿™å¯èƒ½æ˜¯æœ‰ç›Šçš„ï¼Œè®©æˆ‘ä»¬è€ƒè™‘è¿™æ ·çš„æƒ…å†µï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œåœ¨ä¸€æ‰¹æ•°æ®ä¸Šè¡¨ç°å¾—éå¸¸å·®ã€‚è¿™å¯èƒ½å¯¼è‡´å¯¹æˆ‘ä»¬çš„å‚æ•°è¿›è¡Œå¤§é‡æ›´æ–°ï¼Œè¿‡åº¦è¡¥å¿æ‰€è·å¾—çš„é«˜æŸå¤±ï¼Œè¿™å¯¹å³å°†åˆ°æ¥çš„æ‰¹æ¬¡æ˜¯ä¸åˆ©çš„ã€‚é€šè¿‡ä»…å¹¶å…¥æœ€æ–°å‚æ•°çš„ä¸€å°éƒ¨åˆ†ï¼Œå¤§çš„æ›´æ–°å°†è¢«â€œå¹³æ»‘â€ï¼Œå¹¶ä¸”å¯¹æ¨¡å‹çš„æƒé‡å…·æœ‰è¾ƒå°çš„æ•´ä½“å½±å“ã€‚æœ‰æ—¶ï¼Œè¿™äº›å¹³å‡å‚æ•°æœ‰æ—¶å¯ä»¥åœ¨è¯„ä¼°æœŸé—´äº§ç”Ÿæ˜æ˜¾æ›´å¥½çš„ç»“æœï¼Œå¹¶ä¸”è¿™ç§æŠ€æœ¯å·²ç»åœ¨ç”¨äºæµè¡Œæ¨¡å‹çš„è‹¥å¹²è®­ç»ƒæ–¹æ¡ˆä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚è®­ç»ƒ MNASNetã€MobileNet-V3 å’Œ EfficientNet ä½¿ç”¨ TensorFlow ä¸­åŒ…å«çš„å®ç°ã€‚

YOLOv7 ä½œè€…é‡‡ç”¨çš„ EMA æ–¹æ³•ä¸å…¶ä»–å®ç°ç•¥æœ‰ä¸åŒï¼Œå› ä¸ºå®ƒä¸ä½¿ç”¨å›ºå®šçš„è¡°å‡ï¼Œè€Œæ˜¯æ ¹æ®æ›´æ–°æ¬¡æ•°æ¥æ”¹å˜è¡°å‡é‡ã€‚æˆ‘ä»¬å¯ä»¥æ‰©å±• PyTorch-accelerated ä¸­åŒ…å«çš„ [ModelEMA ç±»æ¥å®ç°å¦‚ä¸‹å®šä¹‰çš„è¡Œä¸º:](https://pytorch-accelerated.readthedocs.io/en/latest/utils.html)

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/utils.py

from pytorch-accelerated.utils import ModelEma

class Yolov7ModelEma(ModelEma):
    def __init__(self, model, decay=0.9999):
        super().__init__(model, decay)
        self.num_updates = 0
        self.decay_fn = lambda x: decay * (
            1 - math.exp(-x / 2000)
        )  # decay exponential ramp (to help early epochs)
        self.decay = self.decay_fn(self.num_updates)

    def update(self, model):
        super().update(model)
        self.num_updates += 1
        self.decay = self.decay_fn(self.num_updates)
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¡°å‡æ˜¯é€šè¿‡åœ¨æ¯æ¬¡æ›´æ–°åè°ƒç”¨ä¸€ä¸ªå‡½æ•°æ¥è®¾ç½®çš„ã€‚è®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹è¿™æ˜¯ä»€ä¹ˆæ ·å­:

![](img/d23e441fd8805e3d2ab6061bd013a722.png)

ç”±æ­¤å¯ä»¥çœ‹å‡ºï¼Œè¡°å‡é‡éšç€æ›´æ–°æ¬¡æ•°çš„å¢åŠ è€Œå¢åŠ ï¼Œå³æ¯ä¸ªå†å…ƒä¸€æ¬¡ã€‚

å›æƒ³ä¸Šé¢çš„å…¬å¼ï¼Œè¿™æ„å‘³ç€ï¼Œæœ€åˆï¼Œæˆ‘ä»¬å€¾å‘äºä½¿ç”¨æ›´æ–°çš„æ¨¡å‹æƒé‡ï¼Œè€Œä¸æ˜¯å†å²å¹³å‡å€¼ã€‚ç„¶è€Œï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæˆ‘ä»¬å¼€å§‹åŠ å…¥æ›´å¤šä»¥å‰æ—¶æœŸçš„å¹³å‡ä½“é‡ã€‚è¿™ä¸è¿™ç§æŠ€æœ¯çš„é€šå¸¸ç”¨æ³•æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œè¿™ç§æŠ€æœ¯çš„è®¾è®¡æ˜¯ä¸ºäº†å¸®åŠ© EMA æ¨¡å‹åœ¨æ›´æ—©çš„æ—¶æœŸæ›´å¿«åœ°æ”¶æ•›ã€‚

## é€‰æ‹©åˆé€‚çš„é”šç®±å°ºå¯¸

å›æƒ³ä¸€ä¸‹ä¹‹å‰å…³äºé”šæ¡†çš„è®¨è®ºï¼Œä»¥åŠè¿™äº›å¦‚ä½•åœ¨ YOLOv7 å¦‚ä½•æ£€æµ‹å¯¹è±¡æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è¯„ä¼°æˆ‘ä»¬é€‰æ‹©çš„é”šæ¡†æ˜¯å¦é€‚åˆæˆ‘ä»¬çš„é—®é¢˜ï¼Œå¦‚æœä¸é€‚åˆï¼Œä¸ºæˆ‘ä»¬çš„æ•°æ®é›†æ‰¾åˆ°ä¸€äº›æ˜æ™ºçš„é€‰æ‹©ã€‚

è¿™é‡Œçš„æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä» YOLOv5 ä¸­ä½¿ç”¨çš„è‡ªèº«æŠ—ä½“æ–¹æ³•æ”¹ç¼–è€Œæ¥çš„ï¼ŒYOLOv7 ä¸­ä¹Ÿä½¿ç”¨äº†è¿™ç§æ–¹æ³•ã€‚

**è¯„ä¼°å½“å‰é”šç›’**

æœ€ç®€å•çš„æ–¹æ³•æ˜¯ç®€å•åœ°ä½¿ç”¨ä¸ COCO ç›¸åŒçš„é”šç›’ï¼Œè¿™äº›é”šç›’å·²ç»ä¸å®šä¹‰çš„æ¶æ„æ†ç»‘åœ¨ä¸€èµ·ã€‚

![](img/270573364cfc87287d2a6e2323ff36bb.png)

è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æœ‰ 3 ä¸ªç»„ï¼Œæ¯ä¸ªç»„å¯¹åº”äºç‰¹å¾é‡‘å­—å¡”ç½‘ç»œçš„æ¯ä¸€å±‚ã€‚è¿™äº›æ•°å­—å¯¹åº”äºæˆ‘ä»¬çš„é”šå¤§å°ï¼Œé”šæ¡†çš„å®½åº¦å’Œé«˜åº¦å°†è¢«ç”Ÿæˆã€‚

å›æƒ³ä¸€ä¸‹ï¼Œç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ(FPN)æœ‰ä¸‰ä¸ªè¾“å‡ºï¼Œæ¯ä¸ªè¾“å‡ºçš„ä½œç”¨æ˜¯æ ¹æ®å¯¹è±¡çš„æ¯”ä¾‹æ¥æ£€æµ‹å¯¹è±¡ã€‚

ä¾‹å¦‚:

*   P3 8 å·ç”¨äºæ¢æµ‹è¾ƒå°çš„ç‰©ä½“ã€‚
*   P4/16 ç”¨äºæ¢æµ‹ä¸­ç­‰ç‰©ä½“ã€‚
*   P5/32 ç”¨äºæ¢æµ‹æ›´å¤§çš„ç‰©ä½“ã€‚

è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸€å±‚è®¾ç½®ç›¸åº”çš„é”šç‚¹å¤§å°ã€‚

![](img/323bec4906b15c4af2460068e354de76.png)

ä¸ºäº†è¯„ä¼°æˆ‘ä»¬å½“å‰çš„é”šç›’ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æœ€å¥½çš„å¯èƒ½å¬å›ï¼Œå¦‚æœæ¨¡å‹èƒ½å¤ŸæˆåŠŸåœ°å°†é€‚å½“çš„é”šç›’ä¸åŸºç¡€äº‹å®ç›¸åŒ¹é…ï¼Œè¿™å°†ä¼šå‘ç”Ÿã€‚

**æŸ¥æ‰¾å’Œè°ƒæ•´åœ°é¢çœŸå®è¾¹ç•Œæ¡†**

ä¸ºäº†è¯„ä¼°é”šç›’ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦äº†è§£æ•°æ®é›†ä¸­å¯¹è±¡çš„å½¢çŠ¶å’Œå¤§å°ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬å¯ä»¥è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®æˆ‘ä»¬å°†åœ¨å…¶ä¸Šè®­ç»ƒçš„å›¾åƒçš„å¤§å°æ¥è°ƒæ•´æˆ‘ä»¬çš„åœ°é¢çœŸç›¸æ¡†çš„å®½åº¦å’Œé«˜åº¦â€”â€”å¯¹äºè¯¥æ¶æ„ï¼Œè¿™è¢«æ¨èä¸º 640ã€‚

è®©æˆ‘ä»¬ä»æ‰¾å‡ºè®­ç»ƒé›†ä¸­æ‰€æœ‰åœ°é¢çœŸå€¼æ¡†çš„å®½åº¦å’Œé«˜åº¦å¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥å¦‚ä¸‹æ‰€ç¤ºè®¡ç®—è¿™äº›å€¼:

![](img/cbd2a9592deea263a5ebffa47c5d6633.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ã€‚æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬æå‰æŒæ¡äº†è¿™äº›ä¿¡æ¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›çŸ¥è¯†ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åš:

![](img/ffeee9b12e0e2b7d96819f9138f9c2d6.png)

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†å…¶ä¸ç°æœ‰çš„æ•°æ®æ¡†æ¶åˆå¹¶:

![](img/8eab706fa6f883b590b65e10324b8cdb.png)

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥è·å¾—åœ°é¢å®å†µç›®æ ‡ç›¸å¯¹äºç›®æ ‡å›¾åƒå¤§å°çš„è°ƒæ•´åçš„å®½åº¦å’Œé«˜åº¦ã€‚ä¸ºäº†ä¿æŒå›¾åƒä¸­å¯¹è±¡çš„çºµæ¨ªæ¯”ï¼Œè°ƒæ•´å¤§å°çš„æ¨èæ–¹æ³•æ˜¯ç¼©æ”¾å›¾åƒï¼Œä½¿æœ€é•¿çš„å°ºå¯¸ç­‰äºæˆ‘ä»¬çš„ç›®æ ‡å°ºå¯¸ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°åšåˆ°è¿™ä¸€ç‚¹:

```
 # https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/anchors.py

def calculate_resized_gt_wh(gt_wh, image_sizes, target_image_size=640):
    """
    Given an array of bounding box  widths and heights, and their corresponding image sizes,
    resize these relative to the specified target image size.

    This function assumes that resizing will be performed by scaling the image such that the longest
    side is equal to the given target image size.

    :param gt_wh: an array of shape [N, 2] containing the raw width and height of each box.
    :param image_sizes: an array of shape [N, 2] or [1, 2] containing the width and height of the image corresponding to each box.
    :param target_image_size: the size of the images that will be used during training.

    """
    normalized_gt_wh = gt_wh / image_sizes
    target_image_sizes = (
        target_image_size * image_sizes / image_sizes.max(1, keepdims=True)
    )

    resized_gt_wh = target_image_sizes * normalized_gt_wh

    tiny_boxes_exist = (resized_gt_wh < 3).any(1).sum()
    if tiny_boxes_exist:
        print(
            f"""WARNING: Extremely small objects found.
            {tiny_boxes_exist} of {len(resized_gt_wh)} labels are < 3 pixels in size. These will be removed
            """
        )
        resized_gt_wh = resized_gt_wh[(resized_gt_wh >= 2.0).any(1)]

    return resized_gt_wh
```

![](img/d0c2cd4fdc0e3728f844efad571030d3.png)

æˆ–è€…ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºæˆ‘ä»¬æ‰€æœ‰çš„å›¾åƒéƒ½æ˜¯ç›¸åŒçš„å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°æŒ‡å®šä¸€ä¸ªå›¾åƒå¤§å°ã€‚

![](img/68f37235f35cad4cd70a215f36f89cad.png)

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜è¿‡æ»¤æ‰äº†ç›¸å¯¹äºæ–°å›¾åƒå°ºå¯¸è€Œè¨€éå¸¸å°(é«˜åº¦æˆ–å®½åº¦éƒ½å°äº 3 ä¸ªåƒç´ )çš„ä»»ä½•æ¡†ï¼Œå› ä¸ºè¿™äº›æ¡†é€šå¸¸å¤ªå°è€Œæ²¡æœ‰ç”¨ï¼

**è®¡ç®—æœ€ä½³å¯èƒ½å›å¿†**

æ—¢ç„¶æˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸­æœ‰äº†æ‰€æœ‰åœ°é¢çœŸå®æ¡†çš„å®½åº¦å’Œé«˜åº¦ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹è¯„ä¼°æˆ‘ä»¬å½“å‰çš„é”šæ¡†:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/anchors.py

def calculate_best_possible_recall(anchors, gt_wh):
    """
    Given a tensor of anchors and and an array of widths and heights for each bounding box in the dataset,
    calculate the best possible recall that can be obtained if every box was matched to an appropriate anchor.

    :param anchors: a tensor of shape [N, 2] representing the width and height of each anchor
    :param gt_wh: a tensor of shape [N, 2] representing the width and height of each ground truth bounding box

    """
    best_anchor_ratio = calculate_best_anchor_ratio(anchors=anchors, wh=gt_wh)
    best_possible_recall = (
        (best_anchor_ratio > 1.0 / LOSS_ANCHOR_MULTIPLE_THRESHOLD).float().mean()
    )

    return best_possible_recall
```

![](img/16884c0f821da6bd477dbc5ee11d7c62.png)

ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½“å‰çš„é”šç›’å¾ˆé€‚åˆè¿™ä¸ªæ•°æ®é›†ï¼›è¿™å¾ˆæœ‰é“ç†ï¼Œå› ä¸ºè¿™äº›å›¾åƒä¸ã€Šå¯å¯ã€‹ä¸­çš„å›¾åƒéå¸¸ç›¸ä¼¼ã€‚

*è¿™æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ*

åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œä½ å¯èƒ½æƒ³çŸ¥é“ï¼Œæˆ‘ä»¬åˆ°åº•æ˜¯å¦‚ä½•è®¡ç®—æœ€ä½³å¯èƒ½çš„å›å¿†ã€‚è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè®©æˆ‘ä»¬æ‰‹åŠ¨å®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚

ç›´è§‰ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªé”šå¯ä»¥åŒ¹é…åˆ°æ¯ä¸ªåœ°é¢çœŸç›¸æ¡†ã€‚è™½ç„¶æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†å®ƒæ¡†æ¶åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜æ¥åšåˆ°è¿™ä¸€ç‚¹â€”â€”æˆ‘ä»¬å¦‚ä½•å°†æ¯ä¸ªåœ°é¢çœŸç›¸æ¡†ä¸å…¶æœ€ä½³é”šåŒ¹é…â€”â€”ä½†è¿™å°†ä¸ºæˆ‘ä»¬è¯•å›¾åšçš„äº‹æƒ…å¸¦æ¥å¾ˆå¤šå¤æ‚æ€§ã€‚

ç»™å®šä¸€ä¸ªé”šå®šç›’ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•æ¥è¡¡é‡å®ƒèƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šé€‚åˆåœ°é¢çœŸç›¸ç›’ã€‚è®©æˆ‘ä»¬ç ”ç©¶ä¸€ç§å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹çš„æ–¹æ³•ï¼Œä»å•ä¸ªåœ°é¢çœŸç›¸æ¡†çš„å®½åº¦å’Œé«˜åº¦å¼€å§‹ã€‚

![](img/bdc8c397dadcfbc89399084a84bfe09e.png)

å¯¹äºæ¯ä¸ªé”šç›’ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥å…¶é«˜åº¦å’Œå®½åº¦ä¸åœ°é¢çœŸå®ç›®æ ‡çš„é«˜åº¦å’Œå®½åº¦çš„æ¯”ç‡ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥äº†è§£æœ€å¤§çš„å·®å¼‚åœ¨å“ªé‡Œã€‚

![](img/093723d14ba60fac4f093a9eb12a4936.png)

å› ä¸ºè¿™äº›æ¯”ç‡çš„æ¯”ä¾‹å°†å–å†³äºé”šç›’çš„è¾¹æ˜¯å¤§äºè¿˜æ˜¯å°äºæˆ‘ä»¬çš„åœ°é¢çœŸå€¼ç›’çš„è¾¹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—å€’æ•°å¹¶å–æ¯ä¸ªé”šçš„æœ€å°æ¯”ç‡æ¥ç¡®ä¿æˆ‘ä»¬çš„å¹…åº¦åœ¨èŒƒå›´[0ï¼Œ1]å†…ã€‚

![](img/bfee269bf2b0e0ee9c155e37f7f13410.png)

ç”±æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ªæŒ‡ç¤ºï¼Œå³æ¯ä¸ªé”šç›’çš„å®½åº¦å’Œé«˜åº¦ç‹¬ç«‹åœ°â€œé€‚åˆâ€æˆ‘ä»¬çš„åœ°é¢çœŸå®ç›®æ ‡çš„ç¨‹åº¦ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•è¯„ä¼°å®½åº¦å’Œé«˜åº¦çš„åŒ¹é…åº¦ï¼

ä¸€ç§æ–¹æ³•æ˜¯ï¼Œå¯¹æ¯ä¸ªé”šå–æœ€å°æ¯”ç‡ï¼›ä»£è¡¨æœ€ä¸ç¬¦åˆæˆ‘ä»¬ç°å®çš„ä¸€æ–¹ã€‚

![](img/cdd599fe338648e844707bf57f0ab4a0.png)

æˆ‘ä»¬ä¹‹æ‰€ä»¥åœ¨è¿™é‡Œé€‰æ‹©æœ€ä¸åˆé€‚çš„ä¸€è¾¹ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬çŸ¥é“å¦ä¸€è¾¹ä¸æˆ‘ä»¬çš„ç›®æ ‡*è‡³å°‘*ä»¥åŠæ‰€é€‰æ‹©çš„ä¸€è¾¹ç›¸åŒ¹é…ï¼›æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™æ˜¯æœ€åçš„æƒ…å†µï¼

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»è¿™äº›é€‰é¡¹ä¸­é€‰æ‹©æœ€åŒ¹é…çš„é”šæ¡†ï¼Œè¿™åªæ˜¯æœ€å¤§çš„å€¼ã€‚

![](img/0f4c5b1a3221ceeeb2e5f55e8136464a.png)

åœ¨æœ€å·®çš„æ‹Ÿåˆé€‰é¡¹ä¸­ï¼Œè¿™æ˜¯æˆ‘ä»¬é€‰æ‹©çš„åŒ¹é…ï¼

å›æƒ³ä¸€ä¸‹ï¼ŒæŸå¤±å‡½æ•°åªåŒ¹é…æ¯”åœ°é¢çœŸå®ç›®æ ‡çš„å¤§å°å¤§æˆ–å° 4 å€çš„é”šæ¡†ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥éªŒè¯è¿™ä¸ªé”šæ˜¯å¦åœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œå¹¶ä¸”å°†è¢«è®¤ä¸ºæ˜¯æˆåŠŸçš„åŒ¹é…ã€‚

æˆ‘ä»¬å¯ä»¥å¦‚ä¸‹æ‰€ç¤ºï¼Œå–æŸå¤±å€æ•°çš„å€’æ•°ï¼Œä»¥ç¡®ä¿å®ƒä¸æˆ‘ä»¬çš„ä»·å€¼åœ¨åŒä¸€èŒƒå›´å†…:

![](img/53c73d62d51e637ab144d9ad51e72845.png)

ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè‡³å°‘æˆ‘ä»¬çš„ä¸€ä¸ªé”šå¯ä»¥æˆåŠŸåœ°åŒ¹é…åˆ°æˆ‘ä»¬é€‰æ‹©çš„åœ°é¢çœŸå®ç›®æ ‡ï¼

æ—¢ç„¶æˆ‘ä»¬ç†è§£äº†æ­¥éª¤çš„é¡ºåºï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å°†ç›¸åŒçš„é€»è¾‘åº”ç”¨äºæˆ‘ä»¬æ‰€æœ‰çš„åŸºç¡€äº‹å®æ¡†ï¼Œä»¥æŸ¥çœ‹æˆ‘ä»¬å¯ä»¥ç”¨å½“å‰çš„é”šé›†è·å¾—å¤šå°‘åŒ¹é…:

![](img/7dda35d6a35187c6dc350ef8eebb6fef.png)

ç°åœ¨æˆ‘ä»¬å·²ç»è®¡ç®—äº†ï¼Œå¯¹äºæ¯ä¸ªåœ°é¢çœŸå€¼ç›’ï¼Œå®ƒæ˜¯å¦æœ‰ä¸€ä¸ªåŒ¹é…ã€‚æˆ‘ä»¬å¯ä»¥å–å¹³å‡åŒ¹é…æ•°æ¥æ‰¾å‡ºæœ€ä½³å¯èƒ½å›å¿†ï¼›åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™æ˜¯ 1ï¼Œæ­£å¦‚æˆ‘ä»¬å‰é¢çœ‹åˆ°çš„ï¼

![](img/02a3cd60d6363059d370f61c6dd58008.png)

**é€‰æ‹©æ–°çš„é”šç®±**

è™½ç„¶ä½¿ç”¨é¢„å®šä¹‰é”šå¯èƒ½æ˜¯ç±»ä¼¼æ•°æ®é›†çš„å¥½é€‰æ‹©ï¼Œä½†è¿™å¯èƒ½å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰æ•°æ®é›†ï¼Œä¾‹å¦‚ï¼ŒåŒ…å«å¤§é‡å°å¯¹è±¡çš„æ•°æ®é›†ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ›´å¥½çš„æ–¹æ³•å¯èƒ½æ˜¯é€‰æ‹©å…¨æ–°çš„é”šã€‚

è®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æ¶æ„éœ€è¦çš„é”šç‚¹æ•°é‡ã€‚

![](img/4a115ad453b9bf61a01c4ecc23fc267f.png)

ç°åœ¨ï¼ŒåŸºäºæˆ‘ä»¬çš„è¾¹ç•Œæ¡†ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªåˆç†çš„é”šæ¨¡æ¿çš„å®½åº¦å’Œé«˜åº¦ã€‚æˆ‘ä»¬å¯ä»¥ä¼°è®¡è¿™ä¸€ç‚¹çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨ [K-means](https://en.wikipedia.org/wiki/K-means_clustering) æ¥æ ¹æ®æˆ‘ä»¬éœ€è¦çš„é”šå¤§å°çš„æ•°é‡æ¥èšé›†æˆ‘ä»¬çš„åœ°é¢çœŸå®çºµæ¨ªæ¯”ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›è´¨å¿ƒä½œä¸ºæˆ‘ä»¬çš„å¼€å§‹ä¼°è®¡ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/anchors.py

def estimate_anchors(num_anchors, gt_wh):
    """
    Given a target number of anchors and an array of widths and heights for each bounding box in the dataset,
    estimate a set of anchors using the centroids from Kmeans clustering.

    :param num_anchors: the number of anchors to return
    :param gt_wh: an array of shape [N, 2] representing the width and height of each ground truth bounding box

    """
    print(f"Running kmeans for {num_anchors} anchors on {len(gt_wh)} points...")
    std_dev = gt_wh.std(0)
    proposed_anchors, _ = kmeans(
        gt_wh / std_dev, num_anchors, iter=30
    )  # divide by std so they are in approx same range
    proposed_anchors *= std_dev

    return proposed_anchors
```

![](img/cb326f88753c04388c71fe987eebe07f.png)

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ç»„é”šæ¨¡æ¿ï¼Œå¯ä»¥ç”¨ä½œèµ·ç‚¹ã€‚åƒä»¥å‰ä¸€æ ·ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è¿™äº›é”šç›’æ¥è®¡ç®—æˆ‘ä»¬çš„æœ€ä½³å¯èƒ½å›å¿†:

![](img/17db0e158f4178ff87e09855c0ba9413.png)

æˆ‘ä»¬å†æ¬¡çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æœ€ä½³å¯èƒ½å¬å›ç‡æ˜¯ 1ï¼Œè¿™æ„å‘³ç€è¿™äº›é”šå¤§å°ä¹Ÿå¾ˆé€‚åˆæˆ‘ä»¬çš„é—®é¢˜ï¼

è™½ç„¶åœ¨è¿™ç§æƒ…å†µä¸‹å¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[é—ä¼ ç®—æ³•](https://www.geeksforgeeks.org/genetic-algorithms/)è¿›ä¸€æ­¥æ”¹è¿›è¿™äº›é”šã€‚éµå¾ªè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª*é€‚åº”åº¦*(æˆ–å¥–åŠ±)å‡½æ•°æ¥è¡¡é‡æˆ‘ä»¬çš„é”šç›’ä¸æˆ‘ä»¬çš„æ•°æ®åŒ¹é…çš„ç¨‹åº¦ï¼Œå¹¶å¯¹æˆ‘ä»¬çš„é”šå¤§å°è¿›è¡Œå°çš„éšæœºæ”¹å˜ï¼Œä»¥å°è¯•å¹¶æœ€å¤§åŒ–è¯¥å‡½æ•°ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„é€‚åº”åº¦å‡½æ•°å®šä¹‰å¦‚ä¸‹:

```
def anchor_fitness(anchors, wh):
    """
    A fitness function that can be used to evolve a set of anchors. This function calculates the mean best anchor ratio
    for all matches that are within the multiple range considered during the loss calculation.
    """
    best_anchor_ratio = calculate_best_anchor_ratio(anchors=anchors, gt_wh=wh)
    return (
        best_anchor_ratio
        * (best_anchor_ratio > 1 / LOSS_ANCHOR_MULTIPLE_THRESHOLD).float()
    ).mean()
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ºæ¯åœºæ¯”èµ›å–æœ€ä½³é”šå®šæ¯”ï¼Œåœ¨æŸå¤±è®¡ç®—æ—¶ä¼šè€ƒè™‘ã€‚å¦‚æœä¸€ä¸ªå®šä½æ¡†æ¯”å®ƒåŒ¹é…çš„è¾¹ç•Œæ¡†å¤§æˆ–å°å››å€ä»¥ä¸Šï¼Œå®ƒä¸ä¼šå½±å“æˆ‘ä»¬çš„åˆ†æ•°ã€‚è®©æˆ‘ä»¬ç”¨è¿™ä¸ªæ¥è®¡ç®—æˆ‘ä»¬å»ºè®®çš„é”šå°ºå¯¸çš„é€‚åˆåº¦åˆ†æ•°:

![](img/180d60c8ea370e72d79e99ca27906c20.png)

ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨ä¼˜åŒ–æˆ‘ä»¬çš„é”šæ—¶ä½¿ç”¨è¿™ä¸ªä½œä¸ºé€‚åº”åº¦å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/8babb2e0deb2109a17c3ddca66a99c05.png)

æ£€æŸ¥è¿™ä¸ªå‡½æ•°çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºæŒ‡å®šçš„è¿­ä»£æ¬¡æ•°ï¼Œæˆ‘ä»¬åªæ˜¯ä»æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºå™ªå£°ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥æ”¹å˜æˆ‘ä»¬çš„é”šå¤§å°ã€‚å¦‚æœè¿™ä¸€å˜åŒ–å¯¼è‡´åˆ†æ•°å¢åŠ ï¼Œæˆ‘ä»¬å°†è¿™äº›ä½œä¸ºæˆ‘ä»¬çš„é”šå°ºå¯¸ï¼

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/yolov7/anchors.py

def evolve_anchors(
    proposed_anchors,
    gt_wh,
    num_iterations=1000,
    mutation_probability=0.9,
    mutation_noise_mean=1,
    mutation_noise_std=0.1,
    anchor_fitness_fn=anchor_fitness,
    verbose=False,
):
    """
    Use a genetic algorithm to mutate the given anchors to try and optimise them based on the given widths and heights of the
    ground truth boxes based on the provided fitness function. Anchor dimensions are mutated by adding random noise sampled
    from a normal distribution with the mean and standard deviation provided.

    :param proposed_anchors: a tensor containing the aspect ratios of the anchor boxes to evolve
    :param gt_wh: a tensor of shape [N, 2] representing the width and height of each ground truth bounding box
    :param num_generations: the number of iterations for which to run the algorithm
    :param mutation_probability: the probability that each anchor dimension is mutated during each iteration
    :param mutation_noise_mean: the mean of the normal distribution from which the mutation noise will be sampled
    :param mutation_noise_std: the standard deviation of the normal distribution from which the mutation noise will be sampled
    :param anchor_fitness_fn: the reward function that will be used during the optimization process. This should accept proposed_anchors and gt_wh as arguments
    :param verbose: if True, the value of the fitness function will be printed at the end of each iteration

    """
    best_fitness = anchor_fitness_fn(proposed_anchors, gt_wh)
    anchor_shape = proposed_anchors.shape

    pbar = tqdm(range(num_iterations), desc=f"Evolving anchors with Genetic Algorithm:")
    for i, _ in enumerate(pbar):
        # Define mutation by sampling noise from a normal distribution 
        anchor_mutation = np.ones(anchor_shape)
        anchor_mutation = (
            (np.random.random(anchor_shape) < mutation_probability)
            * np.random.randn(*anchor_shape)
            * mutation_noise_std
            + mutation_noise_mean
        ).clip(0.3, 3.0)

        mutated_anchors = (proposed_anchors.copy() * anchor_mutation).clip(min=2.0)
        mutated_anchor_fitness = anchor_fitness_fn(mutated_anchors, gt_wh)

        if mutated_anchor_fitness > best_fitness:
            best_fitness, proposed_anchors = (
                mutated_anchor_fitness,
                mutated_anchors.copy(),
            )
            pbar.desc = (
                f"Evolving anchors with Genetic Algorithm: fitness = {best_fitness:.4f}"
            )
            if verbose:
                print(f"Iteration: {i}, Fitness: {best_fitness}")

    return proposed_anchors
```

è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦æé«˜äº†æˆ‘ä»¬çš„åˆ†æ•°:

![](img/7ea55b59cdec851738b8a4aa1f3c2546.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬è¿›åŒ–çš„é”šæ¯”æˆ‘ä»¬æœ€åˆæå‡ºçš„é”šå…·æœ‰æ›´å¥½çš„é€‚åº”åº¦åˆ†æ•°ï¼

ç°åœ¨ï¼Œå‰©ä¸‹è¦åšçš„å°±æ˜¯è€ƒè™‘æ¯ä¸ªé”šçš„æœ€å°ç»´åº¦ï¼ŒæŒ‰ç…§ç²—ç•¥çš„å‡åºå¯¹é”šè¿›è¡Œæ’åºã€‚

![](img/b1e5766160afc55d2b9c51f159059fd9.png)

**å°†æ‰€æœ‰è¿™äº›æ”¾åœ¨ä¸€èµ·**

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°åœ¨ä¸€ä¸ªæ­¥éª¤ä¸­ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†è®¡ç®—é”šã€‚

![](img/816adb8319fd3617200f1c5e6d4e47a3.png)

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºæˆ‘ä»¬çš„æœ€ä½³å¬å›ç‡å·²ç»å¤§äºé˜ˆå€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä¿æŒåŸå§‹çš„é”šå¤§å°ï¼

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çš„é”šç‚¹å¤§å°å‘ç”Ÿå˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ‰€ç¤ºè¿›è¡Œæ›´æ–°:

![](img/84b90e4a69418d2fd9fe9c52d96c008e.png)

## è·‘æ­¥è®­ç»ƒ

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ç ”ç©¶äº†åœ¨åŸå§‹åŸ¹è®­é…æ–¹ä¸­ä½¿ç”¨çš„ä¸€äº›æŠ€æœ¯ï¼Œè®©æˆ‘ä»¬æ›´æ–°æˆ‘ä»¬çš„åŸ¹è®­è„šæœ¬ï¼Œä»¥åŒ…æ‹¬å…¶ä¸­çš„ä¸€äº›åŠŸèƒ½ã€‚æ›´æ–°åçš„è„šæœ¬å¦‚ä¸‹æ‰€ç¤º:

```
# https://github.com/Chris-hughes10/Yolov7-training/blob/main/examples/train_cars.py

import random
from functools import partial
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from func_to_script import script
from PIL import Image
from pytorch_accelerated.callbacks import (
    ModelEmaCallback,
    ProgressBarCallback,
    SaveBestModelCallback,
    get_default_callbacks,
)
from pytorch_accelerated.schedulers import CosineLrScheduler
from torch.utils.data import Dataset

from yolov7 import create_yolov7_model
from yolov7.dataset import (
    Yolov7Dataset,
    create_base_transforms,
    create_yolov7_transforms,
    yolov7_collate_fn,
)
from yolov7.evaluation import CalculateMeanAveragePrecisionCallback
from yolov7.loss_factory import create_yolov7_loss
from yolov7.mosaic import MosaicMixupDataset, create_post_mosaic_transform
from yolov7.trainer import Yolov7Trainer, filter_eval_predictions
from yolov7.utils import SaveBatchesCallback, Yolov7ModelEma

def load_cars_df(annotations_file_path, images_path):
    all_images = sorted(set([p.parts[-1] for p in images_path.iterdir()]))
    image_id_to_image = {i: im for i, im in enumerate(all_images)}
    image_to_image_id = {v: k for k, v, in image_id_to_image.items()}

    annotations_df = pd.read_csv(annotations_file_path)
    annotations_df.loc[:, "class_name"] = "car"
    annotations_df.loc[:, "has_annotation"] = True

    # add 100 empty images to the dataset
    empty_images = sorted(set(all_images) - set(annotations_df.image.unique()))
    non_annotated_df = pd.DataFrame(list(empty_images)[:100], columns=["image"])
    non_annotated_df.loc[:, "has_annotation"] = False
    non_annotated_df.loc[:, "class_name"] = "background"

    df = pd.concat((annotations_df, non_annotated_df))

    class_id_to_label = dict(
        enumerate(df.query("has_annotation == True").class_name.unique())
    )
    class_label_to_id = {v: k for k, v in class_id_to_label.items()}

    df["image_id"] = df.image.map(image_to_image_id)
    df["class_id"] = df.class_name.map(class_label_to_id)

    file_names = tuple(df.image.unique())
    random.seed(42)
    validation_files = set(random.sample(file_names, int(len(df) * 0.2)))
    train_df = df[~df.image.isin(validation_files)]
    valid_df = df[df.image.isin(validation_files)]

    lookups = {
        "image_id_to_image": image_id_to_image,
        "image_to_image_id": image_to_image_id,
        "class_id_to_label": class_id_to_label,
        "class_label_to_id": class_label_to_id,
    }
    return train_df, valid_df, lookups

class CarsDatasetAdaptor(Dataset):
    def __init__(
        self,
        images_dir_path,
        annotations_dataframe,
        transforms=None,
    ):
        self.images_dir_path = Path(images_dir_path)
        self.annotations_df = annotations_dataframe
        self.transforms = transforms

        self.image_idx_to_image_id = {
            idx: image_id
            for idx, image_id in enumerate(self.annotations_df.image_id.unique())
        }
        self.image_id_to_image_idx = {
            v: k for k, v, in self.image_idx_to_image_id.items()
        }

    def __len__(self) -> int:
        return len(self.image_idx_to_image_id)

    def __getitem__(self, index):
        image_id = self.image_idx_to_image_id[index]
        image_info = self.annotations_df[self.annotations_df.image_id == image_id]
        file_name = image_info.image.values[0]
        assert image_id == image_info.image_id.values[0]

        image = Image.open(self.images_dir_path / file_name).convert("RGB")
        image = np.array(image)

        image_hw = image.shape[:2]

        if image_info.has_annotation.any():
            xyxy_bboxes = image_info[["xmin", "ymin", "xmax", "ymax"]].values
            class_ids = image_info["class_id"].values
        else:
            xyxy_bboxes = np.array([])
            class_ids = np.array([])

        if self.transforms is not None:
            transformed = self.transforms(
                image=image, bboxes=xyxy_bboxes, labels=class_ids
            )
            image = transformed["image"]
            xyxy_bboxes = np.array(transformed["bboxes"])
            class_ids = np.array(transformed["labels"])

        return image, xyxy_bboxes, class_ids, image_id, image_hw

DATA_PATH = Path("/".join(Path(__file__).absolute().parts[:-2])) / "data/cars"

@script
def main(
    data_path: str = DATA_PATH,
    image_size: int = 640,
    pretrained: bool = False,
    num_epochs: int = 300,
    batch_size: int = 8,
):

    # load data
    data_path = Path(data_path)
    images_path = data_path / "training_images"
    annotations_file_path = data_path / "annotations.csv"
    train_df, valid_df, lookups = load_cars_df(annotations_file_path, images_path)
    num_classes = 1

    # create datasets
    train_ds = DatasetAdaptor(
        images_path, train_df, transforms=create_base_transforms(image_size)
    )
    eval_ds = DatasetAdaptor(images_path, valid_df)

    mds = MosaicMixupDataset(
        train_ds,
        apply_mixup_probability=0.15,
        post_mosaic_transforms=create_post_mosaic_transform(
            output_height=image_size, output_width=image_size
        ),
    )
    if pretrained:
        # disable mosaic if finetuning
        mds.disable()

    train_yds = Yolov7Dataset(
        mds,
        create_yolov7_transforms(training=True, image_size=(image_size, image_size)),
    )
    eval_yds = Yolov7Dataset(
        eval_ds,
        create_yolov7_transforms(training=False, image_size=(image_size, image_size)),
    )

    # create model, loss function and optimizer
    model = create_yolov7_model(
        architecture="yolov7", num_classes=num_classes, pretrained=pretrained
    )
    param_groups = model.get_parameter_groups()

    loss_func = create_yolov7_loss(model, image_size=image_size)

    optimizer = torch.optim.SGD(
        param_groups["other_params"], lr=0.01, momentum=0.937, nesterov=True
    )

    # create evaluation callback and trainer
    calculate_map_callback = (
        CalculateMeanAveragePrecisionCallback.create_from_targets_df(
            targets_df=valid_df.query("has_annotation == True")[
                ["image_id", "xmin", "ymin", "xmax", "ymax", "class_id"]
            ],
            image_ids=set(valid_df.image_id.unique()),
            iou_threshold=0.2,
        )
    )

    trainer = Yolov7Trainer(
        model=model,
        optimizer=optimizer,
        loss_func=loss_func,
        filter_eval_predictions_fn=partial(
            filter_eval_predictions, confidence_threshold=0.01, nms_threshold=0.3
        ),
        callbacks=[
            calculate_map_callback,
            ModelEmaCallback(
                decay=0.9999,
                model_ema=Yolov7ModelEma,
                callbacks=[ProgressBarCallback, calculate_map_callback],
            ),
            SaveBestModelCallback(watch_metric="map", greater_is_better=True),
            SaveBatchesCallback("./batches", num_images_per_batch=3),
            *get_default_callbacks(progress_bar=True),
        ],
    )

    # calculate scaled weight decay and gradient accumulation steps
    total_batch_size = (
        batch_size * trainer._accelerator.num_processes
    )  # batch size across all processes

    nominal_batch_size = 64
    num_accumulate_steps = max(round(nominal_batch_size / total_batch_size), 1)
    base_weight_decay = 0.0005
    scaled_weight_decay = (
        base_weight_decay * total_batch_size * num_accumulate_steps / nominal_batch_size
    )

    optimizer.add_param_group(
        {"params": param_groups["conv_weights"], "weight_decay": scaled_weight_decay}
    )

    # run training
    trainer.train(
        num_epochs=num_epochs,
        train_dataset=train_yds,
        eval_dataset=eval_yds,
        per_device_batch_size=batch_size,
        create_scheduler_fn=CosineLrScheduler.create_scheduler_fn(
            num_warmup_epochs=5,
            num_cooldown_epochs=5,
            k_decay=2,
        ),
        collate_fn=yolov7_collate_fn,
        gradient_accumulation_steps=num_accumulate_steps,
    )

if __name__ == "__main__":
    main()
```

å†æ¬¡å¯åŠ¨è®­ç»ƒï¼Œ[å¦‚æœ¬æ–‡æ‰€è¿°](https://pytorch-accelerated.readthedocs.io/en/latest/quickstart.html)ï¼Œä½¿ç”¨å•ä¸ª V100 GPU å¹¶å¯ç”¨ fp16ï¼Œåœ¨ 300 ä¸ªæ—¶æœŸåï¼Œæˆ‘ä»¬è·å¾—äº†æ¨¡å‹å’Œ EMA æ¨¡å‹çš„`0.997`å›¾ï¼›æ¯”æˆ‘ä»¬çš„è¿ç§»å­¦ä¹ è¿è¡Œç•¥æœ‰å¢åŠ ï¼Œå¹¶ä¸”å¯èƒ½æ˜¯åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šå¯ä»¥å®ç°çš„æœ€é«˜æ€§èƒ½ï¼

# ç»“è®º

å¸Œæœ›è¿™å·²ç»æä¾›äº† YOLOv7 åŸ¹è®­è¿‡ç¨‹ä¸­ä¸€äº›æœ€æœ‰è¶£çš„æƒ³æ³•çš„å…¨é¢æ¦‚è¿°ï¼Œä»¥åŠå¦‚ä½•åœ¨å®šåˆ¶åŸ¹è®­è„šæœ¬ä¸­åº”ç”¨è¿™äº›æƒ³æ³•ã€‚

*å¤åˆ¶è¿™ç¯‡æ–‡ç« æ‰€éœ€çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥ä½œä¸ºç¬”è®°æœ¬* [*åœ¨è¿™é‡Œ*](https://github.com/Chris-hughes10/Yolov7-training/blob/main/blog%20post/Yolo7%20blog%20post.ipynb) *ã€‚è™½ç„¶åœ¨æ•´ç¯‡æ–‡ç« ä¸­ä½¿ç”¨äº†ä»£ç ç‰‡æ®µï¼Œä½†è¿™ä¸»è¦æ˜¯å‡ºäºç¾è§‚çš„ç›®çš„ï¼Œè¯·éµä»ç¬”è®°æœ¬ï¼Œè€Œ*[](https://github.com/Chris-hughes10/Yolov7-training)**ä¸ºå·¥ä½œä»£ç ã€‚**

*[*å…‹é‡Œæ–¯Â·ä¼‘æ–¯*](https://www.linkedin.com/in/chris-hughes1/) *å’Œ*[T21ã€ä¼¯çº³ç‰¹ã€‘æ™®ä¼Šæ ¼é˜µè¥](https://www.linkedin.com/in/bepuca/) *éƒ½åœ¨é¢†è‹±**

# *ä½¿ç”¨çš„æ•°æ®é›†*

*è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª Kaggle çš„[æ±½è½¦ç‰©ä½“æ£€æµ‹æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä½œä¸º](https://www.kaggle.com/datasets/sshikamaru/car-object-detection)[ç«èµ›å…­(tjmachinelearning.com)](https://tjmachinelearning.com/2020/comp_six)çš„ä¸€éƒ¨åˆ†å…¬å¼€æä¾›ã€‚è¯¥æ•°æ®é›†[ç»å¸¸ç”¨äº](https://www.kaggle.com/datasets/sshikamaru/car-object-detection/code)å­¦ä¹ ç›®çš„ã€‚*

*è™½ç„¶è¿™ä¸ªæ•°æ®é›†æ²¡æœ‰æ˜ç¡®çš„è®¸å¯ï¼Œä½†æ˜¯æˆ‘ä»¬ä»ä½œè€…é‚£é‡Œå¾—åˆ°äº†æ˜ç¡®çš„è®¸å¯ï¼Œå¯ä»¥å°†å®ƒä½œä¸ºæœ¬æ–‡çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ã€‚é™¤éå¦æœ‰è¯´æ˜ï¼Œæœ¬æ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰å›¾åƒéƒ½æ¥è‡ªè¯¥æ•°æ®é›†ã€‚*

# *å‚è€ƒ*

*   *[ã€2207.02696ã€‘yolov 7:å¯è®­ç»ƒçš„èµ å“è¢‹ä¸ºå®æ—¶ç‰©ä½“æ¢æµ‹å™¨æ ‘ç«‹äº†æ–°çš„è‰ºæœ¯æ°´å¹³(arxiv.org)](https://arxiv.org/abs/2207.02696)*
*   *[å›åº”å…³äº yolov 5(roboflow.com)çš„äº‰è®®](https://blog.roboflow.com/yolov4-versus-yolov5/)*
*   *[ã€2011.08036ã€‘Scaled-yolov 4:ç¼©æ”¾è·¨çº§å±€éƒ¨ç½‘ç»œ(arxiv.org)](https://arxiv.org/abs/2011.08036)*
*   *[WongKinYiu/yolor:è®ºæ–‡çš„å®ç°â€”â€”ä½ åªå­¦ä¹ ä¸€ç§è¡¨ç¤ºæ³•:å¤šä»»åŠ¡ç»Ÿä¸€ç½‘ç»œ(https://arxiv.org/abs/2105.04206)(github.com)](https://github.com/WongKinYiu/yolor)*
*   *[ultralytics/yolov5: YOLOv5ğŸš€åœ¨ py torch>ONNX>CoreML>TF lite(github.com)](https://github.com/ultralytics/yolov5)*
*   *Chris-Hughes 10/yolov 7-training:yolov 7 æ¨¡å‹ç³»åˆ—çš„ä¸€ä¸ªå¹²å‡€çš„æ¨¡å—åŒ–å®ç°ï¼Œå®ƒä½¿ç”¨å®˜æ–¹çš„é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶å¸¦æœ‰ç”¨äºè®­ç»ƒæ¨¡å‹æ‰§è¡Œå®šåˆ¶(é COCO)ä»»åŠ¡çš„å®ç”¨ç¨‹åºã€‚(github.com)*
*   *ã€WongKinYiu/yolov7:çº¸çš„å®ç°â€”â€”yolov 7:å¯è®­ç»ƒçš„å…è´¹åŒ…ä¸ºå®æ—¶ç‰©ä½“æ¢æµ‹å™¨æ ‘ç«‹äº†æ–°çš„è‰ºæœ¯æ°´å¹³(github.com)*
*   *[ç›¸å†Œ:å¿«é€Ÿçµæ´»çš„å›¾åƒå¢å¼º](https://albumentations.ai/)*
*   *[éæœ€å¤§æŠ‘åˆ¶è§£é‡Š|è®ºæ–‡ä»£ç ](https://paperswithcode.com/method/non-maximum-suppression)*
*   *[ã€1612.03144ã€‘ç”¨äºç›®æ ‡æ£€æµ‹çš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ(arxiv.org)](https://arxiv.org/abs/1612.03144)*
*   *[Jaccard ç´¢å¼•â€”ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Jaccard_index)*
*   *[ã€2103.14259ã€‘OTA:ç›®æ ‡æ£€æµ‹çš„æœ€ä½³è¿è¾“åˆ†é…(arxiv.org)](https://arxiv.org/abs/2103.14259)*
*   *[ã€2107.08430ã€‘YOLOX:2021 å¹´è¶…è¶Š YOLO ç³»åˆ—(arxiv.org)](https://arxiv.org/abs/2107.08430)*
*   *Chris-Hughes 10/pytorch-accelerated:ä¸€ä¸ªè½»é‡çº§åº“ï¼Œæ—¨åœ¨é€šè¿‡æä¾›ä¸€ä¸ªæœ€å°ä½†å¯æ‰©å±•çš„è®­ç»ƒå¾ªç¯æ¥åŠ é€Ÿ py torch æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è®­ç»ƒå¾ªç¯è¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥å¤„ç†å¤§å¤šæ•°ç”¨ä¾‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿåˆ©ç”¨ä¸åŒçš„ç¡¬ä»¶é€‰é¡¹ï¼Œè€Œæ— éœ€æ›´æ”¹ä»£ç ã€‚æ–‡ä»¶:https://pytorch-accelerated.readthedocs.io/en/latest/(github.com)*
*   *[åŸ¹è®­å¸ˆâ€” pytorch åŠ é€Ÿ 0.1.3 æ–‡æ¡£](https://pytorch-accelerated.readthedocs.io/en/latest/trainer.html#pytorch_accelerated.trainer.Trainer)*
*   *[è¯„ä¼°æªæ–½(ä¿¡æ¯æ£€ç´¢)â€”ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)*
*   *[pycocotools PyPI](https://pypi.org/project/pycocotools/)*
*   *[å›è°ƒâ€” pytorch åŠ é€Ÿçš„ 0.1.3 æ–‡æ¡£](https://pytorch-accelerated.readthedocs.io/en/latest/callbacks.html#creating-new-callbacks)*
*   *[å¿«é€Ÿå…¥é—¨â€” pytorch åŠ é€Ÿçš„ 0.1.3 æ–‡æ¡£](https://pytorch-accelerated.readthedocs.io/en/latest/quickstart.html)*
*   *ã€arxiv.org *
*   *[æ·±åº¦å­¦ä¹ åŸºç¡€â€”â€”ä½“é‡è¡°å‡|ä½œè€…:Sophia Yang | Analytics vid hya | Medium](https://medium.com/analytics-vidhya/deep-learning-basics-weight-decay-3c68eb4344e9)*
*   *[ã€1812.01187ã€‘åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå›¾åƒåˆ†ç±»çš„é”¦å›Šå¦™è®¡(arxiv.org)](https://arxiv.org/abs/1812.01187)*
*   *[ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦ç§¯ç´¯ï¼Ÿ|ä½œè€…æ‹‰å…¹Â·ç½—æ»•åšæ ¼|èµ°å‘æ•°æ®ç§‘å­¦](/what-is-gradient-accumulation-in-deep-learning-ec034122cfa)*
*   *[Utils â€” pytorch åŠ é€Ÿçš„ 0.1.3 æ–‡æ¡£](https://pytorch-accelerated.readthedocs.io/en/latest/utils.html#pytorch_accelerated.utils.ModelEma)*
*   *[é—ä¼ ç®—æ³•â€”â€”GeeksforGeeks](https://www.geeksforgeeks.org/genetic-algorithms/)*
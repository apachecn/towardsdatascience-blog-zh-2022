# Word2Vec ç®€ä»‹(è·³è¿‡ç¨‹åº)

> åŸæ–‡ï¼š<https://towardsdatascience.com/introduction-to-word2vec-skip-gram-cb3e9533bcf1>

## NLP åŸºç¡€

## Python ä¸­çš„å•è¯åµŒå…¥ç®€ä»‹

å½“å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—ã€‚ç”¨æ•°å­—æ•°æ®è¡¨ç¤ºæ–‡æœ¬æœ‰ä¸åŒçš„æ–¹å¼ã€‚å•è¯åŒ…(åˆå BOW)æ˜¯ä¸€ç§ç”¨æ•°å­—è¡¨ç¤ºæ–‡æœ¬çš„æµè¡Œè€Œç®€å•çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå•è¯åŒ…ä¸­æ²¡æœ‰å•è¯ç›¸ä¼¼æ€§çš„æ¦‚å¿µï¼Œå› ä¸ºæ¯ä¸ªå•è¯éƒ½æ˜¯ç‹¬ç«‹è¡¨ç¤ºçš„ã€‚å› æ­¤ï¼Œåƒ*ã€ã€å¤ªæ£’äº†ã€‘ã€*ã€ã€å¤ªæ£’äº†ã€‘ã€*è¿™æ ·çš„å•è¯çš„åµŒå…¥å½¼æ­¤ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°±åƒå®ƒä»¬ä¸å•è¯*ã€ã€ä¹¦ã€‘ã€*çš„åµŒå…¥ä¸€æ ·ã€‚*

å•è¯åµŒå…¥æ˜¯ç”¨æ•°å­—è¡¨ç¤ºæ–‡æœ¬çš„å¦ä¸€ç§å¥½æ–¹æ³•ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ¯ä¸ªå•è¯éƒ½ç”±ä¸€ä¸ªåµŒå…¥çš„å¯†é›†å‘é‡(å³ä¸€ä¸ªæ•°å­—æ•°ç»„)æ¥è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ä¿ç•™äº†å•è¯ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ•è·å•è¯ç›¸ä¼¼æ€§ã€‚å‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„å•è¯åœ¨å‘é‡ç©ºé—´ä¸­å…·æœ‰æ›´è¿‘çš„å‘é‡ã€‚å› æ­¤ï¼Œå•è¯*â€˜ä¼Ÿå¤§â€™*å¾ˆå¯èƒ½æ¯”*â€˜ä¹¦â€™*æœ‰æ›´å¤šä¸*â€˜ç‰›é€¼â€™*ç›¸ä¼¼çš„åµŒå…¥ã€‚

![](img/ab85e54ec85a1f15ad802b50fd45f253.png)

ç”± [Sebastian Svenson](https://unsplash.com/@sebastiansvenson?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹å•è¯åµŒå…¥è¿›è¡Œæ¦‚è¿°ï¼Œå°¤å…¶æ˜¯ä¸€ç§ç§°ä¸º Word2Vec çš„åµŒå…¥ç®—æ³•ï¼Œå¹¶æ·±å…¥äº†è§£è¯¥ç®—æ³•å¦‚ä½•åœ¨ Python ä¸­çš„ä¸€ä¸ªç©å…·ç¤ºä¾‹ä¸Šè¿è¡Œã€‚

# ğŸ“œWord2Vec (Skipgram)æ¦‚è¿°

![](img/b9b0d1afdce32b552be5a35c0395029b.png)

å›¾ç‰‡ä½œè€…|é¢„å¤„ç†ä¸€ä¸ªç¤ºä¾‹æ–‡æ¡£çš„å¯¹æ¯”:â€œHello worldï¼â€æœ‰ä¸¤ç§æ–¹æ³•ã€‚å‡è®¾å•è¯åŒ…æ–¹æ³•çš„è¯æ±‡é‡ä¸º 5ï¼Œå•è¯åµŒå…¥çš„åµŒå…¥é‡ä¸º 3ã€‚

å½“ä½¿ç”¨å•è¯åŒ…æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ *n* å°†æ–‡æœ¬è½¬æ¢æˆä¸€ä¸ª *m* çš„æ–‡æ¡£æœ¯è¯­çŸ©é˜µï¼Œå…¶ä¸­ *m* æ˜¯æ–‡æ¡£/æ–‡æœ¬è®°å½•çš„æ•°é‡ï¼Œè€Œ *n* æ˜¯æ‰€æœ‰æ–‡æ¡£ä¸­å”¯ä¸€å•è¯çš„æ•°é‡ã€‚è¿™é€šå¸¸ä¼šäº§ç”Ÿä¸€ä¸ªå¾ˆå¤§çš„ç¨€ç–çŸ©é˜µã€‚å¦‚æœä½ æƒ³è¯¦ç»†äº†è§£è¿™ç§æ–¹æ³•ï¼Œè¯·æŸ¥çœ‹æœ¬æ•™ç¨‹ã€‚

åœ¨å•è¯åµŒå…¥ä¸­ï¼Œæ¯ä¸ªå•è¯ç”±ä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼Œé€šå¸¸å¤§å°ä¸º [100 åˆ° 300](https://datascience.stackexchange.com/a/51549) ã€‚Word2Vec æ˜¯ä¸€ç§åˆ›å»ºåµŒå…¥çš„æµè¡Œæ–¹æ³•ã€‚Word2Vec èƒŒåçš„åŸºæœ¬ç›´è§‰æ˜¯è¿™æ ·çš„:*æˆ‘ä»¬å¯ä»¥é€šè¿‡è§‚å¯Ÿä¸€ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡/é‚»å±…*æ¥è·å¾—å…³äºå®ƒçš„æœ‰ç”¨ä¿¡æ¯ã€‚åœ¨ Word2Vec ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸¤ç§æ¶æ„æˆ–å­¦ä¹ ç®—æ³•æ¥è·å¾—å•è¯çš„çŸ¢é‡è¡¨ç¤º(åªæ˜¯å¦ä¸€ä¸ªç”¨äºåµŒå…¥çš„å•è¯):C *è¿ç»­å•è¯åŒ…*(åˆå *CBOW)* å’Œ S *kip-gram* ã€‚
â—¼ï¸ **CBOW:** é¢„æµ‹*ç„¦ç‚¹è¯*ç»™å®šå‘¨è¾¹*ä¸Šä¸‹æ–‡è¯*t17ã€‘â—¼ï¸**è·³è·ƒå¼:**é¢„æµ‹*ä¸Šä¸‹æ–‡è¯*ç»™å®š*ç„¦ç‚¹è¯*(æœ¬æ–‡çš„é‡ç‚¹)

åœ¨ç°é˜¶æ®µï¼Œè¿™å¯èƒ½æ²¡æœ‰å¤ªå¤§æ„ä¹‰ã€‚æˆ‘ä»¬å°†å¾ˆå¿«çœ‹åˆ°ä¸€ä¸ªä¾‹å­ï¼Œè¿™å°†å˜å¾—æ›´åŠ æ¸…æ¥šã€‚

å½“ä½¿ç”¨ Skip-gram ç®—æ³•è®­ç»ƒåµŒå…¥æ—¶ï¼Œæˆ‘ä»¬åœ¨é«˜å±‚æ¬¡ä¸Šç»å†ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤:
â—¼ï¸ **è·å–æ–‡æœ¬:**æˆ‘ä»¬ä»æœªæ ‡è®°çš„æ–‡æœ¬è¯­æ–™åº“å¼€å§‹â€”â€”æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„å­¦ä¹ é—®é¢˜ã€‚
â—¼ï¸ **å˜æ¢æ•°æ®:**ç„¶åï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶å°†é¢„å¤„ç†åçš„æ•°æ®é‡æ–°æ’åˆ—æˆä½œä¸ºç‰¹å¾çš„ç„¦ç‚¹è¯å’Œä½œä¸ºè™šæ‹Ÿç›‘ç£å­¦ä¹ é—®é¢˜çš„ç›®æ ‡çš„ä¸Šä¸‹æ–‡è¯ã€‚æ‰€ä»¥ï¼Œå®ƒå˜æˆäº†ä¸€ä¸ªå¤šåˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­ P(ä¸Šä¸‹æ–‡è¯|ç„¦ç‚¹è¯)ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªåœ¨å•ä¸ªæ–‡æ¡£ä¸­å¯èƒ½å‡ºç°çš„æƒ…å†µçš„ç¤ºä¾‹:

![](img/6b69d43bf9a7ef6c100ac115fa2516c9.png)

ä½œè€…å›¾ç‰‡|æˆ‘ä»¬é¦–å…ˆå°†æ–‡æœ¬é¢„å¤„ç†æˆæ ‡è®°ã€‚ç„¶åï¼Œå¯¹äºä½œä¸ºç„¦ç‚¹å•è¯çš„æ¯ä¸ªæ ‡è®°ï¼Œæˆ‘ä»¬æ‰¾åˆ°çª—å£å¤§å°ä¸º 2 çš„ä¸Šä¸‹æ–‡å•è¯ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†ç„¦ç‚¹å•è¯å‰åçš„ä¸¤ä¸ªæ ‡è®°è§†ä¸ºä¸Šä¸‹æ–‡å•è¯ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™æ ·ä¸€ä¸ªå°çš„ç¤ºä¾‹æ–‡æœ¬ä¸­ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ ‡è®°å‰åéƒ½æœ‰ 2 ä¸ªæ ‡è®°ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯ç”¨çš„ä»¤ç‰Œã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸ä¸¥æ ¼åœ°ã€å¯äº’æ¢åœ°ä½¿ç”¨æœ¯è¯­å•è¯å’Œä»¤ç‰Œã€‚

åŒä¸€ä¸ªç‰¹æ€§æœ‰å¤šä¸ªç›®æ ‡å¯èƒ½ä¼šè®©äººéš¾ä»¥æƒ³è±¡ã€‚ä»¥ä¸‹æ˜¯è€ƒè™‘å¦‚ä½•å‡†å¤‡æ•°æ®çš„å¦ä¸€ç§æ–¹å¼:

![](img/9927c885e5bf7a224ca3140b8f400fb0.png)

ä½œè€…å›¾ç‰‡

æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å¯¹ã€‚
â—¼ï¸ **å»ºç«‹ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ:**ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°æ„å»ºçš„æ•°æ®é›†ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜è®­ç»ƒä¸€ä¸ªå…·æœ‰å•ä¸ªéšè—å±‚çš„ç®€å•ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬è®­ç»ƒç¥ç»ç½‘ç»œçš„ä¸»è¦åŸå› æ˜¯ä»éšè—å±‚è·å¾—è®­ç»ƒçš„æƒé‡ï¼Œè¯¥éšè—å±‚æˆä¸ºå•è¯åµŒå…¥ã€‚å‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„å•è¯çš„åµŒå…¥å€¾å‘äºå½¼æ­¤ç›¸ä¼¼ã€‚

æ¦‚è¿°å®Œä¹‹åï¼Œæ˜¯æ—¶å€™ç”¨ Python å®ç°å®ƒæ¥å·©å›ºæˆ‘ä»¬æ‰€å­¦çš„å†…å®¹äº†ã€‚

# ğŸ”¨Python ä¸­çš„ Word2Vec

ç”±äºè¿™ç¯‡æ–‡ç« çš„é‡ç‚¹æ˜¯å¼€å‘ç®—æ³•å¦‚ä½•å·¥ä½œçš„æ›´å¥½çš„ç›´è§‰ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºè‡ªå·±æ„å»ºå®ƒï¼Œè€Œä¸æ˜¯ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„ Word2Vec åµŒå…¥æ¥åŠ æ·±æˆ‘ä»¬çš„ç†è§£ã€‚

> *ğŸ”—*å…è´£å£°æ˜:åœ¨å¼€å‘è¿™ç¯‡æ–‡ç« çš„ä»£ç æ—¶ï¼Œæˆ‘å¤§é‡ä½¿ç”¨äº†ä»¥ä¸‹èµ„æºåº“:
> â—¼ï¸ [å•è¯åµŒå…¥åˆ›å»º](https://github.com/Eligijus112/word-embedding-creation)ä½œè€…[åŸƒåˆ©å‰å°¤æ–¯ 112](https://github.com/Eligijus112) (ä»–çš„åª’ä½“é¡µé¢:[åŸƒåˆ©å‰å°¤æ–¯å¸ƒçº¦å¡æ–¯](https://eligijus-bujokas.medium.com/))
> â—¼ï¸[word 2 vec _ numpy](https://github.com/DerekChia/word2vec_numpy)ä½œè€… [DerekChia](https://github.com/DerekChia)
> 
> æˆ‘è¦æ„Ÿè°¢è¿™äº›äº†ä¸èµ·çš„ä½œè€…è®©ä»–ä»¬çš„æœ‰ç”¨çš„å·¥ä½œä¸ºä»–äººæ‰€ç”¨ã€‚å¦‚æœæ‚¨æƒ³åŠ æ·±å¯¹ word2vec çš„ç†è§£ï¼Œä»–ä»¬çš„å­˜å‚¨åº“æ˜¯å¾ˆå¥½çš„é¢å¤–å­¦ä¹ èµ„æºã€‚

## ğŸ”¨å¸¦ Gensim çš„ Word2vec

ç»è¿‡ä»–çš„å…è®¸ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª [Eligijus112](https://github.com/Eligijus112) çŸ¥è¯†åº“çš„[è¿™ä¸ªæ ·æœ¬ç©å…·æ•°æ®é›†](https://github.com/Eligijus112/word-embedding-creation/blob/master/input/sample.csv)ã€‚è®©æˆ‘ä»¬å¯¼å…¥åº“å’Œæ•°æ®é›†ã€‚

```
import numpy as np
import pandas as pd
from nltk.tokenize.regexp import RegexpTokenizer
from nltk.corpus import stopwords
from gensim.models import Word2Vec, KeyedVectors
from scipy.spatial.distance import cosineimport tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Denseimport matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='darkgrid', context='talk')text = ["The prince is the future king.",
        "Daughter is the princess.",
        "Son is the prince.",
        "Only a man can be a king.",
        "Only a woman can be a queen.",
        "The princess will be a queen.",
        "Queen and king rule the realm.", 
        "The prince is a strong man.",
        "The princess is a beautiful woman.",
        "The royal family is the king and queen and their children.",
        "Prince is only a boy now.",
        "A boy will be a man."]
```

æˆ‘ä»¬ç°åœ¨å°†éå¸¸è½»æ¾åœ°å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†æ–‡æœ¬å°å†™ï¼Œå°†æ–‡æ¡£æ ‡è®°ä¸ºå­—æ¯æ ‡è®°ï¼Œå¹¶åˆ é™¤åœç”¨è¯ã€‚

![](img/dffe65538a643764c617ca33f0ae80e0.png)

ä½œè€…å›¾ç‰‡

é¢„å¤„ç†çš„çº§åˆ«å¯ä»¥éšå®ç°çš„ä¸åŒè€Œä¸åŒã€‚åœ¨ä¸€äº›å®ç°ä¸­ï¼Œå¯ä»¥é€‰æ‹©åšå¾ˆå°‘çš„é¢„å¤„ç†ï¼Œä¿æŒæ–‡æœ¬å‡ ä¹åŸæ ·ã€‚å¦ä¸€æ–¹é¢ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©åšä¸€ä¸ªæ¯”è¿™ä¸ªä¾‹å­æ›´å½»åº•çš„é¢„å¤„ç†ã€‚

```
def preprocess_text(document):
    tokeniser = RegexpTokenizer(r"[A-Za-z]{2,}")
    tokens = tokeniser.tokenize(document.lower())
    key_tokens = [token for token in tokens 
                  if token not in stopwords.words('english')]
    return key_tokens

corpus = []
for document in text:
    corpus.append(preprocess_text(document))
corpus
```

![](img/322e49b4965518c6472276c591ec2117.png)

ä½œè€…å›¾ç‰‡

ç°åœ¨æ¯ä¸ªæ–‡æ¡£éƒ½ç”±ä»¤ç‰Œç»„æˆã€‚æˆ‘ä»¬å°†åœ¨è‡ªå®šä¹‰è¯­æ–™åº“ä¸Šä½¿ç”¨ Gensim æ„å»º Word2Vec:

```
dimension = 2
window = 2word2vec0 = Word2Vec(corpus, min_count=1, vector_size=dimension, 
                     window=window, sg=1)
word2vec0.wv.get_vector('king')
```

![](img/00df88705505701ca8c3e981f15f65cc.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬ä¸ºä¸Šä¸‹æ–‡é€‰æ‹©å¤§å°ä¸º 2 çš„`window`ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†åœ¨ç„¦ç‚¹æ ‡è®°ä¹‹å‰å’Œä¹‹åæŸ¥çœ‹ 2 ä¸ªæ ‡è®°ã€‚`dimension`ä¹Ÿè¢«è®¾ç½®ä¸º 2ã€‚è¿™æ˜¯æŒ‡å‘é‡çš„å¤§å°ã€‚æˆ‘ä»¬é€‰æ‹© 2 æ˜¯å› ä¸ºæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨äºŒç»´å›¾è¡¨ä¸­å°†å…¶å¯è§†åŒ–ï¼Œå¹¶ä¸”æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ä¸€ä¸ªéå¸¸å°çš„æ–‡æœ¬è¯­æ–™åº“ã€‚è¿™ä¸¤ä¸ªè¶…å‚æ•°å¯ä»¥ç”¨ä¸åŒçš„å€¼æ¥è°ƒæ•´ï¼Œä»¥æé«˜ç”¨ä¾‹ä¸­å•è¯åµŒå…¥çš„æœ‰ç”¨æ€§ã€‚åœ¨å‡†å¤‡ Word2Vec æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡å®š`sg=1`æ¥ç¡®ä¿ä½¿ç”¨ Skip-gram ç®—æ³•ã€‚ä¸€æ—¦åµŒå…¥å‡†å¤‡å°±ç»ªï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°ä»¤ç‰Œçš„åµŒå…¥`'king'`ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹åµŒå…¥æœ‰å¤šç›´è§‚ã€‚æˆ‘ä»¬ä¼šæŒ‘é€‰ä¸€ä¸ªæ ·æœ¬è¯:`'king'`ï¼Œçœ‹çœ‹å‘é‡ç©ºé—´ä¸­ä¸å®ƒæœ€ç›¸ä¼¼çš„è¯æ˜¯å¦æœ‰æ„ä¹‰ã€‚è®©æˆ‘ä»¬æ‰¾å‡ºä¸`'king'`æœ€ç›¸ä¼¼çš„ 3 ä¸ªè¯:

```
n=3
word2vec0.wv.most_similar(positive=['king'], topn=n)
```

![](img/ad0f4b5d1b2217cbed27534d5074ce96.png)

ä½œè€…å›¾ç‰‡

è¿™ä¸ªå…ƒç»„åˆ—è¡¨æ˜¾ç¤ºäº†ä¸`'king'`æœ€ç›¸ä¼¼çš„å•è¯åŠå…¶ä½™å¼¦ç›¸ä¼¼åº¦ã€‚é‰´äºæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®éå¸¸å°‘ï¼Œè¿™ä¸ªç»“æœè¿˜ä¸é”™ã€‚

è®©æˆ‘ä»¬ä¸ºè¯æ±‡è¡¨å‡†å¤‡ä¸€ä¸ªåµŒå…¥çš„æ•°æ®æ¡†æ¶ï¼Œè¿™æ˜¯å”¯ä¸€æ ‡è®°çš„é›†åˆ:

```
embedding0 = pd.DataFrame(columns=['d0', 'd1'])
for token in word2vec0.wv.index_to_key:
    embedding0.loc[token] = word2vec0.wv.get_vector(token)
embedding0
```

![](img/6f9a4607eb0369274e73311e8128bfd7.png)

ä½œè€…å›¾ç‰‡

ç°åœ¨ï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–äºŒç»´å‘é‡ç©ºé—´ä¸­çš„è®°å·:

```
sns.lmplot(data=embedding0, x='d0', y='d1', fit_reg=False, aspect=2)
for token, vector in embedding0.iterrows():
    plt.gca().text(vector['d0']+.02, vector['d1']+.03, str(token), 
                   size=14)
plt.tight_layout()
```

![](img/8eef33fffa4307ec433029bf6aee089e.png)

ä½œè€…å›¾ç‰‡

> ğŸ”—å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äº Gensim ä¸­ Word2Vec çš„çŸ¥è¯†ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç”± Gensim çš„åˆ›å»ºè€… Radim Rehurek ç¼–å†™çš„æ•™ç¨‹ã€‚

å¥½å§ï¼Œè¿™æ˜¯ä¸ªä¸é”™çš„çƒ­èº«ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåµŒå…¥æˆ‘ä»¬è‡ªå·±çš„ Word2Vecã€‚

## ğŸ”¨æ‰‹åŠ¨ Word2Vec â€”æ–¹æ³• 1

æˆ‘ä»¬å°†ä»è¯­æ–™åº“ä¸­æŸ¥æ‰¾è¯æ±‡å¼€å§‹ã€‚æˆ‘ä»¬å°†ä¸ºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªæ ‡è®°èµ‹å€¼:

```
vocabulary = sorted([*set([token for document in corpus for token in 
                           document])])
n_vocabulary = len(vocabulary)
token_index ={token: i for i, token in enumerate(vocabulary)}
token_index
```

![](img/41fdcf5c1bcc9c38b5fda0f7d1dba37b.png)

ä½œè€…å›¾ç‰‡

ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ¶ä½œæ ‡è®°å¯¹ä½œä¸ºç¥ç»ç½‘ç»œçš„å‡†å¤‡ã€‚

![](img/705e09a54a35b1862a44ae27be02776d.png)

ä½œè€…å›¾ç‰‡

```
token_pairs = []for document in corpus:
    for i, token in enumerate(document):
        for j in range(i-window, i+window+1):
            if (j>=0) and (j!=i) and (j<len(document)):
                token_pairs.append([token] + [document[j]])n_token_pairs = len(token_pairs)
print(f"{n_token_pairs} token pairs")token_pairs[:5]
```

![](img/df01e093eb09191f366c4db474f31366.png)

ä½œè€…å›¾ç‰‡

ä»¤ç‰Œå¯¹å·²ç»å‡†å¤‡å¥½äº†ï¼Œä½†æ˜¯å®ƒä»¬ä»ç„¶æ˜¯æ–‡æœ¬å½¢å¼ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œä¸€æ¬¡çƒ­ç¼–ç ï¼Œä»¥ä¾¿å®ƒä»¬é€‚ç”¨äºç¥ç»ç½‘ç»œã€‚

![](img/1e69bcc87ca81921ba3c8ab688815d7c.png)

ä½œè€…å›¾ç‰‡

```
X = np.zeros((n_token_pairs, n_vocabulary))
Y = np.zeros((n_token_pairs, n_vocabulary))for i, (focus_token, context_token) in enumerate(token_pairs):    
    X[i, token_index[focus_token]] = 1
    Y[i, token_index[context_token]] = 1
print(X[:5])
```

![](img/2fa359cfe8218707e230e47f2ece302f.png)

ä½œè€…å›¾ç‰‡

ç°åœ¨è¾“å…¥æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªåªæœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ:

```
tf.random.set_seed(42)
word2vec1 = Sequential([
    Dense(units=dimension, input_shape=(n_vocabulary,), 
          use_bias=False, name='embedding'),
    Dense(units=n_vocabulary, activation='softmax', name='output')
])
word2vec1.compile(loss='categorical_crossentropy', optimizer='adam')
word2vec1.fit(x=X, y=Y, epochs=100)
```

![](img/2f62cf3624244846d8c2eea513f4b72f.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬æŒ‡å®šéšè—å±‚æ²¡æœ‰åè§æ¡æ¬¾ã€‚å› ä¸ºæˆ‘ä»¬å¸Œæœ›éšè—å±‚æœ‰çº¿æ€§æ¿€æ´»ï¼Œæˆ‘ä»¬ä¸éœ€è¦æŒ‡å®šã€‚å›¾å±‚ä¸­çš„å•å…ƒæ•°åæ˜ äº†çŸ¢é‡çš„å¤§å°:`dimension`ã€‚

è®©æˆ‘ä»¬ä»éšè—å±‚ä¸­æå–æƒé‡ï¼Œæˆ‘ä»¬çš„åµŒå…¥ã€‚

```
embedding1 = pd.DataFrame(columns=['d0', 'd1'])
for token in token_index.keys():
    ind = token_index[token]
    embedding1.loc[token] = word2vec1.get_weights()[0][ind]
embedding1
```

![](img/707dc7cf605c8f1f4e55d60aa0f29657.png)

ä½œè€…å›¾ç‰‡

ä½¿ç”¨æˆ‘ä»¬çš„æ–°åµŒå…¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸`'king'`æœ€ç›¸ä¼¼çš„ 3 ä¸ªå•è¯:

```
vector1 = embedding1.loc['king']
similarities = {}for token, vector in embedding1.iterrows():
    theta_sum = np.dot(vector1, vector)
    theta_den = np.linalg.norm(vector1) * np.linalg.norm(vector)
    similarities[token] = theta_sum / theta_densimilar_tokens = sorted(similarities.items(), key=lambda x: x[1], 
                        reverse=True)
similar_tokens[1:n+1]
```

![](img/7484b9db5bdec0effe02c8d710d92928.png)

ä½œè€…å›¾ç‰‡

å¤ªå¥½äº†ï¼Œè¿™è¯´å¾—é€šã€‚æˆ‘ä»¬å¯ä»¥ä¿å­˜åµŒå…¥å¹¶ä½¿ç”¨ Gensim åŠ è½½å®ƒä»¬ã€‚ä¸€æ—¦åŠ è½½åˆ° Gensimï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„ç›¸ä¼¼æ€§è®¡ç®—ã€‚

```
with open('embedding1.txt' ,'w') as text_file:
    text_file.write(f'{n_vocabulary} {dimension}\n')
    for token, vector in embedding1.iterrows():
        text_file.write(f"{token} {' '.join(map(str, vector))}\n")
text_file.close()embedding1_loaded = KeyedVectors.load_word2vec_format('embedding1.txt', binary=False)
embedding1_loaded.most_similar(positive=['king'], topn=n)
```

![](img/93c40a1ee6d62031bf9d43aa27d0b1f6.png)

ä½œè€…å›¾ç‰‡

Gensim è®¡ç®—çš„ç›¸ä¼¼æ€§ä¸æˆ‘ä»¬çš„æ‰‹åŠ¨è®¡ç®—ç›¸åŒ¹é…ã€‚

æˆ‘ä»¬ç°åœ¨å°†å¯è§†åŒ–å‘é‡ç©ºé—´ä¸­çš„åµŒå…¥:

```
sns.lmplot(data=embedding1, x='d0', y='d1', fit_reg=False, aspect=2)
for token, vector in embedding1.iterrows():
    plt.gca().text(vector['d0']+.02, vector['d1']+.03, str(token), 
                   size=14)
plt.tight_layout()
```

![](img/babc17d1f2bbabd1c4cd6c8df9abb4c8.png)

ä½œè€…å›¾ç‰‡

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨é¢å‘å¯¹è±¡çš„ç¼–ç¨‹æ–¹æ³•æ‰‹åŠ¨åˆ›å»ºå•è¯åµŒå…¥ã€‚

## ğŸ”¨æ‰‹åŠ¨ Word2Vec â€”æ–¹æ³• 2

æˆ‘ä»¬å°†ä»åˆ›å»ºä¸€ä¸ªåä¸º`Data`çš„ç±»å¼€å§‹ï¼Œå®ƒé›†ä¸­äº†ä¸æ•°æ®ç›¸å…³çš„ä»»åŠ¡:

![](img/892e1e556bb655872721b6e7a6b846a7.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°,`corpus`å±æ€§çœ‹èµ·æ¥å’Œå‰é¢å‡ èŠ‚ä¸­çš„ä¸€æ ·ã€‚

```
len([token for document in data.corpus for token in document])
```

![](img/6f2cc037010847ce9a239b080ccd7916.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬çš„ç©å…·è¯­æ–™åº“ä¸­æœ‰ 32 ä¸ªä»£å¸ã€‚

```
len(data.focus_context_data)
```

![](img/6f2cc037010847ce9a239b080ccd7916.png)

ä½œè€…å›¾ç‰‡

ä¸ä¹‹å‰ä¸åŒçš„æ˜¯ï¼Œ`data.focus_context_data`æ²¡æœ‰è¢«æ ¼å¼åŒ–ä¸ºæ ‡è®°å¯¹ã€‚ç›¸åï¼Œè¿™ 32 ä¸ªæ ‡è®°ä¸­çš„æ¯ä¸€ä¸ªéƒ½ä¸å®ƒä»¬æ‰€æœ‰çš„ä¸Šä¸‹æ–‡æ ‡è®°æ˜ å°„åœ¨ä¸€èµ·ã€‚

![](img/e7055efd4635080b0d7d06f71b8778de.png)

ä½œè€…å›¾ç‰‡

```
np.sum([len(context_tokens) for _, context_tokens in 
        data.focus_context_data])
```

![](img/37d89efdf918feac8aa78154792d57d2.png)

ä½œè€…å›¾ç‰‡

åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬æ€»å…±è¿˜æœ‰ 56 ä¸ªä¸Šä¸‹æ–‡æ ‡è®°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†å…³äº Word2Vec çš„ä»£ç é›†ä¸­åœ¨ä¸€ä¸ªå¯¹è±¡ä¸­:

![](img/c57b8f0004dd6d1f2edb607a31bb7754.png)

ä½œè€…å›¾ç‰‡|ä»…éƒ¨åˆ†è¾“å‡º

æˆ‘ä»¬åˆšåˆšè®­ç»ƒäº†æˆ‘ä»¬çš„è‡ªå®šä¹‰ Word2Vec å¯¹è±¡ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸ªæ ·æœ¬å‘é‡:

```
word2vec2.extract_vector('king')
```

![](img/9fba72032faf6db82fb6c2a6f4225018.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬ç°åœ¨æ¥çœ‹çœ‹ä¸`'king'`æœ€ç›¸ä¼¼çš„ä¸‰ä¸ªè¯:

```
word2vec2.find_similar_words("king")
```

![](img/bc430eecd89770f92b63160379f7af81.png)

ä½œè€…å›¾ç‰‡

è¿™å¾ˆå¥½ã€‚æ˜¯æ—¶å€™å°†åµŒå…¥è½¬æ¢æˆæ•°æ®å¸§äº†:

```
embedding2 = pd.DataFrame(word2vec2.w1, columns=['d0', 'd1'])
embedding2.index = embedding2.index.map(word2vec2.data.index_token)
embedding2
```

![](img/f6e21e921805c601f10c620874d846fe.png)

ä½œè€…å›¾ç‰‡

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¾ˆå®¹æ˜“åœ°çœ‹åˆ°æ–°çš„åµŒå…¥:

```
sns.lmplot(data=embedding2, x='d0', y='d1', fit_reg=False, aspect=2)
for token, vector in embedding2.iterrows():
    plt.gca().text(vector['d0']+.02, vector['d1']+.03, str(token), 
                   size=14)
plt.tight_layout()
```

![](img/c0424482830a752d486e287da9952831.png)

ä½œè€…å›¾ç‰‡

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡ä¿å­˜åµŒå…¥å¹¶ä½¿ç”¨ Gensim åŠ è½½å®ƒï¼Œç„¶åè¿›è¡Œæ£€æŸ¥:

```
with open('embedding2.txt' ,'w') as text_file:
    text_file.write(f'{n_vocabulary} {dimension}\n')
    for token, vector in embedding2.iterrows():
        text_file.write(f"{token} {' '.join(map(str, vector))}\n")
text_file.close()embedding2_loaded = KeyedVectors.load_word2vec_format('embedding2.txt', binary=False)
embedding2_loaded.most_similar(positive=['king'], topn=n)
```

![](img/c2b3647a16f7bec3a40dbc655731aa70.png)

ä½œè€…å›¾ç‰‡

åœ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¯»æ‰¾ç›¸ä¼¼è¯æ—¶ï¼Œæˆ‘ä»¬è¿™æ¬¡ç”¨äº†`scipy`ã€‚é™¤äº†æµ®ç‚¹ç²¾åº¦è¯¯å·®ï¼Œè¿™ç§æ–¹æ³•ä¸ Gensim çš„ç»“æœç›¸åŒ¹é…ã€‚

è¿™å°±æ˜¯è¿™ç¯‡æ–‡ç« çš„å…¨éƒ¨å†…å®¹ï¼å¸Œæœ›æ‚¨å·²ç»å¯¹ä»€ä¹ˆæ˜¯å•è¯åµŒå…¥ä»¥åŠ Word2Vec å¦‚ä½•ä½¿ç”¨ Skip-gram ç®—æ³•ç”Ÿæˆå•è¯åµŒå…¥æœ‰äº†åŸºæœ¬çš„äº†è§£ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯ç”¨äº NLP çš„ Word2Vecï¼Œä½†æ˜¯è¿™ç§æŠ€æœ¯å¯¹äºæ¨èç³»ç»Ÿä¹Ÿæ˜¯æœ‰å¸®åŠ©çš„ã€‚[è¿™é‡Œ](http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/)æœ‰ä¸€ç¯‡å…³äºè¿™æ–¹é¢çš„æ·±åˆ»æ–‡ç« ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äº Word2Vec çš„çŸ¥è¯†ï¼Œè¿™é‡Œæœ‰ä¸€äº›æœ‰ç”¨çš„èµ„æº:
â—¼ï¸ [ç¬¬äºŒè®²|å•è¯å‘é‡è¡¨ç¤ºæ³•:word 2 vecâ€”YouTube](https://www.youtube.com/watch?v=ERibwqs9p38)
â—¼ï¸[Google code archiveâ€”Google code project hosting çš„é•¿æœŸå­˜å‚¨](https://code.google.com/archive/p/word2vec/)
â—¼ï¸ [word2vec å‚æ•°å­¦ä¹ è®²è§£](https://arxiv.org/pdf/1411.2738.pdf)

![](img/cb9c3f3f0a980ea5c0a492ae544dbbae.png)

ç”± [Milad Fakurian](https://unsplash.com/@fakurian?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

*æ‚¨æƒ³è¦è®¿é—®æ›´å¤šè¿™æ ·çš„å†…å®¹å—ï¼Ÿåª’ä½“ä¼šå‘˜å¯ä»¥æ— é™åˆ¶åœ°è®¿é—®åª’ä½“ä¸Šçš„ä»»ä½•æ–‡ç« ã€‚å¦‚æœæ‚¨ä½¿ç”¨* [*æˆ‘çš„æ¨èé“¾æ¥*](https://zluvsand.medium.com/membership)*æˆä¸ºä¼šå‘˜ï¼Œæ‚¨çš„ä¸€éƒ¨åˆ†ä¼šè´¹å°†ç›´æ¥ç”¨äºæ”¯æŒæˆ‘ã€‚*

è°¢è°¢ä½ çœ‹æˆ‘çš„å¸–å­ã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œè¿™é‡Œæœ‰æˆ‘çš„ä¸€äº›å¸–å­çš„é“¾æ¥:

â—¼ï¸ï¸ [ç®¡é“ã€ColumnTransformer å’Œ FeatureUnion è§£é‡Š](/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------)â—¼ï¸ï¸[feature unionã€ColumnTransformer &ç®¡é“ç”¨äºé¢„å¤„ç†æ–‡æœ¬æ•°æ®](/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6)t5ã€‘â—¼ï¸[ç”¨è¿™äº›æç¤ºä¸°å¯Œæ‚¨çš„ Jupyter ç¬”è®°æœ¬](/enrich-your-jupyter-notebook-with-these-tips-55c8ead25255)
â—¼ï¸ [ç”¨è¿™äº›æç¤ºç»„ç»‡æ‚¨çš„ Jupyter ç¬”è®°æœ¬](/organise-your-jupyter-notebook-with-these-tips-d164d5dcd51f)
â—¼ï¸ [è§£é‡Š Scikit-ç”¨ SHAP å­¦ä¹ æ¨¡å‹](/explaining-scikit-learn-models-with-shap-61daff21b12a)
â—¼ï¸ï¸ [åœ¨ scikit ä¸­é€‰æ‹©ç‰¹æ€§](/feature-selection-in-scikit-learn-dc005dcf38b7)
â—¼ï¸ï¸ [æ¯”è¾ƒ](/comparing-random-forest-and-gradient-boosting-d7236b429c15)

å†è§ğŸƒ ğŸ’¨
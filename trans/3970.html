<html>
<head>
<title>Primer on Cleaning Text Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">æ¸…ç†æ–‡æœ¬æ•°æ®å…¥é—¨</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/primer-to-cleaning-text-data-7e856d6e5791#2022-09-02">https://towardsdatascience.com/primer-to-cleaning-text-data-7e856d6e5791#2022-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">æ¸…æ´—æ–‡æœ¬æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢„å¤„ç†çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/79002ef8101df35bc463f491fe2531c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AZqJ7x_BbT2md62d"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">æ¥è‡ª<a class="ae ky" href="https://www.pexels.com/ko-kr/photo/macbook-pro-1181373/" rel="noopener ugc nofollow" target="_blank">åƒç´ </a>çš„å…è´¹ä½¿ç”¨ç…§ç‰‡</p></figure><h1 id="4e62" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">ä»‹ç»</h1><p id="c6d4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸï¼Œé¢„å¤„ç†æ˜¯ä¸€ä¸ªé‡è¦çš„é˜¶æ®µï¼Œåœ¨è¿™é‡Œè¿›è¡Œæ–‡æœ¬æ¸…ç†ã€è¯å¹²æå–ã€è¯æ±‡åŒ–å’Œè¯æ€§æ ‡æ³¨ç­‰å·¥ä½œã€‚åœ¨NLPé¢„å¤„ç†çš„è¿™äº›ä¸åŒæ–¹é¢ä¸­ï¼Œæˆ‘å°†æ¶µç›–æˆ‘ä»¬å¯ä»¥åº”ç”¨çš„æ–‡æœ¬æ¸…ç†æ–¹æ³•çš„ç»¼åˆåˆ—è¡¨ã€‚è¿™é‡Œçš„æ–‡æœ¬æ¸…ç†æŒ‡çš„æ˜¯ç§»é™¤æˆ–è½¬æ¢æ–‡æœ¬çš„æŸäº›éƒ¨åˆ†ï¼Œä»¥ä¾¿æ–‡æœ¬å˜å¾—æ›´å®¹æ˜“è¢«æ­£åœ¨å­¦ä¹ æ–‡æœ¬çš„NLPæ¨¡å‹ç†è§£çš„è¿‡ç¨‹ã€‚è¿™é€šå¸¸é€šè¿‡å‡å°‘æ–‡æœ¬æ•°æ®ä¸­çš„å™ªå£°ä½¿NLPæ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚</p><h1 id="3796" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">å°†æ‰€æœ‰å­—ç¬¦è½¬æ¢æˆå°å†™</h1><p id="419c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">stringåŒ…(Pythonä¸­çš„é»˜è®¤åŒ…)åŒ…å«å„ç§æœ‰ç”¨çš„å­—ç¬¦ä¸²å‡½æ•°ã€‚lowerå‡½æ•°å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼ŒæŠŠæ‰€æœ‰å­—ç¬¦éƒ½å˜æˆå°å†™ã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="6109" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">def </strong>make_lowercase(token_list):<br/>    # Assuming word <a class="ae ky" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank">tokenization</a> already happened</span><span id="8f10" class="ms la it mo b gy mx mu l mv mw">    # Using list comprehension --&gt; loop through every word/token, make it into lower case and add it to a new list<br/>    words = [word.lower() for word in token_list]    </span><span id="eb5e" class="ms la it mo b gy mx mu l mv mw">    # join lowercase tokens into one string<br/>    cleaned_string = " ".join(words) <br/>    return cleaned_string</span></pre><h1 id="5ddd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">åˆ é™¤æ ‡ç‚¹ç¬¦å·</h1><p id="9243" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Pythonä¸­çš„string.punctuation(å°±æ˜¯å‰é¢æåˆ°çš„åŒ…)åŒ…å«ä»¥ä¸‹å‡ é¡¹æ ‡ç‚¹ç¬¦å·ã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ea72" class="ms la it mo b gy mt mu l mv mw">#$%&amp;\â€™()*+,-./:;?@[\\]^_{|}~`</span><span id="1de6" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">import </strong>string</span><span id="5b79" class="ms la it mo b gy mx mu l mv mw">text = "It was a great night! Shout out to @Amy Lee for organizing wonderful event (a.k.a. on fire)."</span><span id="3ff3" class="ms la it mo b gy mx mu l mv mw">PUNCT_TO_REMOVE = string.punctuation</span><span id="a440" class="ms la it mo b gy mx mu l mv mw">ans = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))</span><span id="76a1" class="ms la it mo b gy mx mu l mv mw">ans<br/>&gt;&gt; "It was a great night Shout out to Amy Lee for organizing wonderful event aka on fire"</span></pre><p id="72c1" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">å­—ç¬¦ä¸²åŒ…ä¸­çš„å¦ä¸€ä¸ªæ–¹æ³•translateå‡½æ•°ä½¿ç”¨è¾“å…¥å­—å…¸æ¥æ‰§è¡Œæ˜ å°„ã€‚maketranså‡½æ•°æ˜¯translateå‡½æ•°çš„å…„å¼Ÿæ–¹æ³•ï¼Œå®ƒåˆ›å»ºç”¨ä½œtranslateæ–¹æ³•è¾“å…¥çš„å­—å…¸ã€‚è¯·æ³¨æ„ï¼Œmaketranså‡½æ•°æ¥å—3ä¸ªå‚æ•°ï¼Œå¦‚æœæ€»å…±ä¼ é€’äº†3ä¸ªå‚æ•°ï¼Œåˆ™ç¬¬ä¸‰ä¸ªå‚æ•°ä¸­çš„æ¯ä¸ªå­—ç¬¦éƒ½è¢«æ˜ å°„ä¸ºNoneã€‚è¿™ä¸ªç‰¹æ€§å¯ä»¥ç”¨æ¥åˆ é™¤å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦ã€‚</p><p id="1e11" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">æ ¹æ®ä¸Šé¢çš„ä»£ç ç‰‡æ®µï¼Œæˆ‘ä»¬å°†maketranså‡½æ•°çš„ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šä¸ºç©ºå­—ç¬¦ä¸²(å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è¿™äº›å‚æ•°)ï¼Œå¹¶å°†ç¬¬ä¸‰ä¸ªå‚æ•°æŒ‡å®šä¸ºä¸Šé¢string.punctuationä¸­å®šä¹‰çš„æ ‡ç‚¹é¡¹ã€‚ç„¶åï¼Œå­˜å‚¨åœ¨å˜é‡<em class="nd">æ–‡æœ¬</em>ä¸­çš„å­—ç¬¦ä¸²ä¸­çš„æ ‡ç‚¹ç¬¦å·å°†è¢«åˆ é™¤ã€‚</p><h1 id="6f16" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">åˆ é™¤å·ç </h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="7cdb" class="ms la it mo b gy mt mu l mv mw">text = "My cell phone number is 123456. Please take note."</span><span id="a780" class="ms la it mo b gy mx mu l mv mw">text_cleaned = ''.join([i for i in text if not i.isdigit()])</span><span id="f3ec" class="ms la it mo b gy mx mu l mv mw">text_cleaned<br/>&gt;&gt; "My cell phone number is. Please take note."</span></pre><p id="f1f2" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åšåŒæ ·çš„äº‹æƒ…ï¼Œå®ƒæ˜¯å­—ç¬¦ä¸²æ“ä½œæœ€å¥½çš„æœ‹å‹ä¹‹ä¸€ã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ab46" class="ms la it mo b gy mt mu l mv mw">text_cleaned = [re.sub(râ€™\w*\d\w*â€™, â€˜â€™, w) for w in text]</span><span id="5de7" class="ms la it mo b gy mx mu l mv mw">text_cleaned<br/>&gt;&gt; "My cell phone number is. Please take note."</span></pre><h1 id="20aa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">ç§»é™¤è¡¨æƒ…ç¬¦å·</strong></h1><p id="9f5f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">éšç€å„ç§ç¤¾äº¤åª’ä½“å¹³å°ç”Ÿæˆçš„éç»“æ„åŒ–æ–‡æœ¬æ•°æ®çš„æ•°é‡ä¸æ–­å¢åŠ ï¼Œæ›´å¤šçš„æ–‡æœ¬æ•°æ®åŒ…å«éå…¸å‹å­—ç¬¦ï¼Œå¦‚è¡¨æƒ…ç¬¦å·ã€‚è¡¨æƒ…ç¬¦å·å¯èƒ½å¾ˆéš¾è¢«æœºå™¨ç†è§£ï¼Œå¹¶ä¸”å¯èƒ½ä¼šç»™ä½ çš„NLPæ¨¡å‹æ·»åŠ ä¸å¿…è¦çš„å™ªå£°ã€‚ä»æ–‡æœ¬æ•°æ®ä¸­åˆ é™¤è¡¨æƒ…ç¬¦å·å°±æ˜¯è¿™ç§æƒ…å†µã€‚ç„¶è€Œï¼Œå¦‚æœä½ è¯•å›¾è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå°è¯•å°†è¡¨æƒ…ç¬¦å·è½¬æ¢æˆæŸç§æ–‡æœ¬æ ¼å¼è€Œä¸æ˜¯å½»åº•åˆ é™¤å®ƒä»¬å¯èƒ½æ˜¯æœ‰ç›Šçš„ï¼Œå› ä¸ºè¡¨æƒ…ç¬¦å·å¯ä»¥åŒ…å«ä¸æ‰‹å¤´æ–‡æœ¬ç›¸å…³çš„æƒ…æ„Ÿçš„æœ‰ç”¨ä¿¡æ¯ã€‚ä¸€ç§æ–¹æ³•æ˜¯åˆ›å»ºæ‚¨è‡ªå·±çš„è‡ªå®šä¹‰è¯å…¸ï¼Œå°†ä¸åŒçš„è¡¨æƒ…ç¬¦å·æ˜ å°„åˆ°ä¸€äº›è¡¨ç¤ºä¸è¡¨æƒ…ç¬¦å·ç›¸åŒæƒ…æ„Ÿçš„æ–‡æœ¬(ä¾‹å¦‚{ğŸ”¥:ç«})ã€‚</p><p id="985a" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">çœ‹çœ‹è¿™ç¯‡<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2022/01/text-cleaning-methods-in-nlp/" rel="noopener ugc nofollow" target="_blank">å¸–å­</a>ï¼Œå®ƒå±•ç¤ºäº†å¦‚ä½•ä»ä½ çš„æ–‡æœ¬ä¸­åˆ é™¤è¡¨æƒ…ç¬¦å·ã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="1a66" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import </strong>re</span><span id="7e56" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def </strong>remove_emoji(string):</span><span id="c6a8" class="ms la it mo b gy mx mu l mv mw">     emoji_pattern = re.compile(â€œ[â€œ</span><span id="e534" class="ms la it mo b gy mx mu l mv mw">     uâ€U0001F600-U0001F64Fâ€ # emoticons</span><span id="1bf8" class="ms la it mo b gy mx mu l mv mw">     uâ€U0001F300-U0001F5FFâ€ # symbols &amp; pictographs</span><span id="3c9c" class="ms la it mo b gy mx mu l mv mw">     uâ€U0001F680-U0001F6FFâ€ # transport &amp; map symbols</span><span id="1d9a" class="ms la it mo b gy mx mu l mv mw">     uâ€U0001F1E0-U0001F1FFâ€ # flags (iOS)</span><span id="6d22" class="ms la it mo b gy mx mu l mv mw">     uâ€U00002702-U000027B0"</span><span id="c2b4" class="ms la it mo b gy mx mu l mv mw">     uâ€U000024C2-U0001F251"</span><span id="7b72" class="ms la it mo b gy mx mu l mv mw">     â€œ]+â€, flags=re.UNICODE)</span><span id="cf17" class="ms la it mo b gy mx mu l mv mw">     return emoji_pattern.sub(râ€™â€™, string)</span><span id="4e6a" class="ms la it mo b gy mx mu l mv mw">remove_emoji(â€œgame is on ğŸ”¥ğŸ”¥â€)</span><span id="11b2" class="ms la it mo b gy mx mu l mv mw">&gt;&gt; 'game is on '</span></pre><h1 id="8ad4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">æ‹¼å‡ºå®«ç¼©</strong></h1><p id="b3e7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">pythonä¸­çš„æ”¶ç¼©åŒ…(éœ€è¦ä½¿ç”¨ï¼pipå®‰è£…æ”¶ç¼©)å…è®¸æˆ‘ä»¬æ‹¼å‡ºæ”¶ç¼©ã€‚é€šè¿‡åœ¨æ‰§è¡Œæ ‡è®°åŒ–æ—¶åˆ›å»ºæ›´å¤šçš„æ ‡è®°ï¼Œæ‹¼å‡ºç¼©å†™å¯ä»¥ä¸ºæ–‡æœ¬æ•°æ®æ·»åŠ æ›´å¤šçš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œå½“æ‰§è¡ŒåŸºäºç©ºç™½çš„å•è¯æ ‡è®°åŒ–æ—¶ï¼Œæ ‡è®°â€œwouldâ€ä¸ä¼šè¢«è§†ä¸ºå•ç‹¬çš„æ ‡è®°ã€‚ç›¸åï¼Œå®ƒæ˜¯è±¡å¾â€œå¥¹æ„¿æ„â€çš„ä¸€éƒ¨åˆ†ã€‚ä½†æ˜¯ï¼Œä¸€æ—¦æˆ‘ä»¬ä¿®å¤äº†ç¼©å†™ï¼Œæˆ‘ä»¬å°±ä¼šçœ‹åˆ°ï¼Œåœ¨æ‰§è¡Œå•è¯æ ‡è®°åŒ–æ—¶ï¼Œå•è¯â€œwouldâ€ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ ‡è®°å­˜åœ¨ã€‚è¿™ä¸ºNLPæ¨¡å‹æ·»åŠ äº†æ›´å¤šçš„ä»¤ç‰Œä»¥ä¾›ä½¿ç”¨ã€‚è¿™å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£æ–‡æœ¬çš„å«ä¹‰ï¼Œä»è€Œæé«˜å„ç§NLPä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="d3c9" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import </strong>contractions</span><span id="453f" class="ms la it mo b gy mx mu l mv mw">text = â€œShe<strong class="mo iu">'d</strong> like to hang out with you sometime!â€</span><span id="169d" class="ms la it mo b gy mx mu l mv mw">contractions.fix(text)</span><span id="a95a" class="ms la it mo b gy mx mu l mv mw">&gt;&gt; â€œShe <strong class="mo iu">would </strong>like to hang out with you sometime!â€</span></pre><p id="a0c6" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">ä½†æ˜¯ç”±äºè¿™ä¸ªåŒ…å¯èƒ½ä¸æ˜¯100%å…¨é¢çš„(å³æ²¡æœ‰è¦†ç›–å­˜åœ¨çš„æ¯ä¸€ä¸ªç¼©å†™)ï¼Œæ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰å­—å…¸ï¼Œå°†åŒ…æ²¡æœ‰è¦†ç›–çš„æŸäº›ç¼©å†™æ˜ å°„åˆ°è¿™äº›ç¼©å†™çš„æ‹¼å†™ç‰ˆæœ¬ã€‚è¿™ç¯‡<a class="ae ky" href="https://studymachinelearning.com/text-data-cleaning-preprocessing/" rel="noopener ugc nofollow" target="_blank">å¸–å­</a>å‘ä½ å±•ç¤ºäº†å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„ä¾‹å­ï¼</p><h1 id="026d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">å‰¥ç¦»HTMLæ ‡ç­¾</h1><p id="9112" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">æˆ‘ä»¬ä½¿ç”¨Pythonçš„BeautifulSoupåŒ…æ¥å‰¥ç¦»HTMLæ ‡ç­¾ã€‚è¿™ä¸ªåŒ…æ˜¯ç”¨äºç½‘é¡µæŠ“å–çš„ï¼Œä½†æ˜¯å®ƒçš„htmlè§£æå™¨å·¥å…·å¯ä»¥ç”¨æ¥å‰¥ç¦»HTMLæ ‡ç­¾ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="6be3" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">def</strong> strip_html_tags(text):<br/>    soup = BeautifulSoup(text, "html.parser")<br/>    stripped_text = soup.get_text()<br/>    <strong class="mo iu">return</strong> stripped_text</span><span id="4113" class="ms la it mo b gy mx mu l mv mw"><br/># Below is another variation for doing the same thing</span><span id="5f8d" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> clean_html(html):     <br/>     # parse html content<br/>     soup = BeautifulSoup(html, "html.parser")</span><span id="6cfc" class="ms la it mo b gy mx mu l mv mw">     for data in soup(['style', 'script', 'code', 'a']):<br/>     # Remove tags<br/>         data.decompose( )</span><span id="91b1" class="ms la it mo b gy mx mu l mv mw">     # return data by retrieving the tag content<br/>     return ' '.join(soup.stripped_strings)</span></pre><h1 id="4b4b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">åˆ é™¤é‡éŸ³å­—ç¬¦</h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="2d1a" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import</strong> unicodedata</span><span id="fa7a" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> remove_accent_chars(text):<br/>    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')<br/>    <strong class="mo iu">return</strong> text</span></pre><h1 id="d624" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">åˆ é™¤URLã€æåŠ(@)ã€hastags (#)å’Œç‰¹æ®Šå­—ç¬¦</strong></h1><p id="3ec2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥åˆ é™¤URLã€æåŠã€æ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦ï¼Œå› ä¸ºå®ƒä»¬ä¿æŒä¸€å®šçš„ç»“æ„å’Œæ¨¡å¼ã€‚ä¸‹é¢åªæ˜¯ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜æˆ‘ä»¬å¦‚ä½•åŒ¹é…å­—ç¬¦ä¸²ä¸­çš„URLã€æåŠå’Œæ ‡ç­¾æ¨¡å¼ï¼Œå¹¶åˆ é™¤å®ƒä»¬ã€‚è¯·è®°ä½ï¼Œåº”è¯¥æœ‰å¤šç§æ–¹æ³•ï¼Œå› ä¸ºæœ‰å¤šç§æ–¹æ³•å¯ä»¥å½¢æˆæ­£åˆ™è¡¨è¾¾å¼æ¥è·å¾—ç›¸åŒçš„è¾“å‡ºã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="2453" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">## Remove URLs</strong></span><span id="46c4" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">import </strong>re</span><span id="b415" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def </strong>remove_url(text):<br/>     return re.sub(râ€™https?:\S*â€™, â€˜â€™, text)</span><span id="c0c7" class="ms la it mo b gy mx mu l mv mw">print(remove_url('The website <a class="ae ky" href="https://www.google.com/" rel="noopener ugc nofollow" target="_blank">https://www.spotify.com/</a> crashed last night due to high traffic.'))<br/>&gt;&gt; 'The website crashed last night due to high traffic.'</span><span id="f0cf" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">## Remove Mentions (@) and hastags (#)</strong></span><span id="8c46" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">import </strong>re</span><span id="5035" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def </strong>remove_mentions_and_tags(text):<br/>     text = re.sub(r'@\S*', '', text)<br/>     <strong class="mo iu">return </strong>re.sub(r'#\S*', '', text)</span><span id="64ac" class="ms la it mo b gy mx mu l mv mw">print(remove_mentions_and_tags('Thank you @Jay for your contribution to this project! #projectover'))<br/>&gt;&gt; 'Thank you Jay for your contribution to this project! projectover'</span><span id="11d9" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">## Remove Special Characters</strong></span><span id="151b" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> remove_spec_chars(text):<br/>     text = re.sub('[^a-zA-z0-9\s]', '' , text)<br/>     <strong class="mo iu">return </strong>text</span><span id="b159" class="ms la it mo b gy mx mu l mv mw"><a class="ae ky" href="https://medium.com/mlearning-ai/nlp-a-comprehensive-guide-to-text-cleaning-and-preprocessing-63f364febfc5" rel="noopener">https://medium.com/mlearning-ai/nlp-a-comprehensive-guide-to-text-cleaning-and-preprocessing-63f364febfc5</a></span></pre><h1 id="dffb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">åˆ é™¤åœæ­¢å­—</strong></h1><p id="4921" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html" rel="noopener ugc nofollow" target="_blank">åœç”¨è¯æ˜¯ä¸€äº›éå¸¸å¸¸è§çš„è¯ï¼Œåœ¨å¸®åŠ©é€‰æ‹©æ–‡æ¡£æˆ–ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†</a>å»ºæ¨¡æ—¶å¯èƒ½æ²¡æœ‰ä»€ä¹ˆä»·å€¼ã€‚é€šå¸¸ï¼Œå½“æˆ‘ä»¬å¯¹è‡ªç„¶è¯­è¨€å¤„ç†è¿›è¡Œé¢„å¤„ç†æ—¶ï¼Œè¿™äº›å•è¯å¯èƒ½ä¼šä»æ–‡æœ¬æ•°æ®ä¸­è¢«ä¸¢å¼ƒæˆ–åˆ é™¤ã€‚è¿™æ˜¯å› ä¸ºåœç”¨è¯ç”±äºå‡ºç°é¢‘ç‡è¿‡é«˜ï¼Œå¯èƒ½ä¸ä¼šå¢åŠ æé«˜NLPæ¨¡å‹å‡†ç¡®æ€§çš„ä»·å€¼ã€‚å°±åƒå…¸å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œä½æ–¹å·®çš„ç‰¹å¾ä»·å€¼è¾ƒä½ï¼Œå› ä¸ºå®ƒä»¬æ— åŠ©äºæ¨¡å‹åŸºäºè¿™äº›ç‰¹å¾åŒºåˆ†ä¸åŒçš„æ•°æ®ç‚¹ã€‚è¿™åŒæ ·é€‚ç”¨äºNLPï¼Œå…¶ä¸­åœç”¨è¯å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä½æ–¹å·®ç‰¹å¾ã€‚åŒæ ·ï¼Œåœç”¨è¯ä¼šå¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹Ÿåˆï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¼€å‘çš„æ¨¡å‹å¯¹äºçœ‹ä¸è§çš„æ•°æ®è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”ç¼ºä¹æ¨å¹¿åˆ°æ–°æ•°æ®ç‚¹çš„èƒ½åŠ›ã€‚</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ef3f" class="ms la it mo b gy mt mu l mv mw"># Retrieve stop word list from NLTK<br/>stopword_list = nltk.corpus.stopwords.words(â€˜englishâ€™)</span><span id="b4db" class="ms la it mo b gy mx mu l mv mw">stopword_list.remove(â€˜noâ€™)</span><span id="0d57" class="ms la it mo b gy mx mu l mv mw">stopword_list.remove(â€˜notâ€™)</span><span id="fede" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">from</strong> <strong class="mo iu">nltk.tokenize.toktok</strong> <strong class="mo iu">import</strong> ToktokTokenizer</span><span id="bb61" class="ms la it mo b gy mx mu l mv mw">tokenizer = ToktokTokenizer( )</span><span id="25fa" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">def</strong> remove_stopwords(text, is_lower_case=<strong class="mo iu">False</strong>):</span><span id="1e59" class="ms la it mo b gy mx mu l mv mw">     tokens = tokenizer.tokenize(text)</span><span id="9a15" class="ms la it mo b gy mx mu l mv mw">     tokens = [token.strip( ) <strong class="mo iu">for</strong> token <strong class="mo iu">in</strong> tokens] # List comprehension: loop through every token and strip white space</span><span id="669f" class="ms la it mo b gy mx mu l mv mw">     filtered_tokens = [token <strong class="mo iu">for</strong> token <strong class="mo iu">in</strong> tokens <strong class="mo iu">if</strong> token <strong class="mo iu">not</strong> <strong class="mo iu">in</strong> stopword_list] # Keep only the non stop word tokens in the list</span><span id="028c" class="ms la it mo b gy mx mu l mv mw">     filtered_text = ' '.join(filtered_tokens) # join all those tokens using a space as a delimiter</span><span id="b97f" class="ms la it mo b gy mx mu l mv mw"><strong class="mo iu">    return</strong> filtered_text</span></pre><p id="4bb0" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">è¯·æ³¨æ„ï¼Œæœ‰å¦ä¸€ç§æ–¹æ³•å¯ä»¥ä»ä¸€ä¸ªåä¸ºSpaCyçš„ä¸åŒåŒ…ä¸­æ£€ç´¢åœç”¨è¯ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªå¸¸ç”¨äºNLPä»»åŠ¡çš„æœ‰ç”¨åŒ…ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·åš:</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="ecfa" class="ms la it mo b gy mt mu l mv mw"><strong class="mo iu">import</strong> <strong class="mo iu">spacy</strong></span><span id="42c2" class="ms la it mo b gy mx mu l mv mw">en = spacy.load('en_core_web_sm') # load the english language small model of spacy</span><span id="c40b" class="ms la it mo b gy mx mu l mv mw">stopword_list = en.Defaults.stop_words</span></pre><h1 id="9ff6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">è­¦å‘Šå’Œä¸€äº›ç»“æŸè¯­</h1><p id="452e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">å°±åƒä»»ä½•å…¶ä»–æ•°æ®ç§‘å­¦ä»»åŠ¡ä¸€æ ·ï¼Œä¸åº”è¯¥ç›²ç›®åœ°è¿›è¡ŒNLPçš„é¢„å¤„ç†ã€‚è€ƒè™‘ä½ çš„ç›®æ ‡æ˜¯ä»€ä¹ˆã€‚ä¾‹å¦‚ï¼Œä»ä½ æœé›†çš„ç¤¾äº¤åª’ä½“æ–‡æœ¬æ•°æ®ä¸­ç§»é™¤æ ‡ç­¾å’ŒæåŠç¬¦å·ï¼Œä½ æƒ³ä»ä¸­å¾—åˆ°ä»€ä¹ˆï¼Ÿæ˜¯å› ä¸ºè¿™äº›ç¬¦å·æ²¡æœ‰ç»™ä½ æ­£åœ¨æ„å»ºçš„é¢„æµ‹æŸä¸ªè¯­æ–™åº“çš„æƒ…æ„Ÿçš„NLPæ¨¡å‹å¢åŠ å¤šå°‘ä»·å€¼å—ï¼Ÿé™¤éæ‚¨æå‡ºè¿™äº›é—®é¢˜å¹¶ä¸”èƒ½å¤Ÿæ¸…æ¥šåœ°å›ç­”ï¼Œå¦åˆ™æ‚¨ä¸åº”è¯¥ä¸´æ—¶æ¸…ç†æ–‡æœ¬ã€‚è¯·è®°ä½ï¼Œè¯¢é—®â€œä¸ºä»€ä¹ˆâ€åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸéå¸¸é‡è¦ã€‚</p><p id="abc5" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">åœ¨æœ¬æ–‡ä¸­ï¼Œåœ¨è¿›å…¥NLPå¾ªç¯çš„ä¸‹ä¸€é˜¶æ®µä¹‹å‰ï¼Œæˆ‘ä»¬æŸ¥çœ‹äº†æ¸…ç†æ–‡æœ¬çš„å„ç§æ–¹æ³•çš„ç»¼åˆåˆ—è¡¨ï¼Œå¦‚è¯æ±‡åŒ–å’Œå¦‚ä½•å®ç°å®ƒä»¬çš„ä»£ç ç‰‡æ®µã€‚</p><p id="c794" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘é€šè¿‡ä»¥ä¸‹é“¾æ¥æ³¨å†Œmediumæ¥æ”¯æŒæˆ‘: )</p><p id="c9ea" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">joshnjuny.medium.com<a class="ae ky" href="https://joshnjuny.medium.com/membership" rel="noopener"/></p><p id="c778" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">ä½ ä¸ä»…å¯ä»¥çœ‹åˆ°æˆ‘ï¼Œè¿˜å¯ä»¥çœ‹åˆ°å…¶ä»–ä½œè€…å†™çš„è¿™ä¹ˆå¤šæœ‰ç”¨å’Œæœ‰è¶£çš„æ–‡ç« å’Œå¸–å­ï¼</p><h1 id="2ffd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">å…³äºä½œè€…</h1><p id="85a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="nd">æ•°æ®ç§‘å­¦å®¶ã€‚åŠ å·å¤§å­¦æ¬§æ–‡åˆ†æ ¡ä¿¡æ¯å­¦ä¸“ä¸šä¸€å¹´çº§åšå£«ç”Ÿã€‚</em></p><p id="4509" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated"><em class="nd">å¯†æ­‡æ ¹å¤§å­¦åˆ‘äº‹å¸æ³•è¡Œæ”¿è®°å½•ç³»ç»Ÿ(CJARS)ç»æµå­¦å®éªŒå®¤çš„å‰ç ”ç©¶é¢†åŸŸä¸“å®¶ï¼Œè‡´åŠ›äºç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆã€è‡ªåŠ¨åŒ–æ•°æ®è´¨é‡å®¡æŸ¥ã€æ„å»ºæ•°æ®ç®¡é“å’Œæ•°æ®æ ‡å‡†åŒ–&amp;åè°ƒã€‚Spotifyå‰æ•°æ®ç§‘å­¦å®ä¹ ç”Ÿã€‚Inc .(çº½çº¦å¸‚)ã€‚</em></p><p id="15fb" class="pw-post-body-paragraph lr ls it lt b lu my ju lw lx mz jx lz ma na mc md me nb mg mh mi nc mk ml mm im bi translated">ä»–å–œæ¬¢è¿åŠ¨ã€å¥èº«ã€çƒ¹é¥ªç¾å‘³çš„äºšæ´²é£Ÿç‰©ã€çœ‹kdramaså’Œåˆ¶ä½œ/è¡¨æ¼”éŸ³ä¹ï¼Œæœ€é‡è¦çš„æ˜¯å´‡æ‹œæˆ‘ä»¬çš„ä¸»è€¶ç¨£åŸºç£ã€‚ç»“è´¦ä»–çš„ <a class="ae ky" href="http://seungjun-data-science.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="nd">ç½‘ç«™</em> </a> <em class="nd">ï¼</em></p></div></div>    
</body>
</html>
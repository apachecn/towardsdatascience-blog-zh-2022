<html>
<head>
<title>Stable diffusion using Hugging Face â€” Variations of Stable Diffusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">åˆ©ç”¨æ‹¥æŠ±é¢çš„ç¨³å®šæ‰©æ•£â€”â€”ç¨³å®šæ‰©æ•£çš„å˜åŒ–</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265#2022-11-15">https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265#2022-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e6a9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">ä½¿ç”¨<a class="ae kf" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">æ‹¥æŠ±é¢éƒ¨æ‰©æ•£å™¨åº“</a>çš„è´Ÿé¢æç¤ºå’Œå›¾åƒåˆ°å›¾åƒç¨³å®šæ‰©æ•£ç®¡é“çš„ä»‹ç»</h2></div><p id="7604" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">è¿™æ˜¯æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« çš„ç»­ç¯‡â€”â€”<a class="ae kf" href="https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8" rel="noopener">ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£|ä½œè€…:Aayush agr awal | 2022 å¹´ 11 æœˆ|è¿ˆå‘æ•°æ®ç§‘å­¦(medium.com)</a>ã€‚</p><p id="2262" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">åœ¨å‰ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å›é¡¾äº†ç¨³å®šæ‰©æ•£çš„æ‰€æœ‰å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä»¥åŠå¦‚ä½•è®©<code class="fe lc ld le lf b">prompt to image</code>ç®¡é“å·¥ä½œã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ç¼–è¾‘<code class="fe lc ld le lf b">prompt to image</code>å‡½æ•°æ¥ä¸ºæˆ‘ä»¬çš„ç¨³å®šæ‰©æ•£ç®¡é“æ·»åŠ é¢å¤–çš„åŠŸèƒ½ï¼Œå³<code class="fe lc ld le lf b">Negative prompting</code>å’Œ<code class="fe lc ld le lf b">Image to Image</code>ç®¡é“ã€‚å¸Œæœ›è¿™å°†æä¾›è¶³å¤Ÿçš„åŠ¨åŠ›æ¥ç©è¿™ä¸ªå‡½æ•°å¹¶è¿›è¡Œæ‚¨çš„ç ”ç©¶ã€‚</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/26c1ceb006b5d462fec26fa14dd52628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*-MV6xVxqQeRdMxZl.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 1:ä½¿ç”¨ prompt - <br/>â€œåœ¨ä¸¤ä¸ªä¸åŒæ–¹å‘åˆ†å‰çš„é“è·¯â€çš„ç¨³å®šæ‰©æ•£ç”Ÿæˆçš„å›¾åƒ</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="3ced" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">1.å˜ä½“ 1:å¦å®šæç¤º</h1><h2 id="dfc9" class="mr ma iq bd mb ms mt dn mf mu mv dp mj kp mw mx ml kt my mz mn kx na nb mp nc bi translated">1.1 ä»€ä¹ˆæ˜¯è´Ÿé¢æç¤ºï¼Ÿ</h2><p id="e8da" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">å¦å®šæç¤ºæ˜¯æˆ‘ä»¬å¯ä»¥æ·»åŠ åˆ°æ¨¡å‹ä¸­çš„é™„åŠ åŠŸèƒ½ï¼Œç”¨æ¥å‘Šè¯‰ç¨³å®šæ‰©æ•£æ¨¡å‹æˆ‘ä»¬ä¸å¸Œæœ›åœ¨ç”Ÿæˆçš„å›¾åƒä¸­çœ‹åˆ°ä»€ä¹ˆã€‚è¿™ä¸ªç‰¹æ€§å¾ˆå—æ¬¢è¿ï¼Œå¯ä»¥ä»åŸå§‹ç”Ÿæˆçš„å›¾åƒä¸­åˆ é™¤ç”¨æˆ·ä¸æƒ³çœ‹åˆ°çš„ä»»ä½•å†…å®¹ã€‚</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/a5f0cf0d035bcbb82819e9bf25b20e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-ZdRTLNHCqwSarUA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 2:å¦å®šæç¤ºç¤ºä¾‹</p></figure><h2 id="bd20" class="mr ma iq bd mb ms mt dn mf mu mv dp mj kp mw mx ml kt my mz mn kx na nb mp nc bi translated">1.2 é€šè¿‡ä»£ç ç†è§£è´Ÿé¢æç¤º</h2><p id="f59f" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€éœ€çš„åº“å’ŒåŠ©æ‰‹å‡½æ•°å¼€å§‹ã€‚æ‰€æœ‰è¿™äº›éƒ½å·²ç»åœ¨ä¹‹å‰çš„<a class="ae kf" href="https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8" rel="noopener">å¸–å­</a>ä¸­ä½¿ç”¨å’Œè§£é‡Šè¿‡äº†ã€‚</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="0e15" class="nr ma iq lf b be ns nt l nu nv">import torch, logging<br/><br/>## disable warnings<br/>logging.disable(logging.WARNING)  <br/><br/>## Imaging  library<br/>from PIL import Image<br/>from torchvision import transforms as tfms<br/><br/><br/>## Basic libraries<br/>from fastdownload import FastDownload<br/>import numpy as np<br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from IPython.display import display<br/>import shutil<br/>import os<br/><br/>## For video display<br/>from IPython.display import HTML<br/>from base64 import b64encode<br/><br/><br/>## Import the CLIP artifacts <br/>from transformers import CLIPTextModel, CLIPTokenizer<br/>from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler<br/><br/>## Initiating tokenizer and encoder.<br/>tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)<br/>text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")<br/><br/>## Initiating the VAE<br/>vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")<br/><br/>## Initializing a scheduler and Setting number of sampling steps<br/>scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)<br/>scheduler.set_timesteps(50)<br/><br/>## Initializing the U-Net model<br/>unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")<br/><br/>## Helper functions<br/>def load_image(p):<br/>    '''<br/>    Function to load images from a defined path<br/>    '''<br/>    return Image.open(p).convert('RGB').resize((512,512))<br/><br/>def pil_to_latents(image):<br/>    '''<br/>    Function to convert image to latents<br/>    '''<br/>    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0<br/>    init_image = init_image.to(device="cuda", dtype=torch.float16) <br/>    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215<br/>    return init_latent_dist<br/><br/>def latents_to_pil(latents):<br/>    '''<br/>    Function to convert latents to images<br/>    '''<br/>    latents = (1 / 0.18215) * latents<br/>    with torch.no_grad():<br/>        image = vae.decode(latents).sample<br/>    image = (image / 2 + 0.5).clamp(0, 1)<br/>    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()<br/>    images = (image * 255).round().astype("uint8")<br/>    pil_images = [Image.fromarray(image) for image in images]<br/>    return pil_images<br/><br/>def text_enc(prompts, maxlen=None):<br/>    '''<br/>    A function to take a texual promt and convert it into embeddings<br/>    '''<br/>    if maxlen is None: maxlen = tokenizer.model_max_length<br/>    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") <br/>    return text_encoder(inp.input_ids.to("cuda"))[0].half()</span></pre><p id="3829" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">ç°åœ¨æˆ‘ä»¬è¦é€šè¿‡ä¼ é€’ä¸€ä¸ªé¢å¤–çš„å‡½æ•°<code class="fe lc ld le lf b">neg_prompts</code>æ¥æ”¹å˜<code class="fe lc ld le lf b">prompt_2_img</code>å‡½æ•°ã€‚å¦å®šæç¤ºçš„å·¥ä½œæ–¹å¼æ˜¯åœ¨é‡‡æ ·æ—¶ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ–‡æœ¬ä»£æ›¿ç©ºå­—ç¬¦ä¸²è¿›è¡Œæ— æ¡ä»¶åµŒå…¥(<code class="fe lc ld le lf b">uncond</code>)ã€‚</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nw"><img src="../Images/8caf58ac1ca0908894f9afc5a3052e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*okDcT2Tk3rfKUMKF.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 3:è´Ÿæç¤ºä»£ç å˜åŒ–</p></figure><p id="83d3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬åšè¿™ä¸ªæ”¹å˜å¹¶æ›´æ–°æˆ‘ä»¬çš„<code class="fe lc ld le lf b">prompt_2_img</code>å‡½æ•°ã€‚</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="66a2" class="nr ma iq lf b be ns nt l nu nv">def prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    <br/>    # Defining batch size<br/>    bs = len(prompts) <br/>    <br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding negative prompt condition<br/>    if not neg_prompts: uncond =  text_enc([""] * bs, text.shape[1])<br/>    # Adding an unconditional prompt , helps in the generation process<br/>    else: uncond =  text_enc(neg_prompts, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Initiating random noise<br/>    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Adding noise to the latents <br/>    latents = latents.to("cuda").half() * scheduler.init_noise_sigma<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(scheduler.timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/>        <br/>        # Conditioning  the latents<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>        <br/>        # Saving intermediate images<br/>        if save_int: <br/>            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')<br/>            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')<br/>            <br/>    # Returning the latent representation to output an image of 3x512x512<br/>    return latents_to_pil(latents)</span></pre><p id="a929" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå‡½æ•°æ˜¯å¦å¦‚é¢„æœŸçš„é‚£æ ·å·¥ä½œã€‚</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="3c67" class="nr ma iq lf b be ns nt l nu nv">## Image without neg prompt<br/>images = [None, None]<br/>images[0] = prompt_2_img(prompts = ["A dog wearing a white hat"], neg_prompts=[""],steps=50, save_int=False)[0]<br/>images[1] = prompt_2_img(prompts = ["A dog wearing a white hat"], neg_prompts=["White hat"],steps=50, save_int=False)[0]<br/>    <br/>## Plotting side by side<br/>fig, axs = plt.subplots(1, 2, figsize=(12, 6))<br/>for c, img in enumerate(images): <br/>    axs[c].imshow(img)<br/>    if c == 0 : axs[c].set_title(f"A dog wearing a white hat")<br/>    else: axs[c].set_title(f"Neg prompt - white hat")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nx"><img src="../Images/e73a2278b4876ba5048031be3769a31f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wnoQYUZJiCwhtUTg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 4:è´Ÿé¢æç¤ºçš„å¯è§†åŒ–ã€‚å·¦ä¾§ SD ç”Ÿæˆæç¤ºâ€œæˆ´ç™½å¸½å­çš„ç‹—â€,å³ä¾§ç›¸åŒæ ‡é¢˜ç”Ÿæˆå¦å®šæç¤ºâ€œç™½å¸½å­â€</p></figure><p id="719b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åŠŸèƒ½ï¼Œå¯ä»¥æ ¹æ®æ‚¨çš„å–œå¥½å¾®è°ƒå›¾åƒã€‚ä½ ä¹Ÿå¯ä»¥ç”¨å®ƒæ¥ç”Ÿæˆä¸€å¼ éå¸¸é€¼çœŸçš„è„¸ï¼Œå°±åƒè¿™ä¸ª<a class="ae kf" href="https://www.reddit.com/r/StableDiffusion/comments/yqnh2c/closeup_photo_of_a_face_just_txt2img_and_lsdr/" rel="noopener ugc nofollow" target="_blank"> Reddit å¸–å­</a>ä¸€æ ·ã€‚è®©æˆ‘ä»¬è¯•è¯•-</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="d4bf" class="nr ma iq lf b be ns nt l nu nv">prompt = ['Close-up photography of the face of a 30 years old man with brown eyes, (by Alyssa Monks:1.1), by Joseph Lorusso, by Lilia Alvarado, beautiful lighting, sharp focus, 8k, high res, (pores:0.1), (sweaty:0.8), Masterpiece, Nikon Z9, Award - winning photograph']<br/>neg_prompt = ['lowres, signs, memes, labels, text, food, text, error, mutant, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, made by children, caricature, ugly, boring, sketch, lacklustre, repetitive, cropped, (long neck), facebook, youtube, body horror, out of frame, mutilated, tiled, frame, border, porcelain skin, doll like, doll']<br/>images = prompt_2_img(prompts = prompt, neg_prompts=neg_prompt, steps=50, save_int=False)<br/>images[0]</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/7088aff5ea5ea90d218f2242ec3733c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*VoO-iaVaD8rcqwad.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 5:ä½¿ç”¨è´Ÿé¢æç¤ºç”Ÿæˆçš„å›¾åƒã€‚</p></figure><p id="93f2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">ç›¸å½“æ•´æ´ï¼æˆ‘å¸Œæœ›è¿™èƒ½ç»™ä½ ä¸€äº›æƒ³æ³•ï¼Œå…³äºå¦‚ä½•å¼€å§‹ä½ è‡ªå·±çš„ç¨³å®šæ‰©æ•£çš„å˜åŒ–ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ç¨³å®šæ‰©æ•£çš„å¦ä¸€ç§å˜åŒ–ã€‚</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="2e97" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">2.å˜ä½“ 2:å›¾åƒåˆ°å›¾åƒç®¡é“</h1><h2 id="c872" class="mr ma iq bd mb ms mt dn mf mu mv dp mj kp mw mx ml kt my mz mn kx na nb mp nc bi translated">2.1 ä»€ä¹ˆæ˜¯å›¾åƒåˆ°å›¾åƒç®¡é“ï¼Ÿ</h2><p id="ad61" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">å¦‚ä¸Šæ‰€è¿°ï¼Œ<code class="fe lc ld le lf b">prompt_2_img</code>å‡½æ•°å¼€å§‹ä»éšæœºé«˜æ–¯å™ªå£°ä¸­ç”Ÿæˆå›¾åƒï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªåˆå§‹ç§å­å›¾åƒæ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿè¿™æ­£æ˜¯å›¾åƒåˆ°å›¾åƒç®¡é“çš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆå§‹ç§å­å›¾åƒå°†å®ƒä¸ä¸€äº›å™ªå£°æ··åˆ(è¿™å¯ä»¥ç”±ä¸€ä¸ª<code class="fe lc ld le lf b">strength</code>å‚æ•°æ¥å¼•å¯¼)ï¼Œç„¶åè¿è¡Œæ‰©æ•£å¾ªç¯ï¼Œè€Œä¸æ˜¯çº¯ç²¹ä¾èµ–äºè¾“å‡ºå›¾åƒçš„æ–‡æœ¬è°ƒèŠ‚ã€‚</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ny"><img src="../Images/09b08c3cc5482fb3c937cf121e064efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U34RCDrNa6qKj2rp.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 6:å›¾åƒåˆ°å›¾åƒç®¡é“ç¤ºä¾‹ã€‚</p></figure><h1 id="2d1d" class="lz ma iq bd mb mc nz me mf mg oa mi mj jw ob jx ml jz oc ka mn kc od kd mp mq bi translated">2.2 é€šè¿‡ä»£ç ç†è§£å›¾åƒåˆ°å›¾åƒçš„æç¤º</h1><p id="8fbd" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">ç°åœ¨æˆ‘ä»¬è¦æ”¹å˜ä¸Šé¢å®šä¹‰çš„<code class="fe lc ld le lf b">prompt_2_img</code>å‡½æ•°ã€‚æˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬çš„<code class="fe lc ld le lf b">prompt_2_img_i2i</code>å‡½æ•°- <br/> 1 å¼•å…¥å¦å¤–ä¸¤ä¸ªå‚æ•°ã€‚<code class="fe lc ld le lf b">init_img</code>:å®ƒå°†æ˜¯åŒ…å«ç§å­å›¾åƒ<br/> 2 çš„<code class="fe lc ld le lf b">Image</code>å¯¹è±¡ã€‚<code class="fe lc ld le lf b">strength</code>:è¯¥å‚æ•°å– 0 åˆ° 1 ä¹‹é—´çš„å€¼ã€‚å€¼è¶Šé«˜ï¼Œæœ€ç»ˆå›¾åƒçœ‹èµ·æ¥å°±è¶Šä¸åƒç§å­å›¾åƒã€‚</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="fa38" class="nr ma iq lf b be ns nt l nu nv">def prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512, save_int=False):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompt) <br/><br/>    # Adding negative prompt condition<br/>    if not neg_prompts: uncond =  text_enc([""] * bs, text.shape[1])<br/>    # Adding an unconditional prompt , helps in the generation process<br/>    else: uncond =  text_enc(neg_prompts, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # Computing the timestep to start the diffusion loop<br/>    t_start = max(steps - init_timestep, 0)<br/>    timesteps = scheduler.timesteps[t_start:].to("cuda")<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/>        <br/>        # Conditioning  the latents<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>        <br/>        # Saving intermediate images<br/>        if save_int: <br/>            if not os.path.exists(f'./steps'):<br/>                os.mkdir(f'./steps')<br/>            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')<br/>            <br/>    # Returning the latent representation to output an image of 3x512x512<br/>    return latents_to_pil(latents)</span></pre><p id="8f2c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">ä½ ä¼šæ³¨æ„åˆ°ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨éšæœºå™ªå£°ï¼Œè€Œæ˜¯ä½¿ç”¨<code class="fe lc ld le lf b">strength</code>å‚æ•°æ¥è®¡ç®—æ·»åŠ å¤šå°‘å™ªå£°ä»¥åŠè¿è¡Œæ‰©æ•£å¾ªç¯çš„æ­¥éª¤æ•°ã€‚é€šè¿‡å°†å¼ºåº¦(é»˜è®¤å€¼= 0.8)ä¹˜ä»¥ç¬¬ 10(50-50 * 0.8)æ­¥çš„æ­¥æ•°(é»˜è®¤å€¼= 50)å¹¶è¿è¡Œå‰©ä½™ 40(50*0.8)æ­¥çš„æ‰©æ•£å¾ªç¯æ¥è®¡ç®—å™ªæ³¢é‡ã€‚è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªåˆå§‹å›¾åƒï¼Œå¹¶é€šè¿‡<code class="fe lc ld le lf b">prompt_2_img_i2i</code>å‡½æ•°ä¼ é€’å®ƒã€‚</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="b4e2" class="nr ma iq lf b be ns nt l nu nv">p = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')<br/>image = Image.open(p).convert('RGB').resize((512,512))<br/>prompt = ["Wolf howling at the moon, photorealistic 4K"]<br/>images = prompt_2_img_i2i(prompts = prompt, init_img = image)<br/><br/>## Plotting side by side<br/>fig, axs = plt.subplots(1, 2, figsize=(12, 6))<br/>for c, img in enumerate([image, images[0]]): <br/>    axs[c].imshow(img)<br/>    if c == 0 : axs[c].set_title(f"Initial image")<br/>    else: axs[c].set_title(f"Image 2 Image output")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nx"><img src="../Images/41bf4b29ac8c6fff3986b6a8719ad84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jIRRyIbGiUh8KEdw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">å›¾ 7:å›¾åƒåˆ°å›¾åƒç®¡é“çš„å¯è§†åŒ–ã€‚å·¦è¾¹æ˜¯ img2img ç®¡é“ä¸­ä¼ é€’çš„åˆå§‹å›¾åƒï¼Œå³è¾¹æ˜¯ img2img ç®¡é“çš„è¾“å‡ºã€‚</p></figure><p id="12ea" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„<code class="fe lc ld le lf b">prompt_2_img_i2i</code>å‡½æ•°ä»æä¾›çš„åˆå§‹è‰å›¾ä¸­åˆ›å»ºäº†ä¸€ä¸ªæ¼‚äº®çš„å²è¯—å›¾åƒã€‚</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="53b9" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">3 ç»“è®º</h1><p id="f8f6" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">æˆ‘å¸Œæœ›è¿™èƒ½å¾ˆå¥½åœ°æ¦‚è¿°å¦‚ä½•è°ƒæ•´<code class="fe lc ld le lf b">prompt_2_img</code>å‡½æ•°ï¼Œä¸ºä½ çš„ç¨³å®šæ‰©æ•£å¾ªç¯å¢åŠ é¢å¤–çš„èƒ½åŠ›ã€‚å¯¹è¿™ä¸ªä½çº§å‡½æ•°çš„ç†è§£å¯¹äºå°è¯•ä½ è‡ªå·±çš„æƒ³æ³•æ¥æ”¹å–„ç¨³å®šæ‰©æ•£æˆ–å®ç°æˆ‘å¯èƒ½åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æ¶‰åŠçš„æ–°è®ºæ–‡æ˜¯æœ‰ç”¨çš„ã€‚</p><p id="95a4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">æˆ‘å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»å®ƒï¼Œå¹¶éšæ—¶ä½¿ç”¨æˆ‘çš„ä»£ç ï¼Œå¹¶å°è¯•ç”Ÿæˆæ‚¨çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå¦‚æœå¯¹ä»£ç æˆ–åšå®¢å¸–å­æœ‰ä»»ä½•åé¦ˆï¼Œè¯·éšæ—¶è”ç³» aayushmnit@gmail.com çš„ LinkedIn æˆ–å‘ç”µå­é‚®ä»¶ç»™æˆ‘ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ç½‘ç«™ä¸Šé˜…è¯»åšå®¢çš„æ—©æœŸå‘å¸ƒ<a class="ae kf" href="https://aayushmnit.com/blog.html" rel="noopener ugc nofollow" target="_blank">Aayush agr awal-åšå®¢(aayushmnit.com)</a>ã€‚</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4d9d" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">4 å‚è€ƒæ–‡çŒ®</h1><ul class=""><li id="6ec6" class="oe of iq ki b kj nd km ne kp og kt oh kx oi lb oj ok ol om bi translated"><a class="ae kf" href="https://www.fast.ai/posts/part2-2022-preview.html" rel="noopener ugc nofollow" target="_blank"> Fast.ai è¯¾ç¨‹â€”â€”ã€Šä»æ·±åº¦å­¦ä¹ åŸºç¡€åˆ°ç¨³å®šæ‰©æ•£ã€‹å‰ä¸¤èŠ‚</a></li><li id="baa8" class="oe of iq ki b kj on km oo kp op kt oq kx or lb oj ok ol om bi translated"><a class="ae kf" href="https://huggingface.co/blog/stable_diffusion" rel="noopener ugc nofollow" target="_blank">ğŸ§¨æ‰©æ•£å™¨çš„ç¨³å®šæ‰©æ•£</a></li><li id="cbe0" class="oe of iq ki b kj on km oo kp op kt oq kx or lb oj ok ol om bi translated"><a class="ae kf" href="https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">è¿›å…¥ç¨³å®šæ‰©æ•£çš„ä¸–ç•Œ</a></li></ul></div></div>    
</body>
</html>
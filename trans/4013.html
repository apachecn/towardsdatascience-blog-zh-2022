<html>
<head>
<title>Smoothly Transition from Pandas to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä»ç†ŠçŒ«åˆ°PySparkçš„å¹³ç¨³è¿‡æ¸¡</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/equivalents-between-pandas-and-pyspark-c8b5ba57dc1d#2022-09-07">https://towardsdatascience.com/equivalents-between-pandas-and-pyspark-c8b5ba57dc1d#2022-09-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bdf5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">ç†ŠçŒ«å¤§æˆ˜PySpark 101</h2></div><p id="1b3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ä½œè€…:<a class="ae le" href="https://medium.com/u/d38873cbc5aa?source=post_page-----40d1ab7243c2--------------------------------" rel="noopener">é˜¿ç›å°”Â·å“ˆæ–¯å°¼</a> &amp; <a class="ae le" href="https://medium.com/u/7f47bdb8b8c0?source=post_page-----40d1ab7243c2--------------------------------" rel="noopener">è¿ªäºšÂ·èµ«ç±³æ‹‰</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/8a29eda56a3a3ff4103de32036e263b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EDJcxpPju9x60ddilPi0NQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@jeremythomasphoto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">æ°ç‘ç±³Â·æ‰˜é©¬æ–¯</a>åœ¨<a class="ae le" href="https://unsplash.com/s/photos/change?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>ä¸Šæ‹ç…§</p></figure><p id="7b01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç†ŠçŒ«æ˜¯æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶çš„é¦–é€‰å›¾ä¹¦é¦†ã€‚å¯¹äºæ¯ä¸ªå¸Œæœ›æ“ä½œæ•°æ®å’Œæ‰§è¡Œä¸€äº›æ•°æ®åˆ†æçš„äººæ¥è¯´ï¼Œè¿™æ˜¯å¿…ä¸å¯å°‘çš„ã€‚</p><p id="7655" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶è€Œï¼Œå°½ç®¡å®ƒçš„å®ç”¨æ€§å’Œå¹¿æ³›çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¾ˆå¿«å°±å¼€å§‹çœ‹åˆ°å®ƒåœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶çš„å±€é™æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿‡æ¸¡åˆ°PySparkå˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæä¾›äº†åœ¨å¤šå°æœºå™¨ä¸Šè¿è¡Œæ“ä½œçš„å¯èƒ½æ€§ï¼Œä¸åƒPandasã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æä¾›PySparkä¸­pandasæ–¹æ³•çš„ç­‰æ•ˆæ–¹æ³•ï¼Œä»¥åŠç°æˆçš„ä»£ç ç‰‡æ®µï¼Œä»¥ä¾¿äºPySparkæ–°æ‰‹å®Œæˆä»»åŠ¡ğŸ˜‰</p><blockquote class="lv"><p id="6fac" class="lw lx it bd ly lz ma mb mc md me ld dk translated">PySparkæä¾›äº†åœ¨å¤šå°æœºå™¨ä¸Šè¿è¡Œæ“ä½œçš„å¯èƒ½æ€§ï¼Œä¸åƒPandas</p></blockquote><pre class="mf mg mh mi mj mk ml mm mn aw mo bi"><span id="6918" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">Table Of Contents</strong><br/>Â· <a class="ae le" href="#8396" rel="noopener ugc nofollow">DataFrame creation</a><br/>Â· <a class="ae le" href="#dfc7" rel="noopener ugc nofollow">Specifying columns types</a><br/>Â· <a class="ae le" href="#7382" rel="noopener ugc nofollow">Reading and writing files</a><br/>Â· <a class="ae le" href="#710b" rel="noopener ugc nofollow">Filtering</a><br/>    âˆ˜ <a class="ae le" href="#50d3" rel="noopener ugc nofollow">Specific columns</a><br/>    âˆ˜ <a class="ae le" href="#f04d" rel="noopener ugc nofollow">Specific lines</a><br/>    âˆ˜ <a class="ae le" href="#e50f" rel="noopener ugc nofollow">Using a condition</a><br/>Â· <a class="ae le" href="#901e" rel="noopener ugc nofollow">Add a column</a><br/>Â· <a class="ae le" href="#6990" rel="noopener ugc nofollow">Concatenate dataframes</a><br/>    âˆ˜ <a class="ae le" href="#0b25" rel="noopener ugc nofollow">Two Dataframes</a><br/>    âˆ˜ <a class="ae le" href="#da7a" rel="noopener ugc nofollow">Multiple Dataframes</a><br/>Â· <a class="ae le" href="#a849" rel="noopener ugc nofollow">Computing specified statistics</a><br/>Â· <a class="ae le" href="#8b6c" rel="noopener ugc nofollow">Aggregations</a><br/>Â· <a class="ae le" href="#27da" rel="noopener ugc nofollow">Apply a transformation over a column</a></span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="d76e" class="nc mq it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">å…¥é—¨æŒ‡å—</h1><p id="aca3" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">åœ¨æ·±å…¥ç ”ç©¶å¯¹ç­‰ç‰©ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸ºåé¢åšå‡†å¤‡ã€‚ä¸è¨€è€Œå–»ï¼Œç¬¬ä¸€æ­¥æ˜¯å¯¼å…¥æ‰€éœ€çš„åº“:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="861a" class="mp mq it ml b gy mr ms l mt mu">import pandas as pd<br/>import pyspark.sql.functions as F</span></pre><p id="fce1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySparkåŠŸèƒ½çš„å…¥å£ç‚¹æ˜¯SparkSessionç±»ã€‚é€šè¿‡SparkSessionå®ä¾‹ï¼Œæ‚¨å¯ä»¥åˆ›å»ºæ•°æ®å¸§ã€åº”ç”¨å„ç§è½¬æ¢ã€è¯»å†™æ–‡ä»¶ç­‰ã€‚è¦å®šä¹‰SparkSessionï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å†…å®¹:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="d29b" class="mp mq it ml b gy mr ms l mt mu">from pyspark.sql import SparkSession<br/>spark = SparkSession\<br/>.builder\<br/>.appName('SparkByExamples.com')\<br/>.getOrCreate()</span></pre><p id="151b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç°åœ¨ä¸€åˆ‡éƒ½å‡†å¤‡å¥½äº†ï¼Œè®©æˆ‘ä»¬ç›´æ¥è¿›å…¥ç†ŠçŒ«å¤§æˆ˜PySparkçš„éƒ¨åˆ†å§ï¼</p><h1 id="8396" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">æ•°æ®å¸§åˆ›å»º</h1><p id="ad0d" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®æ ·æœ¬:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="50bc" class="mp mq it ml b gy mr ms l mt mu">columns = ["employee","department","state","salary","age"]<br/>data = [("Alain","Sales","Paris",60000,34),<br/>        ("Ahmed","Sales","Lyon",80000,45),<br/>        ("Ines","Sales","Nice",55000,30),<br/>        ("Fatima","Finance","Paris",90000,28),<br/>        ("Marie","Finance","Nantes",100000,40)]</span></pre><p id="e78a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¦åˆ›å»ºä¸€åª<strong class="kk iu">ç†ŠçŒ«</strong> <code class="fe od oe of ml b"> DataFrame</code>ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="e55b" class="mp mq it ml b gy mr ms l mt mu">df = pd.DataFrame(data=data, columns=columns)<br/><br/># Show a few lines<br/>df.head(2)</span></pre><p id="bc70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="07b5" class="mp mq it ml b gy mr ms l mt mu">df = spark.createDataFrame(data).toDF(*columns)<br/><br/># Show a few lines<br/>df.limit(2).show()</span></pre><h1 id="dfc7" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">æŒ‡å®šåˆ—ç±»å‹</h1><p id="0453" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated"><strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="ced4" class="mp mq it ml b gy mr ms l mt mu">types_dict = {<br/>    "employee": pd.Series([r[0] for r in data], dtype='str'),<br/>    "department": pd.Series([r[1] for r in data], dtype='str'),<br/>    "state": pd.Series([r[2] for r in data], dtype='str'),<br/>    "salary": pd.Series([r[3] for r in data], dtype='int'),<br/>    "age": pd.Series([r[4] for r in data], dtype='int')<br/>}<br/><br/>df = pd.DataFrame(types_dict)</span></pre><p id="b565" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æ‚¨å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹ä»£ç è¡Œæ¥æ£€æŸ¥æ‚¨çš„ç±»å‹:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="5204" class="mp mq it ml b gy mr ms l mt mu">df.dtypes</span></pre><p id="c242" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="0ef8" class="mp mq it ml b gy mr ms l mt mu">from pyspark.sql.types import StructType,StructField, StringType, IntegerType<br/><br/>schema = StructType([ \<br/>    StructField("employee",StringType(),True), \<br/>    StructField("department",StringType(),True), \<br/>    StructField("state",StringType(),True), \<br/>    StructField("salary", IntegerType(), True), \<br/>    StructField("age", IntegerType(), True) \<br/>  ])<br/><br/>df = spark.createDataFrame(data=data,schema=schema)</span></pre><p id="6fa4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æ‚¨å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ£€æŸ¥æ•°æ®å¸§çš„æ¨¡å¼:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="ccae" class="mp mq it ml b gy mr ms l mt mu">df.dtypes<br/># OR<br/>df.printSchema()</span></pre><h1 id="7382" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">è¯»å–å’Œå†™å…¥æ–‡ä»¶</h1><p id="db75" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">ç†ŠçŒ«å’ŒPySparkçš„é˜…è¯»å’Œå†™ä½œæ˜¯å¦‚æ­¤çš„ç›¸ä¼¼ã€‚è¯­æ³•å¦‚ä¸‹:<strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="9a64" class="mp mq it ml b gy mr ms l mt mu">df = pd.read_csv(path, sep=';', header=True)<br/>df.to_csv(path, ';', index=False)</span></pre><p id="4f7d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="be3c" class="mp mq it ml b gy mr ms l mt mu">df = spark.read.csv(path, sep=';')<br/>df.coalesce(n).write.mode('overwrite').csv(path, sep=';')</span></pre><p id="5b48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">æ³¨1ğŸ’¡:</strong>æ‚¨å¯ä»¥æŒ‡å®šè¦å¯¹å…¶è¿›è¡Œåˆ†åŒºçš„åˆ—:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="04a8" class="mp mq it ml b gy mr ms l mt mu">df.partitionBy("department","state").write.mode('overwrite').csv(path, sep=';')</span></pre><p id="70ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">æ³¨2ğŸ’¡:</strong>æ‚¨å¯ä»¥é€šè¿‡åœ¨ä¸Šé¢çš„æ‰€æœ‰ä»£ç è¡Œä¸­æ›´æ”¹CSV by parquetæ¥è¯»å†™ä¸åŒçš„æ ¼å¼ï¼Œæ¯”å¦‚parquetæ ¼å¼</p><h1 id="710b" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">è¿‡æ»¤</h1><h2 id="50d3" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">ç‰¹å®šåˆ—</h2><p id="c6c6" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">é€‰æ‹©ç†ŠçŒ«ä¸­çš„æŸäº›åˆ—æ˜¯è¿™æ ·å®Œæˆçš„:<strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="062f" class="mp mq it ml b gy mr ms l mt mu">columns_subset = ['employee', 'salary']<br/><br/>df[columns_subset].head()<br/><br/>df.loc[:, columns_subset].head()</span></pre><p id="50c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è€Œåœ¨PySparkä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹åˆ—åˆ—è¡¨ä½¿ç”¨selectæ–¹æ³•:<strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="de23" class="mp mq it ml b gy mr ms l mt mu">columns_subset = ['employee', 'salary']<br/><br/>df.select(columns_subset).show(5)</span></pre><h2 id="f04d" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">ç‰¹å®šçº¿è·¯</h2><p id="7691" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">è¦é€‰æ‹©ä¸€ç³»åˆ—çº¿æ¡ï¼Œæ‚¨å¯ä»¥åœ¨Pandasä¸­ä½¿ç”¨<code class="fe od oe of ml b">iloc</code>æ–¹æ³•:</p><p id="d2e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="334f" class="mp mq it ml b gy mr ms l mt mu"># Take a sample ( first 2 lines )<br/><br/>df.iloc[:2].head()</span></pre><p id="6218" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">åœ¨Sparkä¸­ï¼Œä¸å¯èƒ½è·å¾—ä»»ä½•èŒƒå›´çš„è¡Œå·ã€‚ç„¶è€Œï¼Œå¯ä»¥åƒè¿™æ ·é€‰æ‹©å‰nè¡Œ:</p><p id="f102" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="2cf3" class="mp mq it ml b gy mr ms l mt mu">df.take(2).head()<br/># Or<br/>df.limit(2).head()</span></pre><p id="6483" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">æ³¨</strong>ğŸ’¡:è¯·è®°ä½sparkï¼Œæ•°æ®å¯èƒ½åˆ†å¸ƒåœ¨ä¸åŒçš„è®¡ç®—èŠ‚ç‚¹ä¸Šï¼Œå¹¶ä¸”â€œç¬¬ä¸€â€è¡Œå¯èƒ½ä¼šå› è¿è¡Œè€Œå¼‚ï¼Œå› ä¸ºæ²¡æœ‰åº•å±‚é¡ºåº</p><h2 id="e50f" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">ä½¿ç”¨æ¡ä»¶</h2><p id="5916" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">å¯ä»¥æ ¹æ®ç‰¹å®šæ¡ä»¶è¿‡æ»¤æ•°æ®ã€‚Pandasä¸­çš„è¯­æ³•å¦‚ä¸‹:</p><p id="2e9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç†ŠçŒ«</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="1f8f" class="mp mq it ml b gy mr ms l mt mu"># First method<br/>flt = (df['salary'] &gt;= 90_000) &amp; (df['state'] == 'Paris')<br/>filtered_df = df[flt]<br/><br/># Second Method: Using query which is generally faster<br/>filtered_df = df.query('(salary &gt;= 90_000) and (state == "Paris")')<br/># Or<br/>target_state = "Paris"<br/>filtered_df = df.query('(salary &gt;= 90_000) and (state == @target_state)')</span></pre><p id="14c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">åœ¨Sparkä¸­ï¼Œé€šè¿‡ä½¿ç”¨<code class="fe od oe of ml b">filter</code>æ–¹æ³•æˆ–æ‰§è¡ŒSQLæŸ¥è¯¢å¯ä»¥å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚è¯­æ³•å¦‚ä¸‹:</p><p id="3393" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="e4b9" class="mp mq it ml b gy mr ms l mt mu"># First method<br/>filtered_df = df.filter((F.col('salary') &gt;= 90_000) &amp; (F.col('state') == 'Paris'))<br/><br/># Second Method:<br/>df.createOrReplaceTempView("people")<br/><br/>filtered_df = spark.sql("""<br/>SELECT * FROM people<br/>WHERE (salary &gt;= 90000) and (state == "Paris")<br/>""") <br/><br/># OR<br/>filtered_df = df.filter(F.expr('(salary &gt;= 90000) and (state == "Paris")'))</span></pre><h1 id="901e" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">æ·»åŠ åˆ—</h1><p id="6949" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">åœ¨Pandasä¸­ï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥æ·»åŠ åˆ—:</p><p id="7251" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="db78" class="mp mq it ml b gy mr ms l mt mu">seniority = [3, 5, 2, 4, 10]<br/># Method 1<br/>df['seniority'] = seniority<br/><br/># Method 2<br/>df.insert(2, "seniority", seniority, True)</span></pre><p id="9b41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">åœ¨PySparkä¸­æœ‰ä¸€ä¸ªå«åš<code class="fe od oe of ml b">withColumn</code>çš„ç‰¹æ®Šæ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥æ·»åŠ ä¸€ä¸ªåˆ—:</p><p id="b90e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySpark </p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="b195" class="mp mq it ml b gy mr ms l mt mu">from itertools import chain</span><span id="1915" class="mp mq it ml b gy or ms l mt mu">seniority= {<br/>    'Alain': 3,<br/>    'Ahmed': 5,<br/>    'Ines': 2,<br/>    'Fatima': 4,<br/>    'Marie': 10,<br/>}</span><span id="1415" class="mp mq it ml b gy or ms l mt mu">mapping = create_map([lit(x) for x in chain(*seniority.items())])</span><span id="7138" class="mp mq it ml b gy or ms l mt mu">df.withColumn('<!-- -->seniority<!-- -->', mapping.getItem(F.col("employee")))</span></pre><h1 id="6990" class="nc mq it bd nd ne ny ng nh ni nz nk nl jz oa ka nn kc ob kd np kf oc kg nr ns bi translated">è¿æ¥æ•°æ®å¸§</h1><h2 id="0b25" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">ä¸¤ä¸ªæ•°æ®å¸§</h2><p id="29a2" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated"><strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="5115" class="mp mq it ml b gy mr ms l mt mu">df_to_add = pd.DataFrame(data=[("Robert","Advertisement","Paris",55000,27)], columns=columns)<br/>df = pd.concat([df, df_to_add], ignore_index = True)</span></pre><p id="4121" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="2749" class="mp mq it ml b gy mr ms l mt mu">df_to_add = spark.createDataFrame([("Robert","Advertisement","Paris",55000,27)]).toDF(*columns)<br/>df = df.union(df_to_add)</span></pre><h2 id="da7a" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">å¤šä¸ªæ•°æ®å¸§</h2><p id="18ce" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">ç†ŠçŒ«</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="6aa8" class="mp mq it ml b gy mr ms l mt mu">dfs = [df, df1, df2,...,dfn]<br/>df = pd.concat(dfs, ignore_index = True)</span></pre><p id="8b02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySparkçš„æ–¹æ³•<code class="fe od oe of ml b">unionAll</code>åªè¿æ¥äº†ä¸¤ä¸ªæ•°æ®å¸§ã€‚è§£å†³è¿™ä¸€é™åˆ¶çš„æ–¹æ³•æ˜¯æ ¹æ®éœ€è¦å¤šæ¬¡è¿­ä»£è¿æ¥ã€‚ä¸ºäº†è·å¾—æ›´ç®€æ´ä¼˜é›…çš„è¯­æ³•ï¼Œæˆ‘ä»¬å°†é¿å…å¾ªç¯ï¼Œå¹¶ä½¿ç”¨reduceæ–¹æ³•æ¥åº”ç”¨<code class="fe od oe of ml b">unionAll</code>:</p><p id="ca98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="76cd" class="mp mq it ml b gy mr ms l mt mu">from functools import reduce<br/>from pyspark.sql import DataFrame<br/><br/>def unionAll(*dfs):<br/>    return reduce(DataFrame.unionAll, dfs)<br/><br/>dfs = [df, df1, df2,...,dfn]<br/>df = unionAll(*dfs)</span></pre><h2 id="a849" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">è®¡ç®—æŒ‡å®šçš„ç»Ÿè®¡æ•°æ®</h2><p id="4be0" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä¸€äº›ç»Ÿè®¡KPIæ¥æ‰§è¡Œä¸€äº›æ•°æ®åˆ†æã€‚Pandaså’ŒPySparkéƒ½æä¾›äº†éå¸¸å®¹æ˜“åœ°è·å¾—æ•°æ®å¸§ä¸­æ¯ä¸€åˆ—çš„ä»¥ä¸‹ä¿¡æ¯çš„å¯èƒ½æ€§:</p><ul class=""><li id="e3cd" class="os ot it kk b kl km ko kp kr ou kv ov kz ow ld ox oy oz pa bi translated">åˆ—å…ƒç´ çš„è®¡æ•°</li><li id="b772" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">åˆ—å…ƒç´ çš„å¹³å‡å€¼</li><li id="5f98" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">æ€§ä¼ æ’­ç–¾ç—…</li><li id="441d" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">æœ€å°å€¼</li><li id="c00d" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">ä¸‰ä¸ªç™¾åˆ†ç‚¹:25%ã€50%å’Œ75%</li><li id="a0b1" class="os ot it kk b kl pb ko pc kr pd kv pe kz pf ld ox oy oz pa bi translated">æœ€å¤§å€¼</li></ul><p id="611d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">æ‚¨å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹è¡Œæ¥è®¡ç®—è¿™äº›å€¼:</p><p id="3596" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">ç†ŠçŒ«</strong>å’Œ<strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="bbec" class="mp mq it ml b gy mr ms l mt mu">df.summary()<br/>#OR<br/>df.describe() # the method describe doesn't return the percentiles</span></pre><h2 id="8b6c" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">èšé›†</h2><p id="052b" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">ä¸ºäº†æ‰§è¡Œä¸€äº›èšåˆï¼Œè¯­æ³•å‡ ä¹æ˜¯Pandaså’ŒPySpark: <strong class="kk iu"> Pandas </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="88bb" class="mp mq it ml b gy mr ms l mt mu">df.groupby('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'})</span></pre><p id="670f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> PySpark </strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="f2ff" class="mp mq it ml b gy mr ms l mt mu">df.groupBy('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'})</span></pre><p id="4b3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ç„¶è€Œï¼Œç†ŠçŒ«å’ŒPySparkçš„ç»“æœéœ€è¦ä¸€äº›è°ƒæ•´æ‰èƒ½ç›¸ä¼¼ã€‚1.åœ¨pandasä¸­ï¼Œåˆ†ç»„ä¾æ®çš„åˆ—æˆä¸ºç´¢å¼•:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pg"><img src="../Images/4496e70482da115672dbd07100dbd5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*au70oO6h8rFW1_9F.png"/></div></div></figure><p id="50ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è¦å°†å®ƒä½œä¸ºä¸€ä¸ªåˆ—å–å›ï¼Œæˆ‘ä»¬éœ€è¦åº”ç”¨<code class="fe od oe of ml b"> reset_index</code>æ–¹æ³•:<strong class="kk iu">ç†ŠçŒ«</strong></p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="1d16" class="mp mq it ml b gy mr ms l mt mu">df.groupby('department').agg({'employee': 'count', 'salary':'max', 'age':'mean'}).reset_index()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ph"><img src="../Images/a7acd662984232b165f8d81c9523348a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ed5DSp1cJR7n9VXN.png"/></div></div></figure><ol class=""><li id="c8f9" class="os ot it kk b kl km ko kp kr ou kv ov kz ow ld pi oy oz pa bi translated">åœ¨<strong class="kk iu"> PySpark </strong>ä¸­ï¼Œåˆ—ååœ¨ç»“æœæ•°æ®å¸§ä¸­è¢«ä¿®æ”¹ï¼Œæåˆ°äº†æ‰§è¡Œçš„èšåˆ:</li></ol><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pj"><img src="../Images/f0779685fba253ad6d7d9200800f3cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eLe_FS2exMDFzSzm.png"/></div></div></figure><p id="9d0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">å¦‚æœæ‚¨å¸Œæœ›é¿å…è¿™ç§æƒ…å†µï¼Œæ‚¨éœ€è¦åƒè¿™æ ·ä½¿ç”¨åˆ«åæ–¹æ³•:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="7eaa" class="mp mq it ml b gy mr ms l mt mu">df.groupBy('department').agg(F.count('employee').alias('employee'), F.max('salary').alias('salary'), F.mean('age').alias('age'))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pk"><img src="../Images/a04cc8d7a8ef8fe2ef5afb30ccb7b1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vamC4GOFUpSK87BG.png"/></div></div></figure><h2 id="27da" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">å¯¹åˆ—åº”ç”¨å˜æ¢</h2><p id="91c1" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">è¦å¯¹åˆ—åº”ç”¨æŸç§è½¬æ¢ï¼ŒPySparkä¸­ä¸å†æä¾›applyæ–¹æ³•ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåä¸º<code class="fe od oe of ml b">udf</code>(æˆ–è€…ç”¨æˆ·å®šä¹‰çš„å‡½æ•°)çš„æ–¹æ³•æ¥å°è£…pythonå‡½æ•°ã€‚</p><p id="484c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ä¾‹å¦‚ï¼Œå¦‚æœå·¥èµ„ä½äº60000è‹±é•‘ï¼Œæˆ‘ä»¬éœ€è¦å¢åŠ 15%çš„å·¥èµ„ï¼Œå¦‚æœè¶…è¿‡60000è‹±é•‘ï¼Œæˆ‘ä»¬éœ€è¦å¢åŠ 5%çš„å·¥èµ„ã€‚</p><p id="1683" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">pandasä¸­çš„è¯­æ³•å¦‚ä¸‹:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="46d5" class="mp mq it ml b gy mr ms l mt mu">df['new_salary'] = df['salary'].apply(lambda x: x*1.15 if x&lt;= 60000 else x*1.05)</span></pre><p id="385d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PySparkä¸­çš„å¯¹ç­‰ç”¨æ³•å¦‚ä¸‹:</p><pre class="lg lh li lj gt mk ml mm mn aw mo bi"><span id="85c9" class="mp mq it ml b gy mr ms l mt mu">from pyspark.sql.types import FloatType<br/><br/>df.withColumn('new_salary', F.udf(lambda x: x*1.15 if x&lt;= 60000 else x*1.05, FloatType())('salary'))</span></pre><p id="1fa1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">âš ï¸æ³¨æ„åˆ°<code class="fe od oe of ml b">udf</code>æ–¹æ³•éœ€è¦æ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯FloatType)</p><h2 id="6dd2" class="mp mq it bd nd og oh dn nh oi oj dp nl kr ok ol nn kv om on np kz oo op nr oq bi translated">æœ€åçš„æƒ³æ³•</h2><p id="bba9" class="pw-post-body-paragraph ki kj it kk b kl nt ju kn ko nu jx kq kr nv kt ku kv nw kx ky kz nx lb lc ld im bi translated">æ€»ä¹‹ï¼Œå¾ˆæ˜æ˜¾ï¼ŒPandaså’ŒPySparkçš„è¯­æ³•æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚è¿™å°†æå¤§åœ°ä¿ƒè¿›ä»ä¸€ä¸ªåˆ°å¦ä¸€ä¸ªçš„è¿‡æ¸¡ã€‚</p><p id="e25e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ä½¿ç”¨PySparkåœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ä¼šç»™ä½ å¸¦æ¥å¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒå…è®¸å¹¶è¡Œè®¡ç®—ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æ­£åœ¨å¤„ç†çš„æ•°æ®é›†å¾ˆå°ï¼Œé‚£ä¹ˆæ¢å¤åˆ°å”¯ä¸€çš„ç†ŠçŒ«ä¼šå¾ˆå¿«å˜å¾—æ›´æœ‰æ•ˆã€‚</p><p id="4d77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">å› ä¸ºè¿™ç¯‡æ–‡ç« æ˜¯å…³äºä»pandasåˆ°PySparkçš„å¹³ç¨³è¿‡æ¸¡ï¼Œæ‰€ä»¥æœ‰å¿…è¦æåˆ°ä¸€ä¸ªPandasçš„ç­‰ä»·APIï¼Œå«åš<a class="ae le" href="https://koalas.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">è€ƒæ‹‰</a>ï¼Œå®ƒå·¥ä½œåœ¨Apache Sparkä¸Šï¼Œå› æ­¤å¡«è¡¥äº†ä¸¤è€…ä¹‹é—´çš„ç©ºç™½ã€‚</p><p id="9f43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">è°¢è°¢ä½ åšæŒåˆ°ç°åœ¨ã€‚æ³¨æ„å®‰å…¨ï¼Œä¸‹ä¸€ä¸ªæ•…äº‹å†è§ğŸ˜Šï¼</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="ccde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£å…³äºscikit-learnçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ç« :</p><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/4-scikit-learn-tools-every-data-scientist-should-use-4ee942958d9e"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">4 sci kitâ€”â€”æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶éƒ½åº”è¯¥ä½¿ç”¨çš„å­¦ä¹ å·¥å…·</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">å¤åˆä¼°ç®—å™¨å’Œå˜å‹å™¨</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc lp po"/></div></div></a></div><div class="pl pm gp gr pn po"><a rel="noopener follow" target="_blank" href="/5-hyperparameter-optimization-methods-you-should-use-521e47d7feb0"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd iu gy z fp pt fr fs pu fu fw is bi translated">æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶éƒ½åº”è¯¥ä½¿ç”¨çš„5ç§è¶…å‚æ•°ä¼˜åŒ–æ–¹æ³•</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">ç½‘æ ¼æœç´¢ï¼Œè¿ç»­å‡åŠå’Œè´å¶æ–¯ç½‘æ ¼æœç´¢â€¦</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">towardsdatascience.com</p></div></div><div class="px l"><div class="qd l pz qa qb px qc lp po"/></div></div></a></div></div></div>    
</body>
</html>
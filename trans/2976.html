<html>
<head>
<title>How to Fine-Tune an NLP Regression Model with Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¦‚ä½•ä½¿ç”¨è½¬æ¢å™¨å¾®è°ƒNLPå›å½’æ¨¡å‹</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f#2022-06-29">https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f#2022-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c09c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">ä»æ•°æ®é¢„å¤„ç†åˆ°ä½¿ç”¨çš„å®Œæ•´æŒ‡å—</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8947c6c9f8d0217916b7cb4156a44c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pLDgN56OWQedBWh9.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ç…§ç‰‡ç”±<a class="ae ky" href="https://unsplash.com/@deepmind?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>åœ¨<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>ä¸Šæ‹æ‘„</p></figure><p id="bb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">åƒ<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>è¿™æ ·çš„åœ¨çº¿å›¾ä¹¦é¦†ä¸ºæˆ‘ä»¬æä¾›äº†æœ€å…ˆè¿›çš„é¢„è®­ç»ƒäººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºæ•°æ®ç§‘å­¦çš„è®¸å¤šä¸åŒåº”ç”¨ã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ¥è§£å†³å›å½’é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯DistilBERTï¼Œå®ƒæ˜¯è‘—åçš„BERTçš„æ›´è½»ã€æ›´å¿«çš„ç‰ˆæœ¬ï¼Œå…¶æ€§èƒ½ä¸º95%ã€‚</p><p id="34c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">å‡è®¾æˆ‘ä»¬æœ‰æ¥è‡ªåœ¨çº¿å¹¿å‘Šçš„æ–‡æœ¬ï¼Œå¹¶ä¸”å®ƒçš„å“åº”ç‡è¢«å¹¿å‘Šé›†æ ‡å‡†åŒ–äº†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå¯ä»¥é¢„æµ‹å¹¿å‘Šæ•ˆæœçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</p><p id="9ab8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">è®©æˆ‘ä»¬é€šè¿‡å¯¼å…¥å¿…è¦çš„åº“å¹¶å¯¼å…¥æˆ‘ä»¬çš„æ•°æ®æ¥å¼€å§‹ç¼–ç :</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2ff4" class="ma mb it lw b gy mc md l me mf">import numpy as np<br/>import pandas as pd</span><span id="bc14" class="ma mb it lw b gy mg md l me mf">import transformers<br/>from datasets import Dataset,load_dataset, load_from_disk<br/>from transformers import AutoTokenizer, AutoModelForSequenceClassification</span><span id="9d6a" class="ma mb it lw b gy mg md l me mf">X=pd.read_csv('ad_data.csv')<br/>X.head(3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c66f5a0233b55803783548c278827e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*YAgEc9qO-roVChcR.png"/></div></figure><p id="9769" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">æ–‡æœ¬</strong>ä»£è¡¨å¹¿å‘Šæ–‡æœ¬ï¼Œè€Œ<strong class="lb iu">æ ‡ç­¾</strong>æ˜¯æ ‡å‡†åŒ–å›å¤ç‡ã€‚</p><h1 id="3635" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">ç†ŠçŒ«åˆ°æ•°æ®é›†</h1><p id="c08e" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">ä¸ºäº†ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦å°†<strong class="lb iu"> Pandasæ•°æ®å¸§</strong>è½¬æ¢ä¸ºâ€œ<strong class="lb iu">æ•°æ®é›†</strong>æ ¼å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹ã€‚è¿™äº›å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤è½»æ¾å®Œæˆ:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="09c5" class="ma mb it lw b gy mc md l me mf">dataset = Dataset.from_pandas(X,preserve_index=False) <br/>dataset = dataset.train_test_split(test_size=0.3) </span><span id="c2a6" class="ma mb it lw b gy mg md l me mf">dataset</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c7914f7238dd8f3f58c35004b833dbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/0*ZOLISw1b6nt69e1X.png"/></div></figure><p id="fedd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†å¯¹è±¡åŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ‚¨ä»ç„¶å¯ä»¥è®¿é—®å¦‚ä¸‹æ‰€ç¤ºçš„æ•°æ®:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bf7f" class="ma mb it lw b gy mc md l me mf">dataset['train']['text'][:5]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3dfe25588df5a42775b966b73c4322e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*UtYADlDs-PWF9cks.png"/></div></figure><h1 id="d3b9" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">ä»¤ç‰ŒåŒ–&amp;å¦‚ä½•æ·»åŠ æ–°ä»¤ç‰Œ</h1><p id="3318" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å¯¼å…¥å®ƒçš„æ ‡è®°å™¨å¹¶æ ‡è®°æˆ‘ä»¬çš„æ•°æ®ã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4160" class="ma mb it lw b gy mc md l me mf">tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")</span></pre><p id="6229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">è®©æˆ‘ä»¬æ¥æ ‡è®°ä¸€ä¸ªå¥å­ï¼Œçœ‹çœ‹æˆ‘ä»¬å¾—åˆ°äº†ä»€ä¹ˆ:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4703" class="ma mb it lw b gy mc md l me mf">tokenizer('ğŸš¨ JUNE DROP LIVE ğŸš¨')['input_ids']</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/38d1e16658d44f97947e182ec6f68395.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/0*saouZ9uv8q-yXCd_.png"/></div></figure><p id="6028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">æˆ‘ä»¬å¯ä»¥è§£ç è¿™äº›idå¹¶çœ‹åˆ°å®é™…çš„ä»¤ç‰Œ:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a37f" class="ma mb it lw b gy mc md l me mf">[tokenizer.decode(i) for i in tokenizer('ğŸš¨ JUNE DROP LIVE ğŸš¨')['input_ids']]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/58a3dee3bad617478798a790f15f3db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/0*uyyFVKMDUkl_DMQQ.png"/></div></figure><p id="2147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">ã€CLSã€‘</strong>å’Œ<strong class="lb iu">ã€SEPã€‘</strong>æ˜¯ç‰¹æ®Šçš„æ ‡è®°ï¼Œæ€»æ˜¯å‡ºç°åœ¨å¥å­çš„å¼€å¤´å’Œç»“å°¾ã€‚å¦‚ä½ æ‰€è§ï¼Œä»£æ›¿è¡¨æƒ…ç¬¦å·çš„æ˜¯ğŸš¨æ˜¯<strong class="lb iu">ã€UNKã€‘</strong>ä»¤ç‰Œï¼Œè¡¨ç¤ºä»¤ç‰ŒæœªçŸ¥ã€‚è¿™æ˜¯å› ä¸ºé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹<strong class="lb iu">è’¸é¦å™¨</strong>çš„å•è¯è¢‹é‡Œæ²¡æœ‰è¡¨æƒ…ç¬¦å·ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å‘æ ‡è®°å™¨æ·»åŠ æ›´å¤šçš„æ ‡è®°ï¼Œä»¥ä¾¿åœ¨æˆ‘ä»¬æ ¹æ®æ•°æ®è°ƒæ•´æ¨¡å‹æ—¶å¯ä»¥å¯¹å®ƒä»¬è¿›è¡Œè®­ç»ƒã€‚è®©æˆ‘ä»¬ç»™æˆ‘ä»¬çš„ç¬¦å·åŒ–å™¨æ·»åŠ ä¸€äº›è¡¨æƒ…ç¬¦å·ã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7b27" class="ma mb it lw b gy mc md l me mf">for i in ['ğŸš¨', 'ğŸ™‚', 'ğŸ˜', 'âœŒï¸' , 'ğŸ¤© ']:<br/>    tokenizer.add_tokens(i)</span></pre><p id="2d81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ç°åœ¨ï¼Œå¦‚æœä½ å°†å¥å­ç¬¦å·åŒ–ï¼Œä½ ä¼šçœ‹åˆ°è¡¨æƒ…ç¬¦å·ä»ç„¶æ˜¯è¡¨æƒ…ç¬¦å·ï¼Œè€Œä¸æ˜¯[UNK]ç¬¦å·ã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="38c6" class="ma mb it lw b gy mc md l me mf">[tokenizer.decode(i) for i in tokenizer('ğŸš¨ JUNE DROP LIVE ğŸš¨')['input_ids']]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/8df6d0529877ce10fd9d7bf7e97580e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*Q5I7X4v9lu7eAFlo.png"/></div></figure><p id="4d37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ä¸‹ä¸€æ­¥æ˜¯å¯¹æ•°æ®è¿›è¡Œæ ‡è®°ã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="216f" class="ma mb it lw b gy mc md l me mf">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True)</span><span id="9525" class="ma mb it lw b gy mg md l me mf">tokenized_datasets = dataset.map(tokenize_function, batched=True)</span></pre><h1 id="a9c3" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">å¾®è°ƒæ¨¡å‹</h1><p id="041a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">æ˜¯æ—¶å€™å¯¼å…¥é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹äº†ã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8130" class="ma mb it lw b gy mc md l me mf">model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=1)</span></pre><p id="6f4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">æ ¹æ®æ–‡æ¡£ï¼Œå¯¹äºå›å½’é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡<strong class="lb iu"> num_labels=1 </strong>ã€‚</p><p id="a6d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ä»¤ç‰ŒåµŒå…¥çš„å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬å‘ä»¤ç‰ŒåŒ–å™¨æ·»åŠ äº†æ›´å¤šçš„ä»¤ç‰Œã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="02e5" class="ma mb it lw b gy mc md l me mf">model.resize_token_embeddings(len(tokenizer))</span></pre><h1 id="af31" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">åº¦é‡å‡½æ•°</h1><p id="7783" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">åœ¨å›å½’é—®é¢˜ä¸­ï¼Œæ‚¨è¯•å›¾é¢„æµ‹ä¸€ä¸ªè¿ç»­å€¼ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦åº¦é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´è·ç¦»çš„æŒ‡æ ‡ã€‚æœ€å¸¸è§çš„æŒ‡æ ‡æ˜¯MSE(å‡æ–¹è¯¯å·®)å’ŒRMSE(å‡æ–¹æ ¹è¯¯å·®)ã€‚å¯¹äºè¿™ä¸ªåº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨RMSEï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°åœ¨è®­ç»ƒæ•°æ®æ—¶ä½¿ç”¨å®ƒã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2686" class="ma mb it lw b gy mc md l me mf">from datasets import load_metric<br/></span><span id="726c" class="ma mb it lw b gy mg md l me mf">def compute_metrics(eval_pred):<br/>    predictions, labels = eval_pred<br/>    rmse = mean_squared_error(labels, predictions, squared=False)<br/>    return {"rmse": rmse}</span></pre><h1 id="9ec4" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">è®­ç»ƒæ¨¡å‹</h1><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4fb9" class="ma mb it lw b gy mc md l me mf">from transformers import TrainingArguments, Trainer</span><span id="515f" class="ma mb it lw b gy mg md l me mf">training_args = TrainingArguments(output_dir="test_trainer",<br/>                                  logging_strategy="epoch",<br/>                                  evaluation_strategy="epoch",<br/>                                  per_device_train_batch_size=16,<br/>                                  per_device_eval_batch_size=16,<br/>                                  num_train_epochs=3,<br/>                                  save_total_limit = 2,<br/>                                  save_strategy = 'no',<br/>                                  load_best_model_at_end=False<br/>                                  )<br/></span><span id="9682" class="ma mb it lw b gy mg md l me mf">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=tokenized_datasets["train"],<br/>    eval_dataset=tokenized_datasets["test"],<br/>    compute_metrics=compute_metrics<br/>)<br/>trainer.train()</span></pre><h1 id="d62b" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">ä¿å­˜å¹¶åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å’Œæ ‡è®°å™¨</h1><p id="429a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">è¦ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="481c" class="ma mb it lw b gy mc md l me mf"># save the model/tokenizer</span><span id="24c3" class="ma mb it lw b gy mg md l me mf">model.save_pretrained("model")<br/>tokenizer.save_pretrained("tokenizer")</span><span id="64ca" class="ma mb it lw b gy mg md l me mf"># load the model/tokenizer</span><span id="4223" class="ma mb it lw b gy mg md l me mf">from transformers import AutoModelForTokenClassification<br/>model = AutoModelForSequenceClassification.from_pretrained("model")<br/>tokenizer = AutoTokenizer.from_pretrained("tokenizer")</span></pre><h1 id="da5e" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">å¦‚ä½•ä½¿ç”¨æ¨¡å‹</h1><p id="8ac4" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">ä¸€æ—¦æˆ‘ä»¬åŠ è½½äº†è®°å·èµ‹äºˆå™¨å’Œæ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨Transformerçš„<strong class="lb iu">è®­ç»ƒå™¨</strong>ä»æ–‡æœ¬è¾“å…¥ä¸­è·å¾—é¢„æµ‹ã€‚æˆ‘åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†æ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶è¿”å›é¢„æµ‹ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ­¥éª¤å¦‚ä¸‹:</p><ol class=""><li id="72f5" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">å°†æ•°æ®å¸§ä¸­çš„æ–‡æœ¬æ·»åŠ åˆ°åä¸ºtextçš„åˆ—ä¸­ã€‚</li><li id="ffb0" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">å°†æ•°æ®å¸§è½¬æ¢ä¸ºæ•°æ®é›†ã€‚</li><li id="4ae5" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">å°†æ•°æ®é›†æ ‡è®°åŒ–ã€‚</li><li id="454c" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">ä½¿ç”¨åŸ¹è®­å¸ˆè¿›è¡Œé¢„æµ‹ã€‚</li></ol><p id="bfe8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">å½“ç„¶ï¼Œä½ å¯ä»¥ä¸ç”¨ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†å¤šä¸ªè¾“å…¥ã€‚è¿™æ ·ï¼Œå®ƒä¼šæ›´å¿«ï¼Œå› ä¸ºå®ƒä½¿ç”¨æ‰¹æ¬¡åšé¢„æµ‹ã€‚</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a55b" class="ma mb it lw b gy mc md l me mf">from transformers import Trainer<br/>trainer = Trainer(model=model)</span><span id="73b2" class="ma mb it lw b gy mg md l me mf">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True) </span><span id="a906" class="ma mb it lw b gy mg md l me mf">def pipeline_prediction(text):<br/>    df=pd.DataFrame({'text':[text]})<br/>    dataset = Dataset.from_pandas(df,preserve_index=False) <br/>    tokenized_datasets = dataset.map(tokenize_function)<br/>    raw_pred, _, _ = trainer.predict(tokenized_datasets) <br/>    return(raw_pred[0][0])</span><span id="127e" class="ma mb it lw b gy mg md l me mf">pipeline_prediction("ğŸš¨ Get 50% now!")</span><span id="a6d7" class="ma mb it lw b gy mg md l me mf">-0.019468416</span></pre><h1 id="e0ee" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">æ€»ç»“ä¸€ä¸‹</h1><p id="a338" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ¥è§£å†³å›å½’é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨Huggingfaceçš„transformersåº“æ¥åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹DistilBERTï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„æ•°æ®å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚æˆ‘è®¤ä¸ºtransformeræ¨¡å‹éå¸¸å¼ºå¤§ï¼Œå¦‚æœä½¿ç”¨å¾—å½“ï¼Œå¯ä»¥æ¯”word2vecå’ŒTF-IDFç­‰æ›´ç»å…¸çš„å•è¯åµŒå…¥æ–¹æ³•äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="1845" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">æƒ³ä»æˆ‘è¿™é‡Œå¾—åˆ°æ›´å¤šï¼Ÿ:<br/></strong><a class="ae ky" href="https://medium.com/@billybonaros" rel="noopener">åœ¨åª’ä½“<br/> </a>ä¸Šå…³æ³¨æˆ‘åœ¨ <br/>ä¸­é“¾æ¥çš„<a class="ae ky" href="https://www.linkedin.com/in/billybonaros/" rel="noopener ugc nofollow" target="_blank">ä¸Šæ·»åŠ æˆ‘é€šè¿‡ä½¿ç”¨</a><a class="ae ky" href="https://billybonaros.medium.com/membership" rel="noopener"> <strong class="lb iu">æˆ‘çš„æ¨èé“¾æ¥</strong> </a>æ³¨å†Œåª’ä½“æ¥æ”¯æŒæˆ‘ã€‚</p></div></div>    
</body>
</html>
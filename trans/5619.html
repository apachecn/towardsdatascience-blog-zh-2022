<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ç”¨ Python å®ç°å¸æ³•åˆ¤å†³çš„è‡ªç„¶è¯­è¨€å¤„ç†</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-9462c15c2a64#2022-12-19">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-9462c15c2a64#2022-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="ir is gp gr it iu gh gi paragraph-image"><div class="ab gu cl iv"><img src="../Images/a42e9b4e9510f434241ac07a2aabc2d6.png" data-original-src="https://miro.medium.com/v2/format:webp/0*g0HJqbQTtFkPy4nh.jpg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated"><a class="ae jc" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="4647" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">ç¬¬ 10 éƒ¨åˆ†:é¢„æµ‹</h2></div><p id="8759" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘ä»¬åˆ°äº†è¿™ä¸€ç³»åˆ—æ–‡ç« çš„æœ€åä¸€éƒ¨åˆ†ï¼Œåœ¨è¿™é‡Œæˆ‘å°†ä½¿ç”¨ ML æ¨¡å‹å¯¹å¸æ³•åˆ¤å†³è¿›è¡Œåˆ†ç±»ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†ä½¿ç”¨å·²ç»æ ‡è®°çš„è®°å½•ä½œä¸ºè®­ç»ƒé›†æ¥æ‰§è¡Œç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä»¥ä¾¿å°†æ¨¡å‹åº”ç”¨äºé‚£äº›æ²¡æœ‰æ ‡è®°çš„è®°å½•ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥æå¤§åœ°å¸®åŠ©åœ¨è®°å½•åˆ°è¾¾å®˜å‘˜æ‰‹ä¸­æ—¶è‡ªåŠ¨å¯¹è®°å½•è¿›è¡Œåˆ†ç±»ã€‚</p><p id="390c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘å°†æ·±å…¥æ¢ç©¶æŠ€æœ¯:</p><ul class=""><li id="f07c" class="lq lr jf kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">æœ€é¢‘ç¹åŸºçº¿</li><li id="d805" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">é€»è¾‘å›å½’</li><li id="761e" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">æ”¯æŒå‘é‡åˆ†ç±»å™¨</li><li id="938c" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">å¸¦æœ‰ Keras çš„æ·±åº¦ç¥ç»ç½‘ç»œ</li></ul><p id="bf64" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„åº“:</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="ac38" class="mn mo jf mj b be mp mq l mr ms"># Libraries we will use in this section<br/>from sklearn.metrics import precision_score, classification_report, f1_score<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.feature_selection import SelectKBest, chi2<br/><br/>from sklearn.multiclass import OneVsRestClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import LinearSVC<br/><br/>from scipy.sparse import random<br/><br/>import sklearn as skl<br/>skl.warnings.filterwarnings("ignore")</span></pre><p id="713e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä¸ºäº†æœ‰ä¸€ä¸ªåŸºå‡†æ¥æ¯”è¾ƒæˆ‘ä»¬çš„æ¨¡å‹ç»“æœï¼Œæˆ‘å°†é¦–å…ˆä½¿ç”¨æœ€é¢‘ç¹çš„åŸºçº¿ä½œä¸ºæ‰€æœ‰æ–‡æ¡£çš„é¢„æµ‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰äº†ä¸ä¹‹ç›¸å…³çš„æ€§èƒ½æŒ‡æ ‡ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹çš„æ€§èƒ½æ¯”æœ€é¢‘ç¹çš„åŸºçº¿å·®ï¼Œå®ƒå°±ä¸å€¼å¾—ã€‚</p><h2 id="b9dd" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">æœ€é¢‘ç¹æ ‡ç­¾åŸºçº¿</h2><p id="f062" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">åœ¨å°†<em class="nq"> df_factor </em>åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹å‰ï¼Œæˆ‘å°†åˆ é™¤æ²¡æœ‰ä¸»é¢˜çš„è¡Œã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†åˆ›å»ºä¸€ä¸ªé®ç½©å¹¶å°†å…¶åº”ç”¨äº dfã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="9ede" class="mn mo jf mj b be mp mq l mr ms">#I will split first the dataset into a test set (20%) and a temporary set (80%). <br/>#Then, I will split the latter into train set (80% of temporary set) <br/>#and development set (20% of temporary set)<br/><br/>from sklearn.model_selection import train_test_split<br/><br/>tmp, test = train_test_split(df_factor_label, test_size=0.2, random_state=123) #for replicability<br/>train, dev = train_test_split(tmp, test_size=0.2, random_state=123) #for replicability<br/><br/>vectorizer_logit = TfidfVectorizer(ngram_range = (2,6), min_df = 0.001, max_df = 0.75, stop_words = 'english')<br/><br/>X_train = vectorizer_logit.fit_transform(train.Lemmas)<br/><br/>#we cannot refit the vectorizer<br/>X_dev = vectorizer_logit.transform(dev.Lemmas)<br/>X_test = vectorizer_logit.transform(test.Lemmas)<br/><br/><br/>y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_dev = dev.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)</span></pre><p id="b6ea" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä»æè¿°æ€§ç»Ÿè®¡éƒ¨åˆ†ï¼Œæˆ‘ä»¬çŸ¥é“æœ€å¸¸è§çš„ç±»åˆ«æ˜¯â€œç¨â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åªé¢„æµ‹æ‰€æœ‰è§‚æµ‹å€¼çš„ç¨æ”¶ä½œä¸ºåŸºçº¿æ¨¡å‹ã€‚è®©æˆ‘ä»¬æ£€ç´¢è¯¥ç±»åˆ«çš„ç´¢å¼•ã€‚ä½œä¸ºä¸€ä¸ªå‚è€ƒæŒ‡æ ‡ï¼Œæˆ‘å°†ä¸»è¦ä¾é å¾®è§‚å¹³å‡ f1 åˆ†æ•°ï¼Œå…¶ä¸­å¾®è§‚åœç•™åœ¨â€œè®¡ç®—æ€»çš„çœŸé˜³æ€§ã€å‡é˜´æ€§å’Œå‡é˜³æ€§ã€‚â€äº‹å®ä¸Šï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘å®è§‚å¹³å‡æŒ‡æ ‡ï¼Œæˆ‘ä»¬ä¸ä¼šè€ƒè™‘æ•°æ®ä¸å¹³è¡¡çš„äº‹å®ã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="722a" class="mn mo jf mj b be mp mq l mr ms">ind = y_train.columns.get_loc("Tax")<br/>#print(ind)<br/><br/>most_frequent = np.zeros(42)<br/>most_frequent[ind] = 1<br/>#print(most_frequent)<br/><br/>most_frequent_prediction = [most_frequent for i in range(y_dev.shape[0])]<br/>most_frequent_prediction = pd.DataFrame(most_frequent_prediction, columns = y_dev.columns)<br/><br/>print(classification_report(y_dev, most_frequent_prediction, target_names =  y_dev.columns))</span></pre><h2 id="2a4c" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">é€»è¾‘å›å½’åŸºçº¿</h2><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="9c20" class="mn mo jf mj b be mp mq l mr ms">logit = OneVsRestClassifier(LogisticRegression())<br/>logit.fit(X_train, y_train)</span></pre><p id="6db2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">è®©æˆ‘ä»¬å°†äº¤å‰éªŒè¯åº”ç”¨åˆ°åŸ¹è®­ä¸­(å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºäº¤å‰éªŒè¯çš„çŸ¥è¯†ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘ä»¥å‰çš„æ–‡ç« <a class="ae jc" href="https://medium.com/dataseries/cross-validation-for-model-selection-5e843c71553d" rel="noopener"/>):</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="0e5e" class="mn mo jf mj b be mp mq l mr ms"># estimating the (test) F1 score via cross validation<br/>for k in [2, 5, 10]:<br/>    cv = cross_val_score(OneVsRestClassifier(LogisticRegression()), X_train, y = y_train, cv = k, n_jobs = -1, scoring = "f1_micro")<br/>    fold_size = X_train.shape[0]/k<br/>    <br/>    print("F1 with {} folds for bag-of-words is {}".format(k, cv.mean()))<br/>    print("Training on {} instances/fold, testing on {}".format(round(fold_size*(k-1)), round(fold_size)))<br/>    print()</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/af67d31ac69ff9fe6b9fdcb34101729d.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*mzSwlM2cXZicsxRFnAXhCQ.png"/></div></figure><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="3bba" class="mn mo jf mj b be mp mq l mr ms">y_pred_logit_baseline = logit.predict(X_dev)<br/>print(classification_report(y_dev, y_pred_logit_baseline, target_names = y_dev.columns))</span></pre><p id="9884" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">å°½ç®¡â€œå¹¼ç¨šâ€,è¿™ä¸ªæ¨¡å‹åœ¨æœ€é¢‘ç¹åŸºçº¿çš„å¼€å‘é›†ä¸­è¡¨ç°å¾—æ›´å¥½ã€‚æˆ‘ä»¬çš„å¾®è§‚ F1 å¾—åˆ†ä¸º 54%,ä½äº 16%ã€‚</p><p id="a65d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬å†³å®šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚å®ƒåœ¨æµ‹è¯•æ•°æ®ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿ</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="eea4" class="mn mo jf mj b be mp mq l mr ms">y_pred_logit_baseline_test = logit.predict(X_test)<br/>print(classification_report(y_test, y_pred_logit_baseline_test, target_names = y_test.columns))</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/acf5e0d4f00888f9e08a560d7044e253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*P1O93U1glEb25R5NoZxzig.png"/></div></figure><p id="d491" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg"> <em class="nq">æ­£è§„åŒ–å¼ºåº¦</em> </strong></p><p id="57ac" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ç°åœ¨æˆ‘å°†æ”¹è¿›æˆ‘çš„é€»è¾‘æ¨¡å‹ã€‚</p><p id="14ee" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æ¯å½“æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éƒ½å¿…é¡»è€ƒè™‘æ–¹å·®åå·®çš„æƒè¡¡å’Œè¿‡åº¦æ‹Ÿåˆçš„é£é™©:äº‹å®ä¸Šï¼Œå¢åŠ å‚æ•°çš„æ•°é‡æ€»æ˜¯ä¼šå¯¼è‡´è®­ç»ƒè¯¯å·®çš„å‡å°‘ï¼Œä½†ä¸ä¼šå¯¼è‡´æµ‹è¯•è¯¯å·®çš„å‡å°‘ï¼Œå› ä¸º(å¹³æ–¹)åå·®ä¼šå‡å°‘ï¼Œä½†æ–¹å·®ä¼šå¢åŠ ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å½“æˆ‘ä»¬å¢åŠ å‚æ•°æ•°é‡æ—¶å¢åŠ æŸå¤±å‡½æ•°çš„æƒ©ç½šé¡¹ã€‚åœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚æ•° cã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œé™ä½ c å°†åŠ å¼ºğœ†è°ƒèŠ‚å™¨ã€‚çš„ç¡®ï¼Œc å¯¹ 1/ğœ†.æ¥è¯´æ˜¯å¥‡æ€ªçš„å› æ­¤ï¼Œğœ†è¶Šé«˜ï¼Œæƒ©ç½šé¡¹è¶Šé«˜ï¼Œæ¨¡å‹çš„å‚æ•°å°±è¶Šå°‘ã€‚</p><p id="741d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æ³¨:æƒ³äº†è§£æ›´å¤šå…³äºæ­£è§„åŒ–çš„å†…å®¹ï¼Œå¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘ä¹‹å‰çš„æ–‡ç« <a class="ae jc" href="https://medium.com/dataseries/preventing-overfitting-regularization-5eda7d5753bc" rel="noopener">ã€‚</a></p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="69c1" class="mn mo jf mj b be mp mq l mr ms">from sklearn.metrics import f1_score<br/>best_c = None<br/>best_performance = 0.0<br/><br/>for c in [20, 10, 5, 2, 0.5, 0.1, 0.05, 0.01]:<br/>    print(c)<br/>    classifier_c = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', <br/>                                             class_weight='balanced',<br/>                                             C=c<br/>                                     ))<br/>    classifier_c.fit(X_train, y_train)<br/>    predictions_c = classifier_c.predict(X_dev)<br/>    score = f1_score(y_dev, predictions_c, average='micro')<br/>    if score &gt; best_performance:<br/>        best_performance = score<br/>        best_c = c<br/>        print("New best performance: {}".format(score))<br/>        <br/>    #print(classification_report(y_dev, predictions_c, target_names = y_dev.columns))<br/>    </span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1c44ed91cdf60b911f10d4746a1e35f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*Wtt7_012zm3So6QIajmB3A.png"/></div></figure><p id="1883" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä»ä¸Šé¢çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œæœ€å¥½çš„æ¨¡å‹è¾“å‡ºç­‰äº 81%çš„å¾® f1 åˆ†æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°:</p><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/88ae29b2f2a99a78bfcdd129dc65522b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*3Nd3kfVMSTfJ4qDgP82H4A.png"/></div></figure><p id="b51f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg"> <em class="nq">åŠŸèƒ½é€‰æ‹©</em> </strong></p><p id="a8c0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">è®©æˆ‘ä»¬æŠŠç‰¹å¾çš„æ•°é‡å‡å°‘åˆ° 4500 ä¸ªã€‚æˆ‘ä¼šæŠŠè¿™ä¸ªæ¨¡å‹å’Œä¹‹å‰ c çš„æœ€ä½³å€¼çš„ç»“æœç»“åˆèµ·æ¥ã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="7527" class="mn mo jf mj b be mp mq l mr ms">from sklearn.feature_selection import SelectKBest, chi2<br/><br/>selector = SelectKBest(chi2, k=4500).fit(X_train, y_train)<br/><br/>X_train_sel = selector.transform(X_train)<br/>X_dev_sel = selector.transform(X_dev)<br/>X_test_sel = selector.transform(X_test)<br/><br/>classifier_sel = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', <br/>                                    class_weight='balanced', C = best_c))<br/>classifier_sel.fit(X_train_sel, y_train)<br/><br/>predictions_sel = classifier_sel.predict(X_dev_sel)<br/>print(classification_report(y_dev, predictions_sel, target_names = y_dev.columns))</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4965c03d1e4715eb0d09d9a470eef668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*KFOyqX4-33teeetglSOIVg.png"/></div></figure><p id="8424" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">çœ‹æ¥ç‰¹å¾é€‰æ‹©å¹¶æ²¡æœ‰å¸¦æ¥æå‡(ç°åœ¨å¾® f1 è¯„åˆ†æ›´ä½ï¼Œ72%)ã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="4151" class="mn mo jf mj b be mp mq l mr ms">#let's also evaluate the model in the test set<br/>predictions_sel_test = classifier_sel.predict(X_test_sel)<br/>print(classification_report(y_test, predictions_sel_test, target_names = y_test.columns))</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/affcff6b6828ded145515b3ddc8c1adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*5RTd3W49oe1ZAN2qWZsXgA.png"/></div></figure><p id="ec93" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg"> <em class="nq">é™ç»´</em> </strong></p><p id="76f2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸åœ¨ç‰¹å¾ä¸­è¿›è¡Œé€‰æ‹©ï¼Œè€Œæ˜¯åˆ›å»ºæ–°çš„ä½ç»´ç‰¹å¾ï¼Œä½œä¸ºåŸå§‹ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="8834" class="mn mo jf mj b be mp mq l mr ms">from sklearn.decomposition import TruncatedSVD<br/><br/>best_performance=0.0<br/>best_k = None<br/><br/>for k in [300, 500, 1000, 1500,2000, 2500]:<br/>    print(k)<br/>    svd = TruncatedSVD(n_components=k)<br/><br/>    X_train_dim = svd.fit_transform(X_train_sel)<br/>    X_dev_dim = svd.transform(X_dev_sel)<br/>    X_test_dim = svd.transform(X_test_sel)<br/><br/>    classifier_dim = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', <br/>                                        class_weight='balanced', C = best_c)) #still including the parameter c<br/>    classifier_dim.fit(X_train_dim, y_train)<br/>    predictions_dim = classifier_dim.predict(X_dev_dim)<br/>    score = f1_score(y_dev, predictions_dim, average='micro')<br/>    if score &gt; best_performance:<br/>        best_performance = score<br/>        best_k = k<br/>        print("New best performance: {}".format(score))<br/><br/>    #print(classification_report(y_dev, predictions_dim, target_names = y_dev.columns))<br/>    print()</span></pre><p id="0e2b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">å®ƒå¯¼è‡´æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ç­‰äº 72%ã€‚</p><h2 id="b3da" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">æ”¯æŒå‘é‡åˆ†ç±»å™¨</h2><p id="d640" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">å¯¹äºæˆ‘çš„ç¬¬äºŒä¸ªåˆ†ç±»æ¨¡å‹ï¼Œæˆ‘å†³å®šä½¿ç”¨ä¸€ä¸ªæ”¯æŒå‘é‡åˆ†ç±»å™¨(ä¸€å¯¹ä¸€å¯¹å…¨éƒ¨)ï¼Œåƒä»¥å‰ä¸€æ ·ä½¿ç”¨è°ƒä¼˜å‚æ•° Cã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="953a" class="mn mo jf mj b be mp mq l mr ms">svc_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df = 0.25, stop_words = 'english')<br/><br/>tmp, test = train_test_split(df_factor_label, test_size=0.2, random_state=123) #for replicability<br/>train, dev = train_test_split(tmp, test_size=0.2, random_state=123) #for replicability<br/><br/>X_train_svc = svc_vectorizer.fit_transform(train.text)<br/>X_dev_svc = svc_vectorizer.transform(dev.text)<br/>X_test_svc = svc_vectorizer.transform(test.text)<br/><br/>y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_dev = dev.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/><br/>best_c = None #same ratio as above<br/>best_f1_score = 0.0<br/>for c in [5, 1.0, 0.5]:<br/>    print(c)<br/>    svc_clf = OneVsRestClassifier(LinearSVC(C = c, class_weight = 'balanced')).fit(X_train_svc, y_train)<br/>    cv_reg = cross_val_score(svc_clf, X_train_svc, y = y_train, cv = 5, n_jobs = -1, scoring = "f1_micro")<br/><br/>    new_predictions_regularized = svc_clf.predict(X_dev_svc)<br/>    f1 = f1_score(y_dev, new_predictions_regularized, average='micro')<br/>    print("5-CV on train at C={}: {}".format(c, cv_reg.mean()))<br/>    print(classification_report(y_dev, new_predictions_regularized, target_names = y_dev.columns))<br/>    print()    <br/>    if f1 &gt; best_f1_score:<br/>        best_f1_score = f1<br/>        best_c = c<br/>        #print("New best performance: {}".format(f1))</span></pre><p id="3306" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€ä½³æ€§èƒ½æ˜¯ C=1 çš„é‚£ä¸ªï¼Œå¾® f1=0.84ã€‚</p><p id="9387" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">å› æ­¤ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æ‰€æœ‰ä¸Šè¿°æ¨¡å‹çš„æŒ‡æ ‡(åœ¨å¼€å‘é›†ä¸­):</p><figure class="me mf mg mh gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/af4984cf13c9c57166dbec4348ac2146.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*1FJAR1ziyQD8u5fGfO6FVQ.png"/></div></div></figure><p id="c1fe" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">å› æ­¤ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œæˆ‘å‡è®¾æœ€å¥½çš„æ˜¯ SVC:è¿™æ˜¯æˆ‘è¦ä¸ä¸¤ä¸ªåŸºçº¿è¿›è¡Œæ¯”è¾ƒçš„æ¨¡å‹ã€‚ä½†æ˜¯åœ¨è¿›å…¥å¼•å¯¼éƒ¨åˆ†ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="7be6" class="mn mo jf mj b be mp mq l mr ms">lsvc = OneVsRestClassifier(LinearSVC(C = best_c, class_weight = "balanced"))<br/>lsvc.fit(X_train_svc, y_train)<br/><br/>y_pred_svc = lsvc.predict(X_test_svc)<br/>print(classification_report(y_test, y_pred_svc, target_names = y_test.columns))<br/><br/>#finally, let's store the dev predictions to be used in the next section.<br/>best_preds = lsvc.predict(X_dev_svc)</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e1ee306f2b5916ae1d1af183f678b7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*u0e44y1AhxKNPvN88j4Mog.png"/></div></figure><h2 id="a784" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">Keras ç¥ç»ç½‘ç»œ</h2><p id="9ac7" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">Keras æ˜¯ä¸€ä¸ªå¼€æºè½¯ä»¶åº“ï¼Œä¸º ann(äººå·¥ç¥ç»ç½‘ç»œ)æä¾›äº† Python æ¥å£ã€‚å®ƒå¯ä»¥è¿è¡Œåœ¨ TensorFlowã€å¾®è½¯è®¤çŸ¥å·¥å…·åŒ…ã€Theano æˆ– PlaidML ä¹‹ä¸Šã€‚</p><p id="2205" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Keras çš„å¼€å‘æ˜¯ä¸ºäº†è®©ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æ›´å®¹æ˜“åŸå‹åŒ–ã€æ„å»ºå’Œå®éªŒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®ƒå…·æœ‰ç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œå…è®¸æ‚¨è½»æ¾åˆ›å»ºå’Œè®­ç»ƒå„ç§ç±»å‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€é€’å½’ç¥ç»ç½‘ç»œ(RNNs)å’Œé•¿çŸ­æœŸè®°å¿†(LSTM)ç½‘ç»œã€‚</p><p id="5ecf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Keras çš„è®¾è®¡æ˜¯çµæ´»å’Œæ¨¡å—åŒ–çš„ï¼Œå› æ­¤æ‚¨å¯ä»¥è½»æ¾åœ°é…ç½®ã€ç»„åˆå’Œå¾®è°ƒæ‚¨åˆ›å»ºçš„æ¨¡å‹ã€‚å®ƒè¿˜å…·æœ‰å„ç§é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºä¸€ç³»åˆ—ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚</p><p id="a7a4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä¸ºæ­¤ï¼Œæˆ‘å°†ä½¿ç”¨ Keras æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¥é¢„æµ‹ä¸å¸æ³•åˆ¤å†³ç›¸å…³çš„æ ‡ç­¾ã€‚</p><p id="a45a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†åªè€ƒè™‘é‚£äº›å…·æœ‰å•ä¸ªæ ‡ç­¾çš„è®°å½•ï¼Œè¿™æ ·ä»»åŠ¡å°†å½’ç»“ä¸ºä¸€ä¸ªå¤šç±»ä»»åŠ¡ï¼Œè€Œä¸æ˜¯å¤šç±»ã€å¤šæ ‡ç­¾ä»»åŠ¡ã€‚</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="21bb" class="mn mo jf mj b be mp mq l mr ms">import tensorflow as tf<br/>from keras.models import Sequential<br/>from keras.layers import InputLayer, Input<br/>from keras.layers import Reshape, MaxPooling2D<br/>from keras.layers import Conv2D, Dense, Flatten, Activation, Dropout<br/>from keras.optimizers import SGD, RMSprop, Adagrad #want to try different optimizers<br/><br/>#downloading df<br/>df_factor = pd.read_pickle('data/df_factor.pkl')<br/><br/><br/>#creating a df containing only records with one label<br/>m=[len(df_factor.category[i])==1 for i in range(len(df_factor))]<br/>df_factor_single_label=df_factor[m]<br/><br/><br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>train, test = train_test_split(df_factor_single_label, test_size=0.2, random_state = 123) #here, we don't need a dev set<br/>                                                                                        #since it can be specified direclty<br/>                                                                                        #during training<br/>#for this purpose, I will use the TfIdf vecotrizer.<br/><br/>vectorizer_nn = TfidfVectorizer(ngram_range = (1, 2), min_df = 0.001, max_df = 0.25, stop_words = 'english')<br/><br/>#let's also store the full dataset into a X_nn variable, so that we will be able to plot the training history.<br/><br/>X_nn = vectorizer_nn.fit_transform(df_factor_single_label.text)<br/>X_train_nn = vectorizer_nn.fit_transform(train.text)<br/>X_test_nn = vectorizer_nn.transform(test.text)<br/><br/>y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y = df_factor_single_label.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)</span></pre><p id="2e28" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ç°åœ¨è®©æˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹:</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="1c94" class="mn mo jf mj b be mp mq l mr ms">model = Sequential()<br/>model.add(Dense(3000, activation='relu', input_dim = X_train_nn.shape[1]))<br/>model.add(Dropout(0.1))<br/>model.add(Dense(600, activation='relu'))<br/>model.add(Dropout(0.1))<br/>model.add(Dense(200, activation='relu'))<br/>model.add(Dense(y_train.shape[1], activation='softmax'))<br/><br/>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)<br/>#rms = RMSprop(learning_rate=0.001, rho=0.9)<br/>#adam = Adagrad(learning_rate=0.01)<br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])<br/><br/>history = model.fit(X_train_nn, y_train, epochs = 5, batch_size = 100, verbose = 1, validation_split=0.2)<br/><br/>#score = model.evaluate(X_test_nn, y_test, batch_size = 100)<br/>#score</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi od"><img src="../Images/353e20bcfb4d79b235ed698622985376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rsbWlp-pexRSKqLgb-tm0Q.png"/></div></div></figure><p id="4eb2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªéªŒè¯/å¼€å‘å‡†ç¡®ç‡ä¸º 91.23%çš„æ¨¡å‹ï¼Œè¿˜ä¸é”™ï¼</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="9d74" class="mn mo jf mj b be mp mq l mr ms">from keras.models import load_model<br/><br/>model.save('Keras Models/NN_labels.h5')  # creates a HDF5 file 'NN_labels.h5'<br/><br/>%matplotlib inline<br/>import pandas as pd<br/>import seaborn<br/><br/>df = pd.DataFrame(history.history)<br/>df[['val_accuracy', 'accuracy']].plot.line()<br/>df[['val_loss', 'loss']].plot.line()</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/56f0d5971583d872fd678fd8ff0ae6b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*4gRIk5qaRJWsRjdg"/></div></figure><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c53c1141454977c9c9299882bfdf6b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*2-WqQTQTzOnyj32D"/></div></figure><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="7bfc" class="mn mo jf mj b be mp mq l mr ms">#downloading model<br/>from keras.models import load_model<br/><br/>model = load_model('Keras Models/NN_labels.h5')<br/><br/>#using it to predict on new, never-seen-before data.<br/><br/>loss, accuracy = model.evaluate(X_test_nn, y_test, batch_size = 100)<br/><br/>print("test loss: ", loss)<br/>print("test accuracy: ", accuracy)</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi og"><img src="../Images/47769b24b32f83087e8a220bdf180210.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*6R9EEWzOD_-AgFw2PO65MQ.png"/></div></figure><p id="e63f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æµ‹è¯•ç²¾åº¦ç›¸å½“ä»¤äººæ»¡æ„(91.3%)ï¼Œå› æ­¤æˆ‘ç›¸ä¿¡è¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åœ°æ ‡è®°æ–°æ–‡ç« ã€‚</p><p id="8a4e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æœ‰äº†è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘æœ‰ä¿¡å¿ƒåœ¨æ–°æ–‡ç« è¢«æ’å…¥åˆ°æ•°æ®åº“ä¸­æ—¶è‡ªåŠ¨æ ‡è®°å®ƒä»¬ã€‚è¿™å¯èƒ½æ˜¯å®ç°èƒ½å¤Ÿè§£å†³åŸå§‹ä»»åŠ¡(å³å¤šç±»ã€å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜)çš„è§£å†³æ–¹æ¡ˆçš„èµ·ç‚¹ã€‚è¿™ä¸ªæ¨¡å‹çš„ä¸€ä¸ªå¯èƒ½çš„æ”¹è¿›æ˜¯:è®­ç»ƒä¸€ç»„ä¸åŒçš„åˆ†ç±»å™¨(å¯ä»¥æ˜¯å…·æœ‰ä¸åŒç»“æ„çš„ç¥ç»ç½‘ç»œï¼Œæˆ–è€…ä¸åŒçš„æ¨¡å‹ï¼Œå¦‚é€»è¾‘å›å½’ã€SVC ç­‰ç­‰)ã€‚).ç„¶åä½¿ç”¨å®ƒä»¬æ¥é¢„æµ‹æ–‡ç« çš„æµ‹è¯•åºåˆ—ï¼Œå¹¶ä¸”æ¯å½“æ¨¡å‹è¾“å‡ºä¸åŒçš„æ ‡ç­¾æ—¶ï¼Œå°†å®ƒä»¬éƒ½å½’å› äºè¯¥è§‚å¯Ÿ/æ–‡æœ¬ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯ä»¥ä½¿ç”¨é€»è¾‘å›å½’ã€SVC å’Œ NN æ¥é¢„æµ‹æ–°æ–‡ç« æ ‡ç­¾ï¼Œå¦‚æœå®ƒä»¬éƒ½è¿”å›ä¸åŒçš„ç»“æœï¼Œåˆ™å°†æ‰€æœ‰ä¸‰ä¸ªæ ‡ç­¾å½’å±äºè¯¥æ–‡ç« ã€‚</p><h2 id="e57b" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">ç»“è®º</h2><p id="5b5a" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">æˆ‘ä»¬åˆ°äº†å…³äºå¸æ³•åˆ¤å†³çš„ NLP ç³»åˆ—æ–‡ç« çš„æœ€åä¸€éƒ¨åˆ†ã€‚å¦‚æœå°½ç®¡æˆ‘å†™çš„ä¸œè¥¿å¾ˆæ— èŠï¼Œä½ è¿˜æ˜¯è®¾æ³•æ¥äº†ğŸ˜ƒè°¢è°¢å¤§å®¶ï¼è¿™å¯¹æˆ‘å¤ªé‡è¦äº†ã€‚</p><p id="2920" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">æˆ‘æ€»æ˜¯æ„Ÿè°¢ä»»ä½•å»ºè®¾æ€§çš„åé¦ˆï¼Œæ‰€ä»¥è¯·éšæ—¶é€šè¿‡ Medium æˆ– Linkedin è”ç³»æˆ‘ã€‚</p><p id="bbf4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ä¸‹ä¸€ç¯‡æ–‡ç« å†è§ï¼</p><h2 id="0253" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">å‚è€ƒ</h2><ul class=""><li id="cb7d" class="lq lr jf kw b kx nl la nm ld oh lh oi ll oj lp lv lw lx ly bi translated"><a class="ae jc" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::è‡ªç„¶è¯­è¨€å·¥å…·åŒ…</a></li><li id="aea8" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">Python ä¸­çš„ spaCy å·¥ä¸šçº§è‡ªç„¶è¯­è¨€å¤„ç†</a></li><li id="8579" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">å¸æ³•æ–°é—»| DOJ |å¸æ³•éƒ¨</a></li><li id="1ac4" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">å¸æ³•éƒ¨ 2009â€“2018 å¹´æ–°é—»å‘å¸ƒ| Kaggle </a></li><li id="4d3e" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://spacy.io/usage/linguistic-features#named-entities" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/linguistic-features#named-entities</a></li><li id="1fab" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://medium.com/p/d81bdfa14d97/edit" rel="noopener">https://medium.com/p/d81bdfa14d97/edit</a></li><li id="2a71" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">https://www.nltk.org/api/nltk.sentiment.vader.html<a class="ae jc" href="https://www.nltk.org/api/nltk.sentiment.vader.html" rel="noopener ugc nofollow" target="_blank"/></li></ul></div></div>    
</body>
</html>
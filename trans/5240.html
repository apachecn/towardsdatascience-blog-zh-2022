<html>
<head>
<title>Stable diffusion using Hugging Face â€” DiffEdit paper implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£â€” DiffEdit çº¸å¼ å®ç°</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-diffedit-paper-implementation-e4c99e3e320c#2022-11-23">https://towardsdatascience.com/stable-diffusion-using-hugging-face-diffedit-paper-implementation-e4c99e3e320c#2022-11-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3473" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DIFFEDIT çš„ä¸€ç§å®ç°:åŸºäºæ‰©æ•£çš„è¯­ä¹‰å›¾åƒç¼–è¾‘ï¼Œå…·æœ‰æ©æ¨¡å¼•å¯¼ğŸ¤—<a class="ae kl" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">æŠ±ç´§è„¸æ‰©æ•£å™¨åº“</a>ã€‚</p><p id="7735" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å®ç° Meta AI å’Œç´¢é‚¦å¤§å­¦çš„ç ”ç©¶äººå‘˜æœ€è¿‘å‘è¡¨çš„ä¸€ç¯‡åä¸º<code class="fe km kn ko kp b"><strong class="jp ir">DIFFEDIT</strong></code>çš„è®ºæ–‡ã€‚è¿™ç¯‡åšå®¢å¯¹äºé‚£äº›ç†Ÿæ‚‰ç¨³å®šæ‰©æ•£è¿‡ç¨‹æˆ–è€…æ­£åœ¨é˜…è¯»æˆ‘å†™çš„å…³äºç¨³å®šæ‰©æ•£çš„å¦å¤–ä¸¤ç¯‡åšå®¢çš„äººæ¥è¯´æ›´æœ‰æ„ä¹‰ã€‚<strong class="jp ir">ç¬¬ä¸€éƒ¨åˆ†</strong> - <a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-501d8dbdd8">ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£|ä½œè€… Aayush agr awal | 2022 å¹´ 11 æœˆ|èµ°å‘æ•°æ®ç§‘å­¦</a> <br/> 2 .<strong class="jp ir">ç¬¬ 2 éƒ¨åˆ†</strong> - <a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265">ä½¿ç”¨æ‹¥æŠ±è„¸çš„ç¨³å®šæ‰©æ•£-ç¨³å®šæ‰©æ•£çš„å˜åŒ–|ä½œè€… Aayush agr awal | 2022 å¹´ 11 æœˆ|èµ°å‘æ•°æ®ç§‘å­¦</a></p><p id="cf09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æœ€åˆï¼Œè¿™æ˜¯æˆ‘æƒ³å†™çš„åšæ–‡ï¼Œä½†æ˜¯æ„è¯†åˆ°æ²¡æœ‰ä¸€ä¸ªåœ°æ–¹å¯ä»¥ç†è§£ä»£ç çš„ç¨³å®šæ‰©æ•£ã€‚è¿™å°±æ˜¯æˆ‘æœ€ç»ˆåˆ›å»ºå…¶ä»–åšå®¢ä½œä¸ºå‚è€ƒæˆ–é¢„è¯»ææ–™æ¥ç†è§£æœ¬æ–‡çš„åŸå› ã€‚</p></div><div class="ab cl kq kr hu ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="ij ik il im in"><h1 id="469c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">ä»€ä¹ˆæ˜¯ DiffEditï¼Ÿ</h1><p id="ae97" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">ç®€å•åœ°è¯´ï¼Œä½ å¯ä»¥æŠŠ<code class="fe km kn ko kp b">DiffEdit</code>æ–¹æ³•çœ‹ä½œæ˜¯<code class="fe km kn ko kp b">Image to Image</code>ç®¡é“çš„ä¸€ä¸ªæ›´å—æ§åˆ¶çš„ç‰ˆæœ¬ã€‚<code class="fe km kn ko kp b">DiffEdit</code>æ¥å—ä¸‰ä¸ªè¾“å…¥- <br/> 1ã€‚ä¸€ä¸ªè¾“å…¥å›¾åƒ<br/> 2ã€‚<code class="fe km kn ko kp b">Caption</code> -æè¿°è¾“å…¥å›¾åƒ<br/> 3ã€‚<code class="fe km kn ko kp b">Target Query</code> -æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„æ–°å›¾åƒ</p><p id="b40e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¹¶ä¸”åŸºäºæŸ¥è¯¢æ–‡æœ¬äº§ç”ŸåŸå§‹å›¾åƒçš„ä¿®æ”¹ç‰ˆæœ¬ã€‚å¦‚æœæ‚¨æƒ³å¯¹å®é™…å›¾åƒç¨ä½œè°ƒæ•´è€Œä¸å®Œå…¨ä¿®æ”¹å®ƒï¼Œè¿™ä¸ªè¿‡ç¨‹ç‰¹åˆ«å¥½ã€‚</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/1bbdd6877209ee6fc64b4cb726e47506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*joQaY9Kc6hUNZF4n.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 1:å·®å¼‚ç¼–è¾‘æ¦‚è¿°ã€‚</p></figure><p id="2260" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä»ä¸Šé¢çš„å›¾ç‰‡ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåªæœ‰å›¾ç‰‡ä¸­çš„æ°´æœéƒ¨åˆ†è¢«æ›¿æ¢æˆäº†æ¢¨ã€‚ç›¸å½“æƒŠäººçš„ç»“æœï¼</p><p id="f9cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä½œè€…è§£é‡Šè¯´ï¼Œä»–ä»¬å®ç°è¿™ä¸€ç›®æ ‡çš„æ–¹å¼æ˜¯é€šè¿‡å¼•å…¥ä¸€ä¸ªé®ç½©ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ç¡®å®šå›¾åƒçš„å“ªä¸€éƒ¨åˆ†åº”è¯¥è¢«ç¼–è¾‘ï¼Œç„¶ååªå¯¹è¢«é®ç½©çš„éƒ¨åˆ†æ‰§è¡ŒåŸºäºæ–‡æœ¬çš„æ‰©æ•£è°ƒèŠ‚ã€‚</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/a3d03abacabe2e13067247629120e915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/0*Hnkz42QOGUB2Lpxj.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 2:æ¥è‡ªè®ºæ–‡<a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank"> DiffEdit </a>ã€‚ä¸€ç§é€šè¿‡æä¾›æ ‡é¢˜æ–‡æœ¬å’Œæ–°æ–‡æœ¬æ¥æ”¹å˜è¾“å…¥å›¾åƒçš„æ–¹æ³•ã€‚</p></figure><p id="59f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ä»ä¸Šé¢å–è‡ªè®ºæ–‡çš„å›¾åƒä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä½œè€…ä»è¾“å…¥å›¾åƒä¸­åˆ›å»ºäº†ä¸€ä¸ªé®ç½©ï¼Œå®ƒå¯ä»¥å‡†ç¡®åœ°ç¡®å®šå›¾åƒä¸­å­˜åœ¨æ°´æœçš„éƒ¨åˆ†ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªé®ç½©(ä»¥æ©™è‰²æ˜¾ç¤º),ç„¶åæ‰§è¡Œé®ç½©æ‰©æ•£ä»¥ç”¨æ¢¨æ›¿æ¢æ°´æœã€‚è¿›ä¸€æ­¥é˜…è¯»ï¼Œä½œè€…æä¾›äº†æ•´ä¸ª<code class="fe km kn ko kp b">DiffEdit</code>è¿‡ç¨‹çš„ä¸€ä¸ªå¾ˆå¥½çš„å¯è§†åŒ–è¡¨ç¤ºã€‚</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mr"><img src="../Images/8438306b3f4eba121c3ae5fced0370f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Da-MJK6Vq2GnZkdn.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ DiffEdit çš„ä¸‰ä¸ªæ­¥éª¤ã€‚<a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank">è®ºæ–‡</a></p></figure><p id="4496" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å½“æˆ‘é˜…è¯»è¿™ç¯‡è®ºæ–‡æ—¶ï¼Œä¼¼ä¹ç”Ÿæˆæ©è”½æ˜¯æœ€é‡è¦çš„æ­¥éª¤ï¼Œå‰©ä¸‹çš„åªæ˜¯ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹çš„æ–‡æœ¬æ¡ä»¶ã€‚ä½¿ç”¨è’™ç‰ˆå¯¹å›¾åƒè¿›è¡Œè°ƒèŠ‚çš„æƒ³æ³•ä¸<a class="ae kl" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py" rel="noopener ugc nofollow" target="_blank">æ‹¥æŠ±é¢éƒ¨ç”»ä¸­ç”»ç®¡é“</a>ä¸­å®ç°çš„æƒ³æ³•ç±»ä¼¼ã€‚æ­£å¦‚ä½œè€…ä»¬æ‰€å»ºè®®çš„ï¼Œâ€œè¿™ä¸ª<code class="fe km kn ko kp b">DiffEdit</code>è¿‡ç¨‹åˆ†ä¸‰æ­¥â€”â€”<br/><strong class="jp ir">ç¬¬ä¸€æ­¥:</strong>ç»™è¾“å…¥å›¾åƒåŠ å™ªå£°ï¼Œå»å™ª:ä¸€æ¬¡æ¡ä»¶åŒ–åœ¨æŸ¥è¯¢æ–‡æœ¬ä¸Šï¼Œä¸€æ¬¡æ¡ä»¶åŒ–åœ¨å‚è€ƒæ–‡æœ¬ä¸Š(æˆ–è€…æ— æ¡ä»¶)ã€‚æˆ‘ä»¬åŸºäºå»å™ªç»“æœçš„å·®å¼‚æ¥å¯¼å‡ºæ©æ¨¡ã€‚<br/></p><p id="8d61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹åœ¨å®é™…çš„ä»£ç ä¸­å®ç°è¿™äº›æƒ³æ³•ã€‚</p><p id="4b82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€éœ€çš„åº“å’ŒåŠ©æ‰‹å‡½æ•°å¼€å§‹ã€‚æ‰€æœ‰è¿™äº›éƒ½å·²ç»åœ¨ç¨³å®šæ‰©æ•£ç³»åˆ—çš„å‰<a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-501d8dbdd8">ç¬¬ 1 éƒ¨åˆ†</a>å’Œ<a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265">ç¬¬ 2 éƒ¨åˆ†</a>ä¸­ä½¿ç”¨å’Œè§£é‡Šè¿‡ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="6e15" class="mw ky iq kp b be mx my l mz na">import torch, logging<br/><br/>## disable warnings<br/>logging.disable(logging.WARNING)  <br/><br/>## Imaging  library<br/>from PIL import Image<br/>from torchvision import transforms as tfms<br/><br/><br/>## Basic libraries<br/>from fastdownload import FastDownload<br/>import numpy as np<br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from IPython.display import display<br/>import shutil<br/>import os<br/><br/>## For video display<br/>from IPython.display import HTML<br/>from base64 import b64encode<br/><br/><br/>## Import the CLIP artifacts <br/>from transformers import CLIPTextModel, CLIPTokenizer<br/>from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler<br/><br/>## Helper functions<br/><br/>def load_artifacts():<br/>    '''<br/>    A function to load all diffusion artifacts<br/>    '''<br/>    vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")<br/>    unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")<br/>    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)<br/>    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")<br/>    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)    <br/>    return vae, unet, tokenizer, text_encoder, scheduler<br/><br/>def load_image(p):<br/>    '''<br/>    Function to load images from a defined path<br/>    '''<br/>    return Image.open(p).convert('RGB').resize((512,512))<br/><br/>def pil_to_latents(image):<br/>    '''<br/>    Function to convert image to latents<br/>    '''<br/>    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0<br/>    init_image = init_image.to(device="cuda", dtype=torch.float16) <br/>    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215<br/>    return init_latent_dist<br/><br/>def latents_to_pil(latents):<br/>    '''<br/>    Function to convert latents to images<br/>    '''<br/>    latents = (1 / 0.18215) * latents<br/>    with torch.no_grad():<br/>        image = vae.decode(latents).sample<br/>    image = (image / 2 + 0.5).clamp(0, 1)<br/>    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()<br/>    images = (image * 255).round().astype("uint8")<br/>    pil_images = [Image.fromarray(image) for image in images]<br/>    return pil_images<br/><br/>def text_enc(prompts, maxlen=None):<br/>    '''<br/>    A function to take a texual promt and convert it into embeddings<br/>    '''<br/>    if maxlen is None: maxlen = tokenizer.model_max_length<br/>    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") <br/>    return text_encoder(inp.input_ids.to("cuda"))[0].half()<br/><br/>vae, unet, tokenizer, text_encoder, scheduler = load_artifacts()</span></pre><p id="b815" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬ä¹Ÿä¸‹è½½ä¸€ä¸ªå›¾åƒï¼Œæˆ‘ä»¬å°†ç”¨äºä»£ç å®ç°è¿‡ç¨‹ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="1f6b" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>init_img</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/89268ca2fbe17090e9fe9ecaa85da8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*FeEeIlb1KycLo-9I.png"/></div></figure></div><div class="ab cl kq kr hu ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="ij ik il im in"><h1 id="9daf" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">2 DiffEdit:çº¯ç²¹çš„å®ç°</h1><p id="d166" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">è®©æˆ‘ä»¬ä»æŒ‰ç…§ä½œè€…çš„å»ºè®®å®ç°è¿™ç¯‡è®ºæ–‡å¼€å§‹ï¼Œå› æ­¤æ˜¯çº¯ç²¹çš„å®ç°ã€‚</p><h1 id="1ba8" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">2.1 é®ç½©åˆ›å»º:DiffEdit è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/a382e1dc0624fff1ec4639e610d165ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Bsq2nuzhMuqMx0SF.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 4:<code class="fe km kn ko kp b">DiffEdit</code>è®ºæ–‡çš„ç¬¬ä¸€æ­¥ã€‚ä¿¡ç”¨â€” <a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank">è®ºæ–‡</a></p></figure><p id="c212" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®ºæ–‡ä¸­æœ‰å¯¹æ­¥éª¤ 1 æ›´è¯¦ç»†çš„è§£é‡Šï¼Œä¸‹é¢æ˜¯æåˆ°çš„å…³é”®éƒ¨åˆ†â€”â€”<br/>1ã€‚ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æ¡ä»¶å¯¹å›¾åƒå»å™ªï¼Œä¸€ä¸ªä½¿ç”¨å‚è€ƒæ–‡æœ¬ï¼Œå¦ä¸€ä¸ªä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬ï¼Œå¹¶ä»ç»“æœä¸­å–å·®ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯åœ¨ä¸åŒçš„éƒ¨åˆ†æœ‰æ›´å¤šçš„å˜åŒ–ï¼Œè€Œä¸æ˜¯åœ¨å›¾åƒçš„èƒŒæ™¯ä¸­ã€‚<br/> 2ã€‚é‡å¤æ­¤å·®åˆ†è¿‡ç¨‹ 10 æ¬¡<br/> 3ã€‚å¹³å‡è¿™äº›å·®å¼‚ï¼Œå¹¶å¯¹é®ç½©è¿›è¡ŒäºŒå€¼åŒ–</p><blockquote class="ni nj nk"><p id="fef3" class="jn jo nl jp b jq jr js jt ju jv jw jx nm jz ka kb nn kd ke kf no kh ki kj kk ij bi translated">æ³¨æ„â€”â€”è’™ç‰ˆåˆ›å»ºçš„ç¬¬ä¸‰æ­¥(å¹³å‡å’ŒäºŒå€¼åŒ–)åœ¨æ–‡ç« ä¸­æ²¡æœ‰è§£é‡Šæ¸…æ¥šï¼Œæˆ‘åšäº†å¾ˆå¤šå®éªŒæ‰å¼„æ¸…æ¥šã€‚</p></blockquote><p id="a6d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬å°†å°è¯•å®Œå…¨æŒ‰ç…§æ‰€æåˆ°çš„æ¥å®ç°è¿™ç¯‡è®ºæ–‡ã€‚æˆ‘ä»¬å°†ä¸ºæ­¤ä»»åŠ¡ä¿®æ”¹<a class="ae kl" href="https://aayushmnit.com/posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline" rel="noopener ugc nofollow" target="_blank"> prompt_2_img_i2i </a>å‡½æ•°ï¼Œä»¥è¿”å› latentsï¼Œè€Œä¸æ˜¯é‡æ–°ç¼©æ”¾å’Œè§£ç çš„å»å™ªå›¾åƒã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="dcdc" class="mw ky iq kp b be mx my l mz na">def prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    if not neg_prompts: uncond =  text_enc([""], text.shape[1])<br/>    else: uncond =  text_enc(neg_prompt, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # Computing the timestep to start the diffusion loop<br/>    t_start = max(steps - init_timestep, 0)<br/>    timesteps = scheduler.timesteps[t_start:].to("cuda")<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/><br/>        # Conditioning  the latents<br/>        #latents = scheduler.step(pred, ts, latents).pred_original_sample<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>    <br/>    # Returning the latent representation to output an array of 4x64x64<br/>    return latents.detach().cpu()</span></pre><p id="80d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ¶ä½œä¸€ä¸ª<code class="fe km kn ko kp b">create_mask</code>å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†è·å–ä¸€ä¸ªåˆå§‹å›¾åƒã€å¼•ç”¨æç¤ºå’Œå¸¦æœ‰æˆ‘ä»¬éœ€è¦é‡å¤è¿™äº›æ­¥éª¤çš„æ¬¡æ•°çš„æŸ¥è¯¢æç¤ºã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…å»ºè®® n=10ï¼Œå¼ºåº¦ä¸º 0.5ï¼Œåœ¨ä»–ä»¬çš„å®éªŒä¸­æ•ˆæœå¾ˆå¥½ã€‚å› æ­¤ï¼Œè¯¥å‡½æ•°çš„é»˜è®¤å€¼è¢«è°ƒæ•´ä¸ºã€‚<code class="fe km kn ko kp b">create_mask</code>åŠŸèƒ½æ‰§è¡Œä»¥ä¸‹æ­¥éª¤- <br/> 1ã€‚åˆ›å»ºä¸¤ä¸ªå»å™ªçš„æ½œåœ¨å€¼ï¼Œä¸€ä¸ªä»¥å‚è€ƒæ–‡æœ¬ä¸ºæ¡ä»¶ï¼Œå¦ä¸€ä¸ªä»¥æŸ¥è¯¢æ–‡æœ¬ä¸ºæ¡ä»¶ï¼Œå–è¿™ä¸¤ä¸ªæ½œåœ¨å€¼çš„å·®<br/> 2ã€‚é‡å¤æ­¤æ­¥éª¤ n æ¬¡<br/> 3ã€‚å–è¿™äº›å·®å¼‚çš„å¹³å‡å€¼å¹¶æ ‡å‡†åŒ–<br/> 4ã€‚é€‰æ‹©é˜ˆå€¼ 0.5 è¿›è¡ŒäºŒå€¼åŒ–å¹¶åˆ›å»ºä¸€ä¸ªé®ç½©</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="d6d2" class="mw ky iq kp b be mx my l mz na">def create_mask(init_img, rp, qp, n=10, s=0.5):<br/>    ## Initialize a dictionary to save n iterations<br/>    diff = {}<br/>    <br/>    ## Repeating the difference process n times<br/>    for idx in range(n):<br/>        ## Creating denoised sample using reference / original text<br/>        orig_noise = prompt_2_img_i2i(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Creating denoised sample using query / target text<br/>        query_noise = prompt_2_img_i2i(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Taking the difference <br/>        diff[idx] = (np.array(orig_noise)-np.array(query_noise))<br/>    <br/>    ## Creating a mask placeholder<br/>    mask = np.zeros_like(diff[0])<br/>    <br/>    ## Taking an average of 10 iterations<br/>    for idx in range(n):<br/>        ## Note np.abs is a key step<br/>        mask += np.abs(diff[idx])  <br/>        <br/>    ## Averaging multiple channels <br/>    mask = mask.mean(0)<br/>    <br/>    ## Normalizing <br/>    mask = (mask - mask.mean()) / np.std(mask)<br/>    <br/>    ## Binarizing and returning the mask object<br/>    return (mask &gt; 0).astype("uint8")<br/><br/>mask = create_mask(init_img=init_img, rp=["a horse image"], qp=["a zebra image"], n=10)</span></pre><p id="bb81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬åœ¨å›¾åƒä¸Šå¯è§†åŒ–ç”Ÿæˆçš„é®ç½©ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="52b8" class="mw ky iq kp b be mx my l mz na">plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'<br/>plt.imshow(<br/>    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>    cmap='cividis', <br/>    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/26188ac0b192988e7f6bd1305a6d7b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*uZvauoHkzAK6neLw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 5:æˆ‘ä»¬çš„é©¬å›¾åƒçš„æ©è”½å¯è§†åŒ–ã€‚</p></figure><p id="c080" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œåˆ¶ä½œçš„é¢å…·å¾ˆå¥½çš„è¦†ç›–äº†é©¬çš„éƒ¨åˆ†ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚</p><h1 id="6c2b" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">2.2 æ©è”½æ‰©æ•£:DiffEdit è®ºæ–‡çš„æ­¥éª¤ 2 å’Œ 3ã€‚</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nq"><img src="../Images/58c77afa23523bd9269b9b9826d1ee7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oAgHGBoneVPAJLOY.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 6:<code class="fe km kn ko kp b">DiffEdit</code>è®ºæ–‡ä¸­çš„ç¬¬äºŒæ­¥å’Œç¬¬ä¸‰æ­¥ã€‚ä¿¡ç”¨â€” <a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank">è®ºæ–‡</a></p></figure><p id="f0d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ­¥éª¤ 2 å’Œ 3 éœ€è¦åœ¨åŒä¸€ä¸ªå¾ªç¯ä¸­å®ç°ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä½œè€…æ˜¯è¯´æ ¹æ®éå±è”½éƒ¨åˆ†çš„å‚è€ƒæ–‡æœ¬å’Œå±è”½éƒ¨åˆ†çš„æŸ¥è¯¢æ–‡æœ¬æ¥è°ƒèŠ‚æ½œåœ¨äº‹ä»¶ã€‚<br/>ä½¿ç”¨è¿™ä¸ªç®€å•çš„å…¬å¼å°†è¿™ä¸¤ä¸ªéƒ¨åˆ†ç»„åˆèµ·æ¥ï¼Œä»¥åˆ›å»ºç»„åˆçš„æ½œåœ¨å®¢æˆ·-</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/cf632b532fa905e82c4d36f424b2dacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*6cvj1V-AEGXDhFDrPBlgFg.png"/></div></figure><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="49a9" class="mw ky iq kp b be mx my l mz na">def prompt_2_img_diffedit(rp, qp, init_img, mask, g=7.5, seed=100, strength =0.7, steps=70, dim=512):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    rtext = text_enc(rp) <br/>    qtext = text_enc(qp)<br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    uncond =  text_enc([""], rtext.shape[1])<br/>    emb = torch.cat([uncond, rtext, qtext])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # Computing the timestep to start the diffusion loop<br/>    t_start = max(steps - init_timestep, 0)<br/>    timesteps = scheduler.timesteps[t_start:].to("cuda")<br/>    <br/>    # Converting mask to torch tensor<br/>    mask = torch.tensor(mask, dtype=unet.dtype).unsqueeze(0).unsqueeze(0).to("cuda")<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u, rt, qt = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)<br/>            <br/>        # Performing Guidance<br/>        rpred = u + g*(rt-u)<br/>        qpred = u + g*(qt-u)<br/><br/>        # Conditioning  the latents<br/>        rlatents = scheduler.step(rpred, ts, latents).prev_sample<br/>        qlatents = scheduler.step(qpred, ts, latents).prev_sample<br/>        latents = mask*qlatents + (1-mask)*rlatents<br/>    <br/>    # Returning the latent representation to output an array of 4x64x64<br/>    return latents_to_pil(latents)</span></pre><p id="3405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬å°†ç”Ÿæˆçš„å›¾åƒå¯è§†åŒ–</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="2c9b" class="mw ky iq kp b be mx my l mz na">output = prompt_2_img_diffedit(<br/>    rp = ["a horse image"], <br/>    qp=["a zebra image"],<br/>    init_img=init_img, <br/>    mask = mask, <br/>    g=7.5, seed=100, strength =0.5, steps=70, dim=512)<br/><br/>## Plotting side by side<br/>fig, axs = plt.subplots(1, 2, figsize=(12, 6))<br/>for c, img in enumerate([init_img, output[0]]): <br/>    axs[c].imshow(img)<br/>    if c == 0 : axs[c].set_title(f"Initial image ")<br/>    else: axs[c].set_title(f"DiffEdit output")</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ns"><img src="../Images/bfa1c3e48b34ce1aed500b8e0a66a506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8LFiik5WC6cp00cV.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 7: DiffEdit è¾“å‡ºå¯è§†åŒ–</p></figure><p id="c014" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬ä¸ºé®ç½©å’Œæ‰©æ•£è¿‡ç¨‹åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="7c5a" class="mw ky iq kp b be mx my l mz na">def diffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=70, dim=512):<br/>    <br/>    ## Step 1: Create mask<br/>    mask = create_mask(init_img=init_img, rp=rp, qp=qp)<br/>    <br/>    ## Step 2 and 3: Diffusion process using mask<br/>    output = prompt_2_img_diffedit(<br/>        rp = rp, <br/>        qp=qp, <br/>        init_img=init_img, <br/>        mask = mask, <br/>        g=g, <br/>        seed=seed,<br/>        strength =strength, <br/>        steps=steps, <br/>        dim=dim)<br/>    return mask , output</span></pre><p id="b21e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬ä¹Ÿä¸º<code class="fe km kn ko kp b">DiffEdit</code>åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–å‡½æ•°ï¼Œæ˜¾ç¤ºåŸå§‹è¾“å…¥å›¾åƒã€å±è”½å›¾åƒå’Œæœ€ç»ˆè¾“å‡ºå›¾åƒã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="d6da" class="mw ky iq kp b be mx my l mz na">def plot_diffEdit(init_img, output, mask):<br/>    ## Plotting side by side<br/>    fig, axs = plt.subplots(1, 3, figsize=(12, 6))<br/>    <br/>    ## Visualizing initial image<br/>    axs[0].imshow(init_img)<br/>    axs[0].set_title(f"Initial image")<br/>    <br/>    ## Visualizing initial image<br/>    axs[2].imshow(output[0])<br/>    axs[2].set_title(f"DiffEdit output")<br/>    <br/>    ## Visualizing the mask <br/>    axs[1].imshow(np.array(init_img), cmap='gray') <br/>    axs[1].imshow(<br/>        Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>        cmap='cividis', <br/>        alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>    )<br/>    axs[1].set_title(f"DiffEdit mask")</span></pre><p id="1b79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬åœ¨ä¸€äº›å›¾åƒä¸Šæµ‹è¯•è¿™ä¸ªå‡½æ•°ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="94d3" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>mask, output = diffEdit(<br/>  init_img, <br/>  rp = ["a horse image"], <br/>  qp=["a zebra image"]<br/>)<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/a49aaa1cc580edc4e466053831aadd5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T9C2ryMjZ5FNI_oG.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 8: Purist å®ç°è¾“å‡ºç¤ºä¾‹</p></figure><p id="5b98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¤ªå¥½äº†ï¼Œè®©æˆ‘ä»¬è¯•è¯•å¦ä¸€ä¸ªã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="3ad2" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')<br/>init_img = load_image(p)<br/>mask, output = diffEdit(<br/>  init_img, <br/>  rp = ['Bowl of Strawberries'], <br/>  qp=['Bowl of Grapes']<br/>)<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/7fa0afcd31383e4919c73f64cd764d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ORLNftQ3EaYlFhBA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 9: Purist å®ç°è¾“å‡ºç¤ºä¾‹</p></figure></div><div class="ab cl kq kr hu ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="ij ik il im in"><h1 id="ac22" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">3 FastDiffEdit:ä¸€ä¸ªæ›´å¿«çš„ DiffEdit å®ç°</h1><p id="8568" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°äº† purist çš„å®ç°ï¼Œæˆ‘å»ºè®®æˆ‘ä»¬å¯ä»¥åœ¨é€Ÿåº¦å’Œæ›´å¥½çš„ç»“æœæ–¹é¢å¯¹åŸå§‹çš„ DiffEdit è¿‡ç¨‹è¿›è¡Œä¸€äº›æ”¹è¿›ã€‚æˆ‘ä»¬å§‘ä¸”ç§°è¿™äº›æ”¹è¿›ä¸º<code class="fe km kn ko kp b">FastDiffEdit</code>ã€‚</p><h1 id="b93d" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">3.1 é®ç½©åˆ›å»º:å¿«é€Ÿ DiffEdit é®ç½©è¿‡ç¨‹</h1><p id="c301" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">æˆ‘å¯¹å½“å‰çš„è’™ç‰ˆæ–¹å¼æœ€å¤§çš„é—®é¢˜æ˜¯å®ƒå¤ªèŠ±æ—¶é—´äº†(åœ¨ 4500 GPU ä¸Šå¤§çº¦ 50 ç§’)ã€‚æˆ‘çš„è§‚ç‚¹æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿è¡Œä¸€ä¸ªå®Œæ•´çš„æ‰©æ•£å¾ªç¯æ¥å¯¹å›¾åƒè¿›è¡Œé™å™ªï¼Œè€Œåªéœ€åœ¨ä¸€æ¬¡æ‹æ‘„ä¸­ä½¿ç”¨åŸå§‹æ ·æœ¬çš„ U-net é¢„æµ‹ï¼Œå¹¶å°†é‡å¤æ¬¡æ•°å¢åŠ åˆ° 20 æ¬¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†è®¡ç®—ä» 10*25 = 250 æ­¥æé«˜åˆ° 20 æ­¥(å°‘ 12x ä¸ªå¾ªç¯)ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦è¡Œå¾—é€šã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="046c" class="mw ky iq kp b be mx my l mz na">def prompt_2_img_i2i_fast(prompts, init_img, g=7.5, seed=100, strength =0.5, steps=50, dim=512):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    uncond =  text_enc([""], text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # We need to scale the i/p latents to match the variance<br/>    inp = scheduler.scale_model_input(torch.cat([latents] * 2), timesteps)<br/>    # Predicting noise residual using U-Net<br/>    with torch.no_grad(): u,t = unet(inp, timesteps, encoder_hidden_states=emb).sample.chunk(2)<br/>         <br/>    # Performing Guidance<br/>    pred = u + g*(t-u)<br/><br/>    # Zero shot prediction<br/>    latents = scheduler.step(pred, timesteps, latents).pred_original_sample<br/>    <br/>    # Returning the latent representation to output an array of 4x64x64<br/>    return latents.detach().cpu()</span></pre><p id="3a19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„å±è”½å‡½æ•°ï¼Œå®ƒå¯ä»¥æ¥å—æˆ‘ä»¬çš„<code class="fe km kn ko kp b">prompt_2_img_i2i_fast</code>å‡½æ•°ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="3501" class="mw ky iq kp b be mx my l mz na">def create_mask_fast(init_img, rp, qp, n=20, s=0.5):<br/>    ## Initialize a dictionary to save n iterations<br/>    diff = {}<br/>    <br/>    ## Repeating the difference process n times<br/>    for idx in range(n):<br/>        ## Creating denoised sample using reference / original text<br/>        orig_noise = prompt_2_img_i2i_fast(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Creating denoised sample using query / target text<br/>        query_noise = prompt_2_img_i2i_fast(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Taking the difference <br/>        diff[idx] = (np.array(orig_noise)-np.array(query_noise))<br/>    <br/>    ## Creating a mask placeholder<br/>    mask = np.zeros_like(diff[0])<br/>    <br/>    ## Taking an average of 10 iterations<br/>    for idx in range(n):<br/>        ## Note np.abs is a key step<br/>        mask += np.abs(diff[idx])  <br/>        <br/>    ## Averaging multiple channels <br/>    mask = mask.mean(0)<br/>    <br/>    ## Normalizing <br/>    mask = (mask - mask.mean()) / np.std(mask)<br/>    <br/>    ## Binarizing and returning the mask object<br/>    return (mask &gt; 0).astype("uint8")</span></pre><p id="305e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ–°çš„è’™ç‰ˆå‡½æ•°æ˜¯å¦èƒ½äº§ç”Ÿä¸€ä¸ªå¥½çš„è’™ç‰ˆã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="bf9e" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>mask = create_mask_fast(init_img=init_img, rp=["a horse image"], qp=["a zebra image"], n=20)<br/>plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'<br/>plt.imshow(<br/>    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>    cmap='cividis', <br/>    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ea85fff307fe72b9c89f037d7fa00f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*IP_tgj3Sao8IUQGq.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 10: <code class="fe km kn ko kp b">FastDiffEdit</code>é®è”½æˆ‘ä»¬çš„é©¬çš„å½¢è±¡ã€‚</p></figure><p id="e4a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢æ‰€çœ‹åˆ°çš„ï¼Œåœ¨æˆ‘çš„æœºå™¨ä¸Šï¼Œå±è”½å¾—åˆ°äº†æ”¹è¿›ï¼Œè®¡ç®—æ—¶é—´ä»å¤§çº¦ 50 ç§’å‡å°‘åˆ°å¤§çº¦ 10 ç§’(æé«˜äº† 5 å€ï¼).</p><p id="4137" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬é€šè¿‡æ·»åŠ  cv2 æŠ€å·§æ¥æ”¹è¿›æˆ‘ä»¬çš„é®ç½©ã€‚è¿™å°†åªæ˜¯å¹³æ»‘æ©è”½å¤šä¸€ç‚¹ç‚¹ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="440b" class="mw ky iq kp b be mx my l mz na">import cv2<br/>def improve_mask(mask):<br/>    mask  = cv2.GaussianBlur(mask*255,(3,3),1) &gt; 0<br/>    return mask.astype('uint8')<br/><br/>mask = improve_mask(mask)<br/>plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'<br/>plt.imshow(<br/>    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>    cmap='cividis', <br/>    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5c8d76afefa3a42ce7e9ca58ecbf44a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*5P29CXoH9G1GbkdD.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 11:ä½¿ç”¨ cv2 é«˜æ–¯æ¨¡ç³ŠæŠ€å·§æ”¹è¿›äº†æˆ‘ä»¬çš„é©¬å›¾åƒçš„<code class="fe km kn ko kp b">FastDiffEdit</code>æ©è”½å¯è§†åŒ–ã€‚</p></figure><p id="68f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çœ‹åˆ°çš„ï¼Œé®ç½©å˜å¾—æ›´åŠ å¹³æ»‘ï¼Œè¦†ç›–äº†æ›´å¤šçš„åŒºåŸŸã€‚</p><h1 id="a6f1" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">3.2 æ©è”½æ‰©æ•£:æ›¿æ¢ä¸ºğŸ¤—ä¿®è¡¥ç®¡é“</h1><p id="c601" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">å› æ­¤ï¼Œä¸æ˜¯ä½¿ç”¨æˆ‘ä»¬çš„å‡½æ•°æ¥æ‰§è¡Œæ©è”½æ‰©æ•£ï¼Œè€Œæ˜¯æœ‰ä¸€ä¸ªç‰¹æ®Šçš„ç®¡é“ğŸ¤—<code class="fe km kn ko kp b">diffusers</code>åº“åä¸º<code class="fe km kn ko kp b">inpaint</code>ç®¡é“ã€‚å®ƒé‡‡ç”¨æŸ¥è¯¢æç¤ºã€åˆå§‹å›¾åƒå’Œç”Ÿæˆçš„é®ç½©æ¥ç”Ÿæˆè¾“å‡ºå›¾åƒã€‚è®©æˆ‘ä»¬ä»è£…å…¥<code class="fe km kn ko kp b">inpaint</code>ç®¡é“å¼€å§‹ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="a649" class="mw ky iq kp b be mx my l mz na">from diffusers import StableDiffusionInpaintPipeline<br/>pipe = StableDiffusionInpaintPipeline.from_pretrained(<br/>    "runwayml/stable-diffusion-inpainting",<br/>    revision="fp16",<br/>    torch_dtype=torch.float16,<br/>).to("cuda")</span></pre><p id="9b0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬ç”Ÿæˆçš„è’™ç‰ˆå’Œå›¾åƒä¿®å¤ç®¡é“ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="013a" class="mw ky iq kp b be mx my l mz na">pipe(<br/>    prompt=["a zebra image"], <br/>    image=init_img, <br/>    mask_image=Image.fromarray(mask*255).resize((512,512)), <br/>    generator=torch.Generator("cuda").manual_seed(100),<br/>    num_inference_steps = 20<br/>).images[0]<br/>image</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b780ec9c6d42db7707d6febf71e5197f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*q8zzN__VxZIg54EZ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 12:æ²¹æ¼†ç®¡é“è¾“å‡ºã€‚</p></figure><p id="11a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æ­£å¦‚æˆ‘ä»¬ä¸Šé¢çœ‹åˆ°çš„ï¼Œä¿®å¤ç®¡é“åˆ›å»ºäº†ä¸€ä¸ªæ›´çœŸå®çš„æ–‘é©¬å›¾åƒã€‚è®©æˆ‘ä»¬ä¸ºé®ç½©å’Œæ‰©æ•£è¿‡ç¨‹åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="77d7" class="mw ky iq kp b be mx my l mz na">def fastDiffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=20, dim=512):<br/>    <br/>    ## Step 1: Create mask<br/>    mask = create_mask_fast(init_img=init_img, rp=rp, qp=qp, n=20)<br/>    <br/>    ## Improve masking using CV trick<br/>    mask = improve_mask(mask)<br/>    <br/>    ## Step 2 and 3: Diffusion process using mask<br/>    output = pipe(<br/>        prompt=qp, <br/>        image=init_img, <br/>        mask_image=Image.fromarray(mask*255).resize((512,512)), <br/>        generator=torch.Generator("cuda").manual_seed(100),<br/>        num_inference_steps = steps<br/>    ).images<br/>    return mask , output</span></pre><p id="03fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">è®©æˆ‘ä»¬åœ¨ä¸€äº›å›¾åƒä¸Šæµ‹è¯•è¿™ä¸ªå‡½æ•°ã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="90f4" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>mask, output = fastDiffEdit(init_img, rp = ["a horse image"], qp=["a zebra image"])<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/306fea9b50282cc5baea2b37bce9de6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QmgCMZXnBzpQmX4j.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 13: <code class="fe km kn ko kp b">FastDiffEdit</code>è¾“å‡ºç¤ºä¾‹</p></figure><p id="018c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">å¤ªå¥½äº†ï¼Œè®©æˆ‘ä»¬è¯•è¯•å¦ä¸€ä¸ªã€‚</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="56ca" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')<br/>init_img = load_image(p)<br/>mask, output = fastDiffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/85e83579ff8a94530d009d407cfc83df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7-9KnjbOc7clRg63.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">å›¾ 14: <code class="fe km kn ko kp b">FastDiffEdit</code>è¾“å‡ºç¤ºä¾‹</p></figure><h1 id="d7a6" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">4 ç»“è®º</h1><p id="fa9b" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä½œè€…æåˆ°çš„<code class="fe km kn ko kp b">DiffEdit</code>è®ºæ–‡ï¼Œç„¶åæˆ‘ä»¬å¯¹åˆ›å»º<code class="fe km kn ko kp b">FastDiffEdit</code>çš„æ–¹æ³•æå‡ºäº†æ”¹è¿›ï¼Œå°†è®¡ç®—é€Ÿåº¦æé«˜äº† 5 å€ã€‚</p><p id="ec7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">æˆ‘å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»å®ƒï¼Œå¹¶éšæ—¶ä½¿ç”¨æˆ‘çš„ä»£ç ï¼Œå¹¶å°è¯•ç”Ÿæˆæ‚¨çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå¦‚æœå¯¹ä»£ç æˆ–åšå®¢å¸–å­æœ‰ä»»ä½•åé¦ˆï¼Œè¯·éšæ—¶è”ç³» LinkedIn æˆ–ç»™æˆ‘å‘ç”µå­é‚®ä»¶ï¼Œåœ°å€æ˜¯ aayushmnit@gmail.comã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ç½‘ç«™ä¸Šé˜…è¯»åšå®¢çš„æ—©æœŸå‘å¸ƒã€aayushmnit.com<a class="ae kl" href="https://aayushmnit.com/blog.html" rel="noopener ugc nofollow" target="_blank">Aayush agr awal-åšå®¢</a>ã€‚</p></div></div>    
</body>
</html>